<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Just Do It"><title>DeepLearning-2.线性代数 | DeepLearner</title><link rel="stylesheet" type="text/css" href="../../../../css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/static/css/normalize.css"><link rel="stylesheet" type="text/css" href="/static/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/static/css/grids-responsive-min.css"><link rel="stylesheet" href="/static/css/font-awesome.min.css"><script type="text/javascript" src="/static/js/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="../../../../favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="../../../../favicon.ico"><link rel="apple-touch-icon" href="../../../../apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="../../../../apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="../../../../atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-83251621-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + 'b207d9febf3295d375e6e9b42006f9eb';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/static/js/clipboard.min.js"></script><script type="text/javascript" src="/static/js/toastr.min.js"></script><link rel="stylesheet" href="/static/css/toastr.min.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">DeepLearning-2.线性代数</h1><a id="logo" href="../../../../.">DeepLearner</a><p class="description">Deep Learning Notes</p></div><div id="nav-menu"><a class="current" href="../../../../."><i class="fa fa-home"> 首页</i></a><a href="../../../../archives/"><i class="fa fa-archive"> 归档</i></a><a href="../../../../tagcloud/"><i class="fa fa-tags"> 标签</i></a><a href="../../../../resource/"><i class="fa fa-cloud"> 资源</i></a><a href="../../../../guestbook/"><i class="fa fa-wechat"> 留言</i></a><a href="../../../../codeline/"><i class="fa fa-circle"> 码迹</i></a><a href="../../../../resume/"><i class="fa fa-user"> 个人简历</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">DeepLearning-2.线性代数</h1><div class="post-meta">created:2021-12-09</div><div class="post-meta">updated:2021-12-21<span> | </span><span class="category"><a href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><a class="disqus-comment-count" href="#waline"><span class="waline-comment-count" id=""></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E9%87%8F%E3%80%81%E5%90%91%E9%87%8F%E3%80%81%E7%9F%A9%E9%98%B5%E5%92%8C%E5%BC%A0%E9%87%8F"><span class="toc-number">1.</span> <span class="toc-text">标量、向量、矩阵和张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%87%E9%87%8F%EF%BC%88scalar%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">标量（scalar）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%91%E9%87%8F%EF%BC%88vector%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">向量（vector）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%EF%BC%88matrix%EF%BC%89"><span class="toc-number">1.3.</span> <span class="toc-text">矩阵（matrix）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%EF%BC%88tensor%EF%BC%89"><span class="toc-number">1.4.</span> <span class="toc-text">张量（tensor）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%AC%E7%BD%AE%EF%BC%88transpose%EF%BC%89"><span class="toc-number">1.5.</span> <span class="toc-text">转置（transpose）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A8%8B%E5%BA%8F%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.6.</span> <span class="toc-text">程序实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E5%8A%A0%E6%B3%95"><span class="toc-number">1.7.</span> <span class="toc-text">矩阵加法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A8%8B%E5%BA%8F%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">1.8.</span> <span class="toc-text">程序实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E5%92%8C%E5%90%91%E9%87%8F%E7%9B%B8%E4%B9%98"><span class="toc-number">2.</span> <span class="toc-text">矩阵和向量相乘</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A8%8B%E5%BA%8F%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">2.1.</span> <span class="toc-text">程序实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E4%BD%8D%E7%9F%A9%E9%98%B5%E5%92%8C%E9%80%86%E7%9F%A9%E9%98%B5"><span class="toc-number">3.</span> <span class="toc-text">单位矩阵和逆矩阵</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A8%8B%E5%BA%8F%E5%AE%9E%E7%8E%B0-3"><span class="toc-number">3.1.</span> <span class="toc-text">程序实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8E%A8%E7%AE%97"><span class="toc-number">3.2.</span> <span class="toc-text">手动推算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8C%83%E6%95%B0"><span class="toc-number">4.</span> <span class="toc-text">范数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A8%8B%E5%BA%8F%E5%AE%9E%E7%8E%B0-4"><span class="toc-number">4.1.</span> <span class="toc-text">程序实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8E%A8%E7%AE%97-1"><span class="toc-number">4.2.</span> <span class="toc-text">手动推算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E6%AE%8A%E7%B1%BB%E5%9E%8B%E7%9A%84%E7%9F%A9%E9%98%B5%E5%92%8C%E5%90%91%E9%87%8F"><span class="toc-number">5.</span> <span class="toc-text">特殊类型的矩阵和向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3"><span class="toc-number">6.</span> <span class="toc-text">特征分解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A8%8B%E5%BA%8F%E5%AE%9E%E7%8E%B0-5"><span class="toc-number">6.1.</span> <span class="toc-text">程序实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8E%A8%E7%AE%97-2"><span class="toc-number">6.2.</span> <span class="toc-text">手动推算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3"><span class="toc-number">7.</span> <span class="toc-text">奇异值分解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SVD%E5%88%86%E8%A7%A3%E7%9F%A9%E9%98%B5A%E7%9A%84%E6%AD%A5%E9%AA%A4"><span class="toc-number">7.1.</span> <span class="toc-text">SVD分解矩阵A的步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A8%8B%E5%BA%8F%E5%AE%9E%E7%8E%B0-6"><span class="toc-number">7.2.</span> <span class="toc-text">程序实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8E%A8%E7%AE%97-3"><span class="toc-number">7.3.</span> <span class="toc-text">手动推算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%B9%E8%BF%90%E7%AE%97"><span class="toc-number">8.</span> <span class="toc-text">迹运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%8C%E5%88%97%E5%BC%8F"><span class="toc-number">9.</span> <span class="toc-text">行列式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90"><span class="toc-number">10.</span> <span class="toc-text">主成分分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E4%BD%9C%E7%94%A8"><span class="toc-number">10.1.</span> <span class="toc-text">背景与作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PCA%E6%A6%82%E5%BF%B5"><span class="toc-number">10.2.</span> <span class="toc-text">PCA概念</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PCA%E7%AE%97%E6%B3%95"><span class="toc-number">10.3.</span> <span class="toc-text">PCA算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">10.4.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8E%A8%E7%AE%97-4"><span class="toc-number">10.5.</span> <span class="toc-text">手动推算</span></a></li></ol></li></ol></div></div><div class="post-content"><p>线性代数，主要面向连续数学，而非离散数学。</p>
<span id="more"></span>

<h3 id="标量、向量、矩阵和张量"><a href="#标量、向量、矩阵和张量" class="headerlink" title="标量、向量、矩阵和张量"></a>标量、向量、矩阵和张量</h3><h4 id="标量（scalar）"><a href="#标量（scalar）" class="headerlink" title="标量（scalar）"></a>标量（scalar）</h4><p>&emsp;&emsp;表示一个<strong>单独的数</strong>，通常用<em>斜体</em>小写字母表示，如：$\mathit{s}\in \mathbb{R},\mathit{n}\in \mathbb{N}$。</p>
<h4 id="向量（vector）"><a href="#向量（vector）" class="headerlink" title="向量（vector）"></a>向量（vector）</h4><p>&emsp;&emsp;表示<strong>一列数</strong>，这些数是有序列的，并且可以通过下标索引获取对应值，通常用<strong>粗体</strong>小写字母表示，如：$\mathbf{x}\in \mathbb{R}^{n}$，表示元素取实数，且有$n$个元素，第一个元素：$x_{1}$，第n个元素：$x_{n}$。向量写成列形式：</p>
<p>$$\begin{bmatrix}<br>x_{1}\\<br>x_{2}\\<br>…\\<br>x_{n}<br>\end{bmatrix}$$</p>
<h4 id="矩阵（matrix）"><a href="#矩阵（matrix）" class="headerlink" title="矩阵（matrix）"></a>矩阵（matrix）</h4><p>&emsp;&emsp;表示一个<strong>二维数组</strong>，每个元素下标由两个数字确定，通常用<strong>大写粗体</strong>字母表示，如：$\mathbf{x}\in \mathbb{R}^{m\times n}$，表示元素取实数的$m$行$n$列矩阵，有$m\times n$个元素，可表示为$A_{1,1},A_{m,n}$，$A_{i:}$表示为第$i$行，$A_{:j}$表示为第$j$列。矩阵形式：</p>
<p>$$\begin{bmatrix}<br>A_{1,1} &amp; A_{1,2} \\<br>A_{2,1} &amp; A_{2,2}<br>\end{bmatrix}$$</p>
<p>&emsp;&emsp;矩阵逐元素操作：将函数$f$应用到<strong>A</strong>的所有元素上，用$f(A)_{i,j}$表示。</p>
<h4 id="张量（tensor）"><a href="#张量（tensor）" class="headerlink" title="张量（tensor）"></a>张量（tensor）</h4><p>&emsp;&emsp;<strong>超过二维的数组</strong>，表示同矩阵，如：<strong>A</strong>表示三维张量，$A_{i,j,k}$表示其元素。</p>
<h4 id="转置（transpose）"><a href="#转置（transpose）" class="headerlink" title="转置（transpose）"></a>转置（transpose）</h4><p>&emsp;&emsp;矩阵转置相当于<strong>沿着对角线翻转</strong>，定义如下：$A_{i,j}^{\top } &#x3D; A_{j,i}$。</p>
<ul>
<li><font color="#ff0000">矩阵转置的转置等于矩阵本身：$\left ( A^{\top} \right )^{\top} &#x3D; A$</font></li>
<li><font color="#ff0000">$(A+B)^{\top} &#x3D; A^{\top} + B^{\top}$</font></li>
<li><font color="#ff0000">$(\lambda A)^{\top} &#x3D; \lambda A^{\top}$</font></li>
<li><font color="#ff0000">$(AB)^{\top} &#x3D; B^{\top}A^{\top}$</font></li>
</ul>
<h4 id="程序实现"><a href="#程序实现" class="headerlink" title="程序实现"></a>程序实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵转置</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">2</span>,<span class="number">3</span>]])  <span class="comment"># 矩阵A</span></span><br><span class="line">A_t = A.transpose() <span class="comment"># 矩阵转置</span></span><br><span class="line">A_t_t = A.transpose().transpose() <span class="comment"># 矩阵转置的转置</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A:\n&quot;</span>, A)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A的转置:\n&quot;</span>, A_t)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A转置的转置:\n&quot;</span>, A_t_t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">A:</span><br><span class="line"> [[<span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]]</span><br><span class="line">A的转置:</span><br><span class="line"> [[<span class="number">1</span> <span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">0</span> <span class="number">3</span>]]</span><br><span class="line">A转置的转置:</span><br><span class="line"> [[<span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span>]]</span><br></pre></td></tr></table></figure>

<h4 id="矩阵加法"><a href="#矩阵加法" class="headerlink" title="矩阵加法"></a>矩阵加法</h4><p>&emsp;&emsp;加法即<strong>对应元素相加</strong>，要求两个矩阵形状一样：</br></p>
<center>$C = A + B,C_{i,j} = A_{i,j} + B_{i,j}$</center></br>

<p>&emsp;&emsp;<strong>数乘</strong>即一个<strong>标量</strong>与<strong>矩阵每个元素相乘</strong>：</br></p>
<center>$D=a\cdot B+c,D_{i,j}=a\cdot B_{i,j}+c$</center></br>

<p>&emsp;&emsp;广播是矩阵和向量相加，得到一个矩阵，将$b$加到了$A$的每一行上，本质上是构造了一个将$b$按行复制的一个新矩阵。</br></p>
<center>$C=A+b,C_{i,j}=A_{i,j}+b_{j}$</center>

<h4 id="程序实现-1"><a href="#程序实现-1" class="headerlink" title="程序实现"></a>程序实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵加法</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line">a = <span class="number">2</span>     <span class="comment"># 标量a</span></span><br><span class="line">b = np.array([<span class="number">1</span>,<span class="number">1</span>]) <span class="comment"># 向量b</span></span><br><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]) <span class="comment"># 矩阵A</span></span><br><span class="line">B = np.array([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]]) <span class="comment"># 矩阵B</span></span><br><span class="line"></span><br><span class="line">C = A + B <span class="comment"># 矩阵相加</span></span><br><span class="line">D = a * B <span class="comment"># 数乘</span></span><br><span class="line">E = A + b <span class="comment"># 广播</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵相加:\n&quot;</span>, C)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数乘:\n&quot;</span>, D)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;广播:\n&quot;</span>, E)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">矩阵相加:</span><br><span class="line"> [[ <span class="number">6</span>  <span class="number">8</span>]</span><br><span class="line"> [<span class="number">10</span> <span class="number">12</span>]]</span><br><span class="line">数乘:</span><br><span class="line"> [[<span class="number">10</span> <span class="number">12</span>]</span><br><span class="line"> [<span class="number">14</span> <span class="number">16</span>]]</span><br><span class="line">广播:</span><br><span class="line"> [[<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span>]]</span><br></pre></td></tr></table></figure>

<h3 id="矩阵和向量相乘"><a href="#矩阵和向量相乘" class="headerlink" title="矩阵和向量相乘"></a>矩阵和向量相乘</h3><p>&emsp;&emsp;两个矩阵相乘得到第三个矩阵，$A_{m\times n},B_{n\times p}$，相乘得到矩阵$C_{m\times p}$：</p>
<center>$C=AB$</center></br>

<p>&emsp;&emsp;具体定义为：</p>
<center>$C_{i,j}=\sum_{k}A_{i,k}B_{k,j}$</center></br>

<p>&emsp;&emsp;矩阵相乘不是对应元素相乘，元素对应相乘又叫Hadamard乘积，记作$A\odot B$。</br><br>&emsp;&emsp;向量可看作列为1的矩阵，两个相同维数的向量$x$和$y$的点乘（Dot Product）或者内积，可以表示为$x^{\top }y$。</br></p>
<p>&emsp;&emsp;矩阵乘积运算满足<strong>分配率</strong>和<strong>结合律</strong>：</p>
<center><font color="#ff0000">$A\left ( B+C \right )=AB+AC$</font></center></br>
<center><font color="#ff0000">$A\left ( BC \right )=\left ( AB \right )C$</font></center></br>

<p>&emsp;&emsp;不满足<strong>交换律</strong>：</p>
<center><font color="#ff0000">$AB=BA$，情况并非总满足</font></center></br>
&emsp;&emsp;乘积的转置：
<center><font color="#ff0000">$\left ( AB \right )^{\top }=B^{\top}A^{\top}$</font></center>

<h4 id="程序实现-2"><a href="#程序实现-2" class="headerlink" title="程序实现"></a>程序实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵乘法</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]) <span class="comment"># 矩阵A</span></span><br><span class="line">B = np.array([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]]) <span class="comment"># 矩阵B</span></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1</span>,<span class="number">2</span>]) <span class="comment"># 向量x</span></span><br><span class="line">y = np.array([<span class="number">3</span>,<span class="number">4</span>]) <span class="comment"># 向量y</span></span><br><span class="line"></span><br><span class="line">C = np.dot(A, B) <span class="comment"># 矩阵相乘</span></span><br><span class="line">D = np.multiply(A, B) <span class="comment"># 矩阵逐元素相乘，又叫Hadamard乘积，同 A*B</span></span><br><span class="line"></span><br><span class="line">z = np.dot(x, y) <span class="comment"># 向量点乘或内积, 同x的转置乘y </span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵相乘:\n&quot;</span>, C)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵逐元素相乘:\n&quot;</span>, D)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;向量内积:\n&quot;</span>, z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">矩阵相乘:</span><br><span class="line"> [[<span class="number">19</span> <span class="number">22</span>]</span><br><span class="line"> [<span class="number">43</span> <span class="number">50</span>]]</span><br><span class="line">矩阵逐元素相乘:</span><br><span class="line"> [[ <span class="number">5</span> <span class="number">12</span>]</span><br><span class="line"> [<span class="number">21</span> <span class="number">32</span>]]</span><br><span class="line">向量内积:</span><br><span class="line"> <span class="number">11</span></span><br></pre></td></tr></table></figure>

<h3 id="单位矩阵和逆矩阵"><a href="#单位矩阵和逆矩阵" class="headerlink" title="单位矩阵和逆矩阵"></a>单位矩阵和逆矩阵</h3><p>&emsp;&emsp;<strong>单位矩阵</strong>（Identity Matrix）为乘以任意一个向量等于这个向量本身，记为：$I_{n}$，为保持$n$维向量不变的单位矩阵：</p>
<center>$I_{n}\in \mathbb{R}^{n\times n},\forall x\in \mathbb{R}^{n},I_{n}x=x$</center></br>

<p>&emsp;&emsp;单位矩阵结构如：</p>
<p>$$\begin{bmatrix}<br>1 &amp; 0 &amp; 0\\<br>0 &amp; 1 &amp; 0\\<br>0 &amp; 0 &amp; 1<br>\end{bmatrix}$$</p>
<p>&emsp;&emsp;<strong>逆矩阵（Inverse Matrix）</strong>：对于$n$阶矩阵$A$，如果有一个$n$阶矩阵$B$，使$AB&#x3D;BA&#x3D;I^{n}$，则矩阵$A$可逆，$B$为$A$的<strong>逆矩阵</strong>，$B&#x3D;A^{-1}$。</p>
<p>&emsp;&emsp;如果$A^{-1}$存在，则线性方程组$Ax&#x3D;b$的解为：</p>
<center>$A^{-1}Ax=I_{n}x=x=A^{-1}b$</center></br>

<ul>
<li><font color="#ff0000">定理1：若矩阵$A$可逆，则$|A|\neq 0$。</font></li>
<li><font color="#ff0000">定理2：若$|A|\neq 0$，则矩阵$A$可逆，且$A^{-1}&#x3D;\frac{1}{|A|}A^{\ast }$，$A^{\ast}$为矩阵$A$的<strong>伴随矩阵</strong>。</font></li>
<li><font color="#ff0000">定理3：当$|A|&#x3D;0$时，称$A$为<strong>奇异矩阵</strong>。</font></li>
<li><font color="#ff0000">定理4：$\left ( \lambda A \right )^{-1}&#x3D;\frac{1}{\lambda}A^{-1}$</font></li>
<li><font color="#ff0000">定理5：$\left ( AB \right )^{-1}&#x3D;B^{-1}A^{-1}$</font></li>
</ul>
<h4 id="程序实现-3"><a href="#程序实现-3" class="headerlink" title="程序实现"></a>程序实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单位矩阵</span></span><br><span class="line">I3 = np.identity(<span class="number">3</span>)    <span class="comment"># 单位矩阵</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;单位矩阵:\n&quot;</span>,I3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 逆矩阵</span></span><br><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">3</span>]]) <span class="comment"># 矩阵A</span></span><br><span class="line">A_inv = np.linalg.inv(A)  <span class="comment"># 矩阵A的逆矩阵</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A的逆矩阵:\n&quot;</span>, A_inv)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">单位矩阵:</span><br><span class="line"> [[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">A的逆矩阵:</span><br><span class="line"> [[ <span class="number">1.</span>   <span class="number">3.</span>  -<span class="number">2.</span> ]</span><br><span class="line"> [-<span class="number">1.5</span> -<span class="number">3.</span>   <span class="number">2.5</span>]</span><br><span class="line"> [ <span class="number">1.</span>   <span class="number">1.</span>  -<span class="number">1.</span> ]]</span><br></pre></td></tr></table></figure>

<h4 id="手动推算"><a href="#手动推算" class="headerlink" title="手动推算"></a>手动推算</h4><p><img src="/images/inverse_matrix.jpg" width="600px"></img></p>
<h3 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h3><p>&emsp;&emsp;<strong>范数（norm）</strong>用来衡量向量的大小，向量$\mathbb{L}^{p}$范数定义为：</p>
<center>$\left \| x \right \|_{p}=\left ( \sum_{i}\left | x_{i} \right |^{p} \right )^{\frac{1}{p}},p\in \mathbb{R},p\geqslant 1$</center></br>

<p>&emsp;&emsp;$L^{2}$范数，也称为欧几里得范数（Euclidean norm），是<strong>向量$x$到原点的欧几里得距离</strong>，$L^{2}$范数不一定适用于所有情况，当区别0和非常小但非0值的情况，$L^{1}$范数是一个比较好的选择。<br>&emsp;&emsp;$L^{1}$范数，在所有方向上的速率是一样的，定义为：</p>
<center>$\left \| x \right \|_{1}=\sum_{i}\left | x_{i} \right |$</center></br>

<p>&emsp;&emsp;<font color="#ff0000">经常用于<b>区分0</b>和<b>非0元素</b>的情形中。</font></br><br>&emsp;&emsp;$L^{0}$范数，可用于衡量向量中非0元素的个数，但它并不是一个范数。</p>
<p>&emsp;&emsp;$L^{\infty }$范数，向量元素绝对值的最大值，也叫做（Max norm）：</p>
<center>$\left \| x \right \|_{\infty }=\underset{i}{max}\left | x_{i} \right |$</center></br>
&emsp;&emsp;机器学习中常用的$F$范数（Frobenius norm），定义为：
<center>$\left \| A \right \|_{F}=\sqrt{\sum_{i,j}A_{i,j}^{2}}$</center>

<h4 id="程序实现-4"><a href="#程序实现-4" class="headerlink" title="程序实现"></a>程序实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 范数</span></span><br><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])  <span class="comment"># 向量a</span></span><br><span class="line">L1 = np.linalg.norm(a, <span class="built_in">ord</span>=<span class="number">1</span>) <span class="comment"># 向量1范数</span></span><br><span class="line">L2 = np.linalg.norm(a, <span class="built_in">ord</span>=<span class="number">2</span>) <span class="comment"># 向量2范数（欧几里得范数）</span></span><br><span class="line">Ln = np.linalg.norm(a, <span class="built_in">ord</span>=np.inf) <span class="comment"># 向量无穷范数（最大范数）</span></span><br><span class="line"></span><br><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]) <span class="comment"># 矩阵A</span></span><br><span class="line">A_f = np.linalg.norm(A, <span class="built_in">ord</span>=<span class="string">&quot;fro&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(L1)</span><br><span class="line"><span class="built_in">print</span>(L2)</span><br><span class="line"><span class="built_in">print</span>(Ln)</span><br><span class="line"><span class="built_in">print</span>(A_f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output：</span></span><br><span class="line"><span class="number">6.0</span></span><br><span class="line"><span class="number">3.7416573867739413</span></span><br><span class="line"><span class="number">3.0</span></span><br><span class="line"><span class="number">5.477225575051661</span></span><br></pre></td></tr></table></figure>

<h4 id="手动推算-1"><a href="#手动推算-1" class="headerlink" title="手动推算"></a>手动推算</h4><p><img src="/images/norm.jpg" width="400px"></img></p>
<h3 id="特殊类型的矩阵和向量"><a href="#特殊类型的矩阵和向量" class="headerlink" title="特殊类型的矩阵和向量"></a>特殊类型的矩阵和向量</h3><p>&emsp;&emsp;<strong>对角矩阵（diagonal matrix）</strong>：只在主对角线含有非零元素，其余位置为零。如：</p>
<p>$$\begin{bmatrix}<br>1 &amp; 0 &amp; 0\\<br>0 &amp; 2 &amp; 0\\<br>0 &amp; 0 &amp; 3<br>\end{bmatrix}$$</p>
<p>并非所有的对角矩阵都是方阵，长方形矩阵也可能为对角矩阵，但没有逆矩阵。</p>
<p>&emsp;&emsp;<strong>对称矩阵（symmetric matrix）</strong>：是转置和自己相等的矩阵。即：</p>
<center>$A=A^{\top}$</center></br>

<p>&emsp;&emsp;<strong>单位向量（unit ventor）</strong>：是具有<strong>单位范数（unit norm）</strong>的向量，即：</p>
<center>$\left \| x_{2} \right \|=1$</center></br>

<p>若$x^{\top}y&#x3D;0$，则向量$x$和向量$y$互相<strong>正交（orthogonal）</strong>。正交且范数为1，则称为<strong>标准正交</strong>，即：$A^{\top}A&#x3D;AA^{\top}&#x3D;I$，$A^{-1}&#x3D;A^{\top}$。</p>
<h3 id="特征分解"><a href="#特征分解" class="headerlink" title="特征分解"></a>特征分解</h3><p>&emsp;&emsp;<strong>矩阵分解（eigendecompostion）</strong>是使用最广的矩阵分解之一，即将矩阵分解成一组<strong>特征向量</strong>和<strong>特征值</strong>。</p>
<p>&emsp;&emsp;方阵$A$的<strong>特征向量（eigenvector）</strong>是指与$A$相乘后相当于对该向量进行缩放的非零向量$v$:</p>
<center>$Av=\lambda v$</center></br>

<p>&emsp;&emsp;其中标量$\lambda$称为这个特征向量对应的<strong>特征值（eigenvalue）</strong>。</p>
<p>&emsp;&emsp;如果一个$n\times n$矩阵$A$有$n$组线性无关的单位特征向量$\{ v^{(1)},…,v^{(n)} \}$，以及对应的特征值$\lambda _{1},…,\lambda _{n}$。将这些特征向量按列拼接成一个矩阵$V&#x3D;\left [ v^{(1)},…,v^{(n)} \right]$，并将对应的特征值拼接成一个向量：$\lambda&#x3D;\left [\lambda _{1},…,\lambda _{n}\right ]$。<br>&emsp;&emsp;$A$的特征值分解为：</p>
<center>$A=V_{diag}(\lambda)V^{-1}$</center></br>

<p>&emsp;&emsp;所有特征值都是正数的矩阵称为<strong>正定（positive definite）</strong>；所有特征值都是非负数的矩阵称为<strong>半正定（positive semidefinite）</strong>；所有特征值都是负数的矩阵称为<strong>负定（negative definite）</strong>；所有特征值都是非正数的矩阵称为<strong>半负定（negative semidefinite）</strong>；</p>
<p>注意：</p>
<ul>
<li>不是所有的矩阵都有特征分解。</li>
<li>在某些情况下，实矩阵的特征值分解可能会得到复矩阵。</li>
</ul>
<h4 id="程序实现-5"><a href="#程序实现-5" class="headerlink" title="程序实现"></a>程序实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征值和特征向量</span></span><br><span class="line">A=np.array([[-<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[-<span class="number">4</span>,<span class="number">3</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>]]) <span class="comment"># 矩阵A</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算特征值</span></span><br><span class="line">A_eig = np.linalg.eigvals(A)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算特征值和特征向量</span></span><br><span class="line">A_eig,A_eigvector = np.linalg.eig(A) </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征值：\n&quot;</span>, A_eig)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征向量：\n&quot;</span>, A_eigvector)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">特征值：</span><br><span class="line"> [<span class="number">2.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line">特征向量：</span><br><span class="line"> [[ <span class="number">0.</span>          <span class="number">0.40824829</span>  <span class="number">0.40824829</span>]</span><br><span class="line"> [ <span class="number">0.</span>          <span class="number">0.81649658</span>  <span class="number">0.81649658</span>]</span><br><span class="line"> [ <span class="number">1.</span>         -<span class="number">0.40824829</span> -<span class="number">0.40824829</span>]]</span><br></pre></td></tr></table></figure>

<h4 id="手动推算-2"><a href="#手动推算-2" class="headerlink" title="手动推算"></a>手动推算</h4><p><img src="/images/eigvector.jpg" width="600px"></img></p>
<h3 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h3><p>&emsp;&emsp;<strong>奇异值分解（singular value decomposition,SVD）</strong>将矩阵分解为<strong>奇异向量（singular vector）</strong>和<strong>奇异值（singular value）</strong>。与特征值分解相⽐，奇异值分解更加通⽤，所有的实矩阵都可以进⾏奇异值分解，⽽特征值分解只对某些⽅阵可以。</p>
<p>&emsp;&emsp;奇异值分解的形式为：</p>
<center>$A=UDV^{\top}$</center></br>

<p>&emsp;&emsp;若$A$是 $m \times n$ 的，那么 $U$ 是 $m \times m$ 的，其列向量称为左奇异向量，⽽ $V$ 是 $n \times n$ 的，其列向量称为右奇异向量，⽽ $D$ 是 $m \times n$ 的⼀个对⾓矩阵，其对⾓元素称为矩阵 $A$ 的奇异值。左奇异向量是 $AA^{\top}$ 的特征向量，⽽右奇异向量是 $A^{\top}A$ 的特征向量，⾮ 0 奇异值的平⽅是 $A^{\top}A$ 的⾮ 0 特征值。</p>
<h4 id="SVD分解矩阵A的步骤"><a href="#SVD分解矩阵A的步骤" class="headerlink" title="SVD分解矩阵A的步骤"></a>SVD分解矩阵A的步骤</h4><hr>
<ol>
<li>求$AA^{\top }$的特征值和特征向量，用单位化的特征向量构成$U$。</li>
<li>求$A^{\top }A$的特征值和特征向量，用单位化的特征向量构成$V$。</li>
<li>将$AA^{\top }$或者$A^{\top }A$的特征值求平方根，构成$D$。</li>
</ol>
<hr>
<h4 id="程序实现-6"><a href="#程序实现-6" class="headerlink" title="程序实现"></a>程序实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 奇异值分解</span></span><br><span class="line">A = np.array([[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>]]) <span class="comment"># 矩阵A</span></span><br><span class="line"></span><br><span class="line">U,D,V = np.linalg.svd(A)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵U:\n&quot;</span>, U)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵D:\n&quot;</span>, D)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵V:\n&quot;</span>, V)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">矩阵U:</span><br><span class="line"> [[-<span class="number">4.08248290e-01</span>  <span class="number">7.07106781e-01</span>  <span class="number">5.77350269e-01</span>]</span><br><span class="line"> [-<span class="number">8.16496581e-01</span>  <span class="number">2.64811510e-17</span> -<span class="number">5.77350269e-01</span>]</span><br><span class="line"> [-<span class="number">4.08248290e-01</span> -<span class="number">7.07106781e-01</span>  <span class="number">5.77350269e-01</span>]]</span><br><span class="line">矩阵D:</span><br><span class="line"> [<span class="number">1.73205081</span> <span class="number">1.</span>        ]</span><br><span class="line">矩阵V:</span><br><span class="line"> [[-<span class="number">0.70710678</span> -<span class="number">0.70710678</span>]</span><br><span class="line"> [-<span class="number">0.70710678</span>  <span class="number">0.70710678</span>]]</span><br></pre></td></tr></table></figure>

<h4 id="手动推算-3"><a href="#手动推算-3" class="headerlink" title="手动推算"></a>手动推算</h4><p><img src="/images/svd1.jpg" width="600px"></img><br><img src="/images/svd2.jpg" width="600px"></img></p>
<h3 id="迹运算"><a href="#迹运算" class="headerlink" title="迹运算"></a>迹运算</h3><p>&emsp;&emsp;<strong>迹运算</strong>返回的是矩阵对角元素的和：</p>
<center>$Tr(A)=\sum_{i}A_{i,j}$</center></br>

<h3 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h3><p>&emsp;&emsp;<strong>行列式</strong>，$det(A)$是将一个方阵$A$映射到实数的函数。行列式等于矩阵特征值的乘积。行列式的绝对值可以用来衡量矩阵参与矩阵乘法后空间扩大或者缩小了多少。</p>
<h3 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h3><p>&emsp;&emsp;花书中对于PCA的讲解不是很清晰，理论性很强，很难根据书中公式进行推导。于是查阅相关资料，总结出一套清晰的推导步骤：</p>
<h4 id="背景与作用"><a href="#背景与作用" class="headerlink" title="背景与作用"></a>背景与作用</h4><p>&emsp;&emsp;在研究与应用中，需要对收集的大量数据进行分析，随着数据集变量增多，且变量间可能存在相关性，便加大了问题分析的复杂性，如果对每个指标进行分析，往往分析是孤立的，不能完全利用数据中的信息，而盲目的减少指标会损失很多有用的信息，产生错误的结论。<br>&emsp;&emsp;因此，需要找到一种合理的方法，对指标进行降维，既要减少分析的指标，又要达到对所收集的数据进行全面分析的目的。降维算法如：<strong>奇异值分解(SVD)</strong> 、<strong>主成分分析(PCA)</strong> 、<strong>因子分析(FA)</strong> 、<strong>独立成分分析(ICA)</strong> 。<br>&emsp;&emsp;降维是一种对高维度特征数据预处理方法。是将高维度的数据保留下最重要的一些特征，去除噪声和不重要特征，实现提升数据处理速度的目的。在实际生产和应用中，降维在一定的信息损失范围内，可帮助我们节省大量时间和成本。降维的优点：</p>
<ul>
<li>使数据集更易使用</li>
<li>降低算法的计算开销</li>
<li>去除噪声</li>
<li>使得结果容易理解</li>
</ul>
<h4 id="PCA概念"><a href="#PCA概念" class="headerlink" title="PCA概念"></a>PCA概念</h4><p>&emsp;&emsp;PCA(Principal Component Analysis)，即主成分分析方法，是一种使用最广泛的数据降维算法。PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。</p>
<h4 id="PCA算法"><a href="#PCA算法" class="headerlink" title="PCA算法"></a>PCA算法</h4><hr>
<p><strong>输入：</strong> 样本集$D&#x3D;\left \{ x_{1},x_{2},…,x_{m} \right \}$ ; 低维空间维数$k$ 。<br><strong>过程：</strong></p>
<ol>
<li>对所有样本进行去中心化，即每一位特征减去各自的平均值：$x_{i}\leftarrow x_{i}-\frac{1}{m}\sum_{i&#x3D;1}^{m}x_{i}$ ;</li>
<li>计算样本的协方差矩阵：$XX^{\top}$ ;</li>
<li>对协方差矩阵$XX^{\top}$做特征值分解 ;</li>
<li>对特征值从大到小排序，取最大的$k$个所对应的特征向量$w_{1},w_{2},…,w_{k}$ 。<br><strong>输出：</strong> 投影矩阵$W&#x3D;(w_{1},w_{2},…,w_{k})$。</li>
</ol>
<hr>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python实现</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PCA</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, k</span>):</span><br><span class="line">        n_samples = np.shape(X)[<span class="number">0</span>]</span><br><span class="line">        covariance_matrix = (<span class="number">1</span> / (n_samples-<span class="number">1</span>)) * (X - X.mean(axis=<span class="number">0</span>)).T.dot(X - X.mean(axis=<span class="number">0</span>))</span><br><span class="line">        <span class="comment"># 对协方差矩阵进行特征值分解</span></span><br><span class="line">        eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)</span><br><span class="line">        <span class="comment"># 对特征值（特征向量）从大到小排序</span></span><br><span class="line">        idx = eigenvalues.argsort()[::-<span class="number">1</span>]</span><br><span class="line">        eigenvalues = eigenvalues[idx][:k]</span><br><span class="line">        eigenvectors = np.atleast_1d(eigenvectors[:, idx])[:, :k]</span><br><span class="line">        <span class="comment"># 得到低维表示</span></span><br><span class="line">        X_transformed = X.dot(eigenvectors)</span><br><span class="line">        <span class="keyword">return</span> X_transformed</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵X降到1维</span></span><br><span class="line">X = np.array([[-<span class="number">1</span>,-<span class="number">2</span>],[-<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">2</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>]]) <span class="comment"># 矩阵X</span></span><br><span class="line"></span><br><span class="line">model = PCA()</span><br><span class="line">Y = model.fit(X, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(Y)</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">[[-<span class="number">2.12132034</span>]</span><br><span class="line"> [-<span class="number">0.70710678</span>]</span><br><span class="line"> [ <span class="number">0.</span>        ]</span><br><span class="line"> [ <span class="number">2.12132034</span>]</span><br><span class="line"> [ <span class="number">0.70710678</span>]]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sklearn实现</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line">X = np.array([[-<span class="number">1</span>,-<span class="number">2</span>],[-<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">2</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>]]) <span class="comment"># 矩阵X</span></span><br><span class="line"></span><br><span class="line">pca=PCA(n_components=<span class="number">1</span>)</span><br><span class="line">pca.fit(X)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pca.transform(X))</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">[[ <span class="number">2.12132034</span>]</span><br><span class="line"> [ <span class="number">0.70710678</span>]</span><br><span class="line"> [ <span class="number">0.</span>        ]</span><br><span class="line"> [-<span class="number">2.12132034</span>]</span><br><span class="line"> [-<span class="number">0.70710678</span>]]</span><br></pre></td></tr></table></figure>

<p>sklearn对奇异值分解结果进行了处理，会导致PCA降维结果不一致，但都正确。</p>
<h4 id="手动推算-4"><a href="#手动推算-4" class="headerlink" title="手动推算"></a>手动推算</h4><p><img src="/images/pca1.jpg" width="600px"></img><br><img src="/images/pca2.jpg" width="600px"></img></p>
</div><script type="text/javascript" src="../../../../js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://deeplearner.top/2021/12/09/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" data-id="clkor69p20047c6s6b6bqbvhm" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAEmCAAAAADqr2IGAAAEi0lEQVR42u3ay27iQBAF0Pz/TzPSrAPcW2Uk2jleRQHs7uNF1+vnJ74e/6/Xnz77ziO4Xn/z2UqeffrsnsleVhcmTJgwYfpKptkGEtDXd5u9jAQ0eW353jFhwoQJ0z2Y8tslnybL3YQO7X3ykOLp/zFhwoQJ0x9gep1w5p8mIUJ+n/zOq9AEEyZMmDD9eaZ8ibMDPvlPfmHChAkTpr/GNLvdjKOFbpPwPBn+SC0cEyZMmDB9GVN7JN/p7w/ON2HChAkTpi9g2qSLycPakCJpi86ClTwI+OUpmDBhwoTpWKa2xdgeuvuBns03N682qhBgwoQJE6YDmfIjc7aZTSiwYdqk9JgwYcKE6VymvDXYbiP/7SxJbsd3NqNImDBhwoTpXKarjuo8Ac6HY2a/TQZ3cg5MmDBhwnQPpllyu1/QHvo1UFuALioEmDBhwoTpQKbZA/Lhnk1IMXsxs5btm1IvJkyYMGE6imnfPrxqjKZNjDcl6eQ+v9TCMWHChAnTjZjy43/WXGyXe1U7c9g6xYQJEyZMxzK1x3k77NKWVvfBQTuE1IYUmDBhwoTpdKY8QZ21Kjdp9ubNt4HIm4AAEyZMmDAdwtSmiPmx2qaULets5W1RGxMmTJgwnc7UFjrz+GIWIuRLz8vNeWvzza8wYcKECdOBTO3D2pS4DQg25eP9WE+UeWPChAkTpqOY2iM8ShFHT2zLwe0BP3vBv/wfEyZMmDAdxbQZkWnT5jYBnqXlq8MeEyZMmDDdlCk/sGcb3pSAk1CgfZF1aRgTJkyYMB3O1BZ59wf/rKDcpt/DgdSrauGYMGHChOkLmF4XOmd8s0LtnvWqJPnp7jBhwoQJ04FMs3GWhG/TvNyHHZv26psKASZMmDBhOpbpcwXZfBtJ0puHIEnIEu0FEyZMmDAdy5Q3Jh/llY/gJMf8ZiB10+D8ab+ECRMmTJi+jGn2sE1zMcGdhQKbcvMbMkyYMGHCdCxTXhJtS7SzVuUmxZ29jCRpx4QJEyZM92PKF50HEG35dVYCbtPgYiWYMGHChOlApnbz+bE6G6bZFJrbV5WXfTFhwoQJ0+lM15ZKZ99vW4+zUvJwnZgwYcKE6VimNhXMQ4F9i3QWWMxWFQVGmDBhwoTpQKb2FlHbL95MWw7Og4bZmNHT9WDChAkTplswtQftbNuz479F2Qz0RNNAmDBhwoTpcKa8STlLO5OwY5O+RptvawOYMGHChOlYpnxBybHdhgVtoXl2zLcJ/NP5JkyYMGHCdCxTnhLPCrtXFWRngUL++jFhwoQJ052Y9vXMywqpowQ1od+/QkyYMGHCdC7To7w2YzT7Yu6+pJsHAW8GdzBhwoQJ0yFMsyO5PfJnzcgZeh7QXNUQxYQJEyZM38/UBgFJqpkwbYqts+N/ld5jwoQJE6bDmdoiaR4WJMuatS3z9c9A68dgwoQJE6bDmWajPHm2naevbU6/KS5jwoQJEyZMbYl2liR/IiVO0mNMmDBhwnQPptmwzub79ZG8SMgvSKoxYcKECdOxTLPjc7aNzWZmbcv2iasLEyZMmDB9F9M/IwCgYVklin0AAAAASUVORK5CYII=">分享</a><div class="tags"><a href="../../../../tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><i class="fa fa-tag"></i>深度学习笔记</a><a href="../../../../tags/%E8%8A%B1%E4%B9%A6/"><i class="fa fa-tag"></i>花书</a></div><div class="post-nav"><a class="pre" href="../../22/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-%E6%A6%82%E7%8E%87%E4%B8%8E%E4%BF%A1%E6%81%AF%E8%AE%BA/">DeepLearning-3.概率与信息论</a><a class="next" href="../../08/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-0-%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7/">DeepLearning-0.数学符号</a></div><div class="nofancybox" id="waline"></div><script src="//unpkg.com/@waline/client@v2/dist/waline.js"></script><link rel="stylesheet" type="text/css" href="//unpkg.com/@waline/client@v2/dist/waline.css"><script>let metaInfo = ['nick', 'mail', 'link']
let requiredMeta = 'nick'.split(',').filter(item => {
  return metaInfo.indexOf(item) > -1
})
Waline.init({
  el: '#waline',
  comment: true,
  serverURL: 'https://waline-deeplearner.vercel.app',
  pageSize: '10',
  wordLimit: '500',
  requiredMeta,
})
</script></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/images/blog/avatar_new.png"/></a><p>心外无物，知行合一</p><a class="info-icon" href="/resume/" title="个人简历" target="_blank" style="margin-inline:5px"> <i class="fa fa-share-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:conghaoyuan@gmail.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/conghaoyuan" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/AIGC/">AIGC</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Algorithm/">Algorithm</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/CV/">CV</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/GUI/">GUI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/NLP/">NLP</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Python/">Python</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%97%E4%BA%BA/">数字人</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%A6/">数学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/">深度学习模型</a><span class="category-list-count">11</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/10/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8B%E9%80%9A%E4%B9%89Prompt%E5%9B%9B%E5%8D%81%E5%BC%8F/">AIGC-LLM-辟邪剑谱之通义Prompt四十式</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/06/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8BPrompt%20Engineering/">AIGC-LLM-辟邪剑谱之Prompt Engineering</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/11/18/AIGC-Agent-%E4%BD%9C%E6%96%87%E6%89%B9%E6%94%B9%E6%A0%87%E6%B3%A8/">AIGC-Agent-作文批改标注</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/09/27/LLama32%E8%B0%83%E7%A0%94/">LLama3.2调研报告</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/04/09/AIGC-LLM-Prompt%E6%8A%80%E8%83%BD/">AIGC-LLM-Prompt工程技能综述</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/03/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-BERT/">Deeeplearning模型-NLP-BERT</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/01/16/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E8%AF%AD%E6%96%99%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/">Deeeplearning模型-NLP-大语言模型微调语料生成工具</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-Transformer%E6%A8%A1%E5%9E%8B/">Deeeplearning模型-NLP-Transformer模型</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/01/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-NLP%E5%9F%BA%E7%A1%80/">Deeeplearning模型-NLP-NLP基础</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/10/26/%E6%95%B0%E5%AD%97%E4%BA%BA-%E6%95%B0%E5%AD%97%E4%BA%BA%E6%8A%80%E6%9C%AF%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94/">数字人-数字人技术效果对比</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://conghaoyuan.github.io" title="Fern[个人早期博客]" target="_blank">Fern[个人早期博客]</a></div><div class="widget"><div class="widget-title"><i class="fa fa-star"> 天文学</i></div><ul></ul><a href="/tools/tianwenli.html" title="天文历" target="_self">天文历</a></div><div class="widget"><div class="widget-title"><i class="fa fa-plus"> 数学小工具</i></div><ul></ul><a href="/tools/buy_house.html" title="丐版买房计算器" target="_self">丐版买房计算器</a><ul></ul><a href="/tools/equation_12.html" title="一元二次方程" target="_self">一元二次方程</a><ul></ul><a href="/tools/equation_line.html" title="直线方程" target="_self">直线方程</a><ul></ul><a href="/tools/equation_polynomial.html" title="多项式方程" target="_self">多项式方程</a><ul></ul><a href="/tools/regression_ridge.html" title="岭回归" target="_self">岭回归</a><ul></ul><a href="/tools/fourier_transform.html" title="傅里叶变换" target="_self">傅里叶变换</a><ul></ul><a href="/tools/gradient_descent.html" title="梯度下降" target="_self">梯度下降</a><ul></ul><a href="/tools/function_e.html" title="指数函数" target="_self">指数函数</a><ul></ul><a href="/tools/function_inverse_scale.html" title="反比例函数" target="_self">反比例函数</a><ul></ul><a href="/tools/function_log.html" title="对数函数" target="_self">对数函数</a><ul></ul><a href="/tools/function_sin.html" title="正弦函数" target="_self">正弦函数</a><ul></ul><a href="/tools/coord_grid.html" title="网格化坐标" target="_self">网格化坐标</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="../../../../." rel="nofollow">DeepLearner.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="../../../../js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="../../../../js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="../../../../css/search.css?v=1.0.0"><script type="text/javascript" src="../../../../js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="/static/js/MathJax.js?config=TeX-MML-AM_CHTML" async></script><div id="script" type="text/javascript" src="../../../../js/mathjaxs.js" async></div><script type="text/javascript" src="../../../../js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="../../../../js/smartresize.js?v=1.0.0"></script></div><!-- hexo injector body_end start -->
<script src="assets/prism-bundle.js"></script>
<script src="assets/prism-plus.js" data-pjax=""></script>
<!-- hexo injector body_end end --></body></html>