<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Just Do It"><title>AIGC-LLM-Prompt工程技能综述 | DeepLearner</title><link rel="stylesheet" type="text/css" href="../../../../css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/static/css/normalize.css"><link rel="stylesheet" type="text/css" href="/static/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/static/css/grids-responsive-min.css"><link rel="stylesheet" href="/static/css/font-awesome.min.css"><script type="text/javascript" src="/static/js/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="../../../../favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="../../../../favicon.ico"><link rel="apple-touch-icon" href="../../../../apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="../../../../apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="../../../../atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-83251621-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + 'b207d9febf3295d375e6e9b42006f9eb';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/static/js/clipboard.min.js"></script><script type="text/javascript" src="/static/js/toastr.min.js"></script><link rel="stylesheet" href="/static/css/toastr.min.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">AIGC-LLM-Prompt工程技能综述</h1><a id="logo" href="../../../../.">DeepLearner</a><p class="description">Deep Learning Notes</p></div><div id="nav-menu"><a class="current" href="../../../../."><i class="fa fa-home"> 首页</i></a><a href="../../../../archives/"><i class="fa fa-archive"> 归档</i></a><a href="../../../../tagcloud/"><i class="fa fa-tags"> 标签</i></a><a href="../../../../resource/"><i class="fa fa-cloud"> 资源</i></a><a href="../../../../guestbook/"><i class="fa fa-wechat"> 留言</i></a><a href="../../../../codeline/"><i class="fa fa-circle"> 码迹</i></a><a href="../../../../resume/"><i class="fa fa-user"> 个人简历</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">AIGC-LLM-Prompt工程技能综述</h1><div class="post-meta">created:2024-04-09</div><div class="post-meta">updated:2024-10-13<span> | </span><span class="category"><a href="../../../../categories/AIGC/">AIGC</a></span></div><a class="disqus-comment-count" href="#waline"><span class="waline-comment-count" id=""></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Zero-shot-Prompting"><span class="toc-number">1.</span> <span class="toc-text">Zero-shot Prompting</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%90%86%E8%A7%A3%E6%9C%AA%E8%A7%81%E5%91%BD%E4%BB%A4-Understanding-Unseen-Commands"><span class="toc-number">1.1.</span> <span class="toc-text">理解未见命令(Understanding Unseen Commands)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E8%AF%86%E5%88%AB-Image-Recognition"><span class="toc-number">1.2.</span> <span class="toc-text">图片识别(Image Recognition)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-Recommendation-Systems"><span class="toc-number">1.3.</span> <span class="toc-text">推荐系统(Recommendation Systems)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB-Text-Classification"><span class="toc-number">1.4.</span> <span class="toc-text">文本分类(Text Classification)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B7%A8%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3%E8%BD%AC%E6%8D%A2-Cross-lingual-Transfer"><span class="toc-number">1.5.</span> <span class="toc-text">跨语言理解转换(Cross-lingual Transfer)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Few-shot-Prompting"><span class="toc-number">2.</span> <span class="toc-text">Few-shot Prompting</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E7%BF%BB%E8%AF%91"><span class="toc-number">2.1.</span> <span class="toc-text">语言翻译</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90"><span class="toc-number">2.2.</span> <span class="toc-text">情感分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81"><span class="toc-number">2.3.</span> <span class="toc-text">文本摘要</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F"><span class="toc-number">2.4.</span> <span class="toc-text">问答系统</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E7%BC%96%E7%A8%8B%E4%BB%A3%E7%A0%81"><span class="toc-number">2.5.</span> <span class="toc-text">生成编程代码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Chain-of-Thought-CoT-Prompting"><span class="toc-number">3.</span> <span class="toc-text">Chain-of-Thought(CoT) Prompting</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Cot"><span class="toc-number">3.1.</span> <span class="toc-text">Cot</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Zero-Shot-CoT"><span class="toc-number">3.2.</span> <span class="toc-text">Zero-Shot CoT</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Auto-CoT"><span class="toc-number">3.3.</span> <span class="toc-text">Auto-CoT</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Active-Prompt"><span class="toc-number">4.</span> <span class="toc-text">Active-Prompt</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multimodal-CoT-Prompting"><span class="toc-number">5.</span> <span class="toc-text">Multimodal CoT Prompting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-Consistency-CoT"><span class="toc-number">6.</span> <span class="toc-text">Self-Consistency CoT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Generated-Knowledge-Prompting"><span class="toc-number">7.</span> <span class="toc-text">Generated Knowledge Prompting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Prompt-Chaining"><span class="toc-number">8.</span> <span class="toc-text">Prompt Chaining</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Multi-step"><span class="toc-number">8.1.</span> <span class="toc-text">Multi-step</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Validating-outputs"><span class="toc-number">8.2.</span> <span class="toc-text">Validating outputs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Parallel-processing"><span class="toc-number">8.3.</span> <span class="toc-text">Parallel processing</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tree-of-Thoughts-ToT"><span class="toc-number">9.</span> <span class="toc-text">Tree of Thoughts (ToT)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Retrieval-Augmented-Generation-RAG-Prompting"><span class="toc-number">10.</span> <span class="toc-text">Retrieval Augmented Generation (RAG) Prompting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Automatic-Reasoning-and-Tool-use-ART-Prompting"><span class="toc-number">11.</span> <span class="toc-text">Automatic Reasoning and Tool-use (ART) Prompting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Directional-Stimulus-Prompting"><span class="toc-number">12.</span> <span class="toc-text">Directional Stimulus Prompting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Program-Aided-Language-Models-PAL-Prompting"><span class="toc-number">13.</span> <span class="toc-text">Program-Aided Language Models(PAL) Prompting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReAct-Prompting"><span class="toc-number">14.</span> <span class="toc-text">ReAct Prompting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reflection"><span class="toc-number">15.</span> <span class="toc-text">Reflection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Useful-Prompt"><span class="toc-number">16.</span> <span class="toc-text">Useful Prompt</span></a></li></ol></div></div><div class="post-content"><p>最近研究在AGI方面投入精力较多，通过Workflow的方式，做了一些LLM+知识库+Tools的Agent。其中Workflow的实现，一种是通过Prompt来编排，另一种是LangGraph的方式。而Prompt书写质量的好坏，也严重影响了最后的效果。整理了当前主流的Prompt写法。其中包括一些Prompt示例，以及一些Prompting的思想。梳理、总结分析了20篇关于Prompt Engineering的论文。</p>
<span id="more"></span>

<h3 id="Zero-shot-Prompting"><a href="#Zero-shot-Prompting" class="headerlink" title="Zero-shot Prompting"></a>Zero-shot Prompting</h3><p>Zero-shot指的是在预训练和微调阶段，没有针对某个特定的任务进行训练，但依然在该任务上有准确的预测能力。说明模型的泛化能力、抽象推理能力、知识迁移能力、语义理解和高效的特征表示学习能力不错。指令微调可以提高模型的Zero-shot能力，论文参考<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.01652.pdf">《FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS》</a></p>
<p>Zero-shot可以处理以下任务：</p>
<h4 id="理解未见命令-Understanding-Unseen-Commands"><a href="#理解未见命令-Understanding-Unseen-Commands" class="headerlink" title="理解未见命令(Understanding Unseen Commands)"></a>理解未见命令(Understanding Unseen Commands)</h4><p>Understand and Execute commands it has never encountered during its training phase. </p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">任务： 解释一项模型未见过的运动规则。</span><br><span class="line"></span><br><span class="line">Prompt： 假设有一项运动叫做‘光球大战’，在这项运动中，两队通过控制一束光来得分。请解释这项运动的可能规则。</span><br></pre></td></tr></table></figure>

<h4 id="图片识别-Image-Recognition"><a href="#图片识别-Image-Recognition" class="headerlink" title="图片识别(Image Recognition)"></a>图片识别(Image Recognition)</h4><p>Identify objects or animals it has never seen before.</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">任务： 识别模型未曾见过的动物——“鹦鹉螺”。</span><br><span class="line"></span><br><span class="line">Prompt： 鹦鹉螺是一种具有螺旋壳和鲜艳颜色的海洋生物，常见于热带海域。根据这些信息，判断以下图片中是否存在鹦鹉螺，并说明理由。</span><br></pre></td></tr></table></figure>

<h4 id="推荐系统-Recommendation-Systems"><a href="#推荐系统-Recommendation-Systems" class="headerlink" title="推荐系统(Recommendation Systems)"></a>推荐系统(Recommendation Systems)</h4><p>Suggest items (movies, books, etc.) to users based on descriptions or attributes of the items, even if those specific items have never been rated before.</p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">任务： 推荐用户可能感兴趣的未评分的书籍。</span><br><span class="line"></span><br><span class="line">Prompt： 给定用户喜欢的是关于&#x27;未来科技和人工智能的科幻小说&#x27;，请推荐一本该用户可能感兴趣但系统中未有评分记录的书籍。</span><br></pre></td></tr></table></figure>

<h4 id="文本分类-Text-Classification"><a href="#文本分类-Text-Classification" class="headerlink" title="文本分类(Text Classification)"></a>文本分类(Text Classification)</h4><p>Categorize texts into topics or sentiments it has not been explicitly trained on.</p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">任务： 对于一个新的主题<span class="string">&quot;区块链技术&quot;</span>的文章进行分类。</span><br><span class="line"></span><br><span class="line">Prompt： 考虑以下段落是关于‘区块链技术’的概念解释、应用案例或对未来的预测。请将其分类为&#x27;技术解释&#x27;、&#x27;应用案例&#x27;或&#x27;未来预测&#x27;。</span><br></pre></td></tr></table></figure>

<h4 id="跨语言理解转换-Cross-lingual-Transfer"><a href="#跨语言理解转换-Cross-lingual-Transfer" class="headerlink" title="跨语言理解转换(Cross-lingual Transfer)"></a>跨语言理解转换(Cross-lingual Transfer)</h4><p>A model trained on tasks in one language (e.g., English) can perform the same task in another language (e.g., French) that it was not trained on. </p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">任务： 使用英语完成的模型解释一个中文概念。</span><br><span class="line"></span><br><span class="line">Prompt： <span class="string">&quot;八卦&quot;</span>在中文中是一个复杂的概念，涉及到宇宙哲学、自然科学以及人的命运。请用英语解释<span class="string">&quot;八卦&quot;</span>的含义及其在文化中的应用。</span><br></pre></td></tr></table></figure>

<h3 id="Few-shot-Prompting"><a href="#Few-shot-Prompting" class="headerlink" title="Few-shot Prompting"></a>Few-shot Prompting</h3><p>虽然大模型有不错的Zero-shot能力，但在更加复杂的任务上仍然不足。Few-shot Prompting可以用来支持大模型进行上下文学习，从而使模型具备更好的表现。Few-shot使用有限数量（10-100个）的标记数据作为上下文的信息，模型会从提供的示例中进行任务抽取，并用于新的、未见过的示例上，通常标记数据很少时可使用该技术。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.14165.pdf">《Language Models are Few-Shot Learners》</a>主要介绍了增加模型参数来提升Few-shot等场景下的性能。<br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.12837.pdf">《Rethinking the Role of Demonstrations:What Makes In-Context Learning Work?》</a> 介绍了Few-shot的一些使用技巧：</p>
<ul>
<li>示例中，标签空间（所有可能的标签）和输入文本的分布（即不同类型文本的出现频率或样式）都是非常重要的因素，但不受示例中标签是否准确的影响。<blockquote><p>the label space and the distribution of the input text specified by the demonstrations are both key to in-context learning (regardless of whether the labels are correct for individual inputs);</p>
</blockquote></li>
<li>明确整体的格式至关重要，如标签未知，即使用随机单词作为标签也比没有标签效果要好。<blockquote><p>specifying the overall format is also crucial, e.g., when the label space is unknown, using random English words as labels is significantly better than using no labels;</p>
</blockquote></li>
<li>带有上下文学习目标的meta-training放大了模型在演示方面的效果，比如格式，而不是输入标签的映射。这也是常见的模型幻觉问题，有时候过度依赖于上下文，而导致不能深入理解和预测，增加幻觉的产生。<blockquote><p>meta-training with an in-context learning objective magnifies these effects—the models almost exclusively exploit simpler aspects of the demonstrations like the format rather than the input-label mapping.</p>
</blockquote></li>
</ul>
<p>Few-shot可以处理以下任务：</p>
<h4 id="语言翻译"><a href="#语言翻译" class="headerlink" title="语言翻译"></a>语言翻译</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">任务： 将英文翻译成法文。</span><br><span class="line"></span><br><span class="line">Prompt：</span><br><span class="line">英文：<span class="selector-tag">I</span> want <span class="selector-tag">to</span> eat.</span><br><span class="line">法文：Je veux manger.</span><br><span class="line">英文：How are you?</span><br><span class="line">法文：Comment ç<span class="selector-tag">a</span> va?</span><br><span class="line">英文：It is <span class="selector-tag">a</span> beautiful day.</span><br><span class="line">法文：</span><br></pre></td></tr></table></figure>
<h4 id="情感分析"><a href="#情感分析" class="headerlink" title="情感分析"></a>情感分析</h4><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">任务： 分析客户评论的情绪倾向。</span><br><span class="line"></span><br><span class="line"><span class="variable">Prompt</span>：</span><br><span class="line">评论：<span class="variable">This</span> <span class="variable">product</span> <span class="variable">works</span> <span class="variable">great</span><span class="operator">!</span></span><br><span class="line">情感：<span class="built_in">Positive</span></span><br><span class="line">评论：<span class="built_in">I</span> <span class="variable">am</span> <span class="variable">disappointed</span> <span class="variable">with</span> <span class="variable">the</span> <span class="variable">service</span><span class="operator">.</span></span><br><span class="line">情感：<span class="built_in">Negative</span></span><br><span class="line">评论：<span class="variable">Their</span> <span class="variable">support</span> <span class="variable">team</span> <span class="variable">is</span> <span class="variable">very</span> <span class="variable">helpful</span><span class="operator">.</span></span><br><span class="line">情感：</span><br></pre></td></tr></table></figure>

<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">This product works great! <span class="comment">// Positive</span></span><br><span class="line">I am disappointed <span class="keyword">with</span> the service. <span class="comment">// Negative</span></span><br><span class="line">Their support team <span class="keyword">is</span> very helpful.</span><br></pre></td></tr></table></figure>

<h4 id="文本摘要"><a href="#文本摘要" class="headerlink" title="文本摘要"></a>文本摘要</h4><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">任务： 对新闻文章进行摘要。</span><br><span class="line"></span><br><span class="line">Prompt：</span><br><span class="line">文章：The company reported a <span class="number">20</span>% increase <span class="keyword">in</span> profit this quarter, attributing the growth <span class="keyword">to</span> an increase <span class="keyword">in</span> sales <span class="keyword">and</span> <span class="keyword">cost</span> reduction measures.</span><br><span class="line">摘要：Company profit grew <span class="number">20</span>% this quarter due <span class="keyword">to</span> higher sales <span class="keyword">and</span> <span class="keyword">cost</span> cutting.</span><br><span class="line">文章：Sacramento <span class="keyword">is</span> the wild card here, <span class="keyword">and</span> <span class="keyword">having</span> a final game against Portland <span class="keyword">on</span> Sunday could be the difference that keeps them <span class="keyword">on</span> the <span class="number">7</span><span class="number">-8</span> <span class="type">line</span> <span class="keyword">and</span> away <span class="keyword">from</span> the win-<span class="keyword">or</span>-go-home <span class="number">9</span><span class="number">-10</span> <span class="type">line</span>. </span><br><span class="line">摘要：</span><br></pre></td></tr></table></figure>

<h4 id="问答系统"><a href="#问答系统" class="headerlink" title="问答系统"></a>问答系统</h4><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">任务： 根据提供的信息回答问题。</span><br><span class="line"></span><br><span class="line">Prompt：</span><br><span class="line">文本：The Eiffel Tower is located <span class="keyword">in</span> Paris, France. It was constructed <span class="keyword">in</span> <span class="number">1889.</span></span><br><span class="line">问题：Where is <span class="keyword">the</span> Eiffel Tower located?</span><br><span class="line">答案：Paris, France</span><br><span class="line">文本：Alexander Hamilton was <span class="literal">one</span> <span class="keyword">of</span> <span class="keyword">the</span> founding fathers <span class="keyword">of</span> <span class="keyword">the</span> United States.</span><br><span class="line">问题：Who was Alexander Hamilton?</span><br><span class="line">答案：</span><br></pre></td></tr></table></figure>

<h4 id="生成编程代码"><a href="#生成编程代码" class="headerlink" title="生成编程代码"></a>生成编程代码</h4><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">任务： 根据需求描述生成简单的编程代码。</span><br><span class="line"></span><br><span class="line"><span class="title class_">Prompt</span>：</span><br><span class="line">描述：写一个函数，计算两个数字的和。</span><br><span class="line">代码：<span class="keyword">def</span> <span class="title function_">add_numbers</span>(<span class="params">a, b</span>): <span class="keyword">return</span> a + b</span><br><span class="line">描述：写一个函数，返回列表中的最大值。</span><br><span class="line">代码：</span><br></pre></td></tr></table></figure>

<h3 id="Chain-of-Thought-CoT-Prompting"><a href="#Chain-of-Thought-CoT-Prompting" class="headerlink" title="Chain-of-Thought(CoT) Prompting"></a>Chain-of-Thought(CoT) Prompting</h3><p>思维链（CoT）提示通过中间推理步骤实现复杂的推理能力。可以与Few-shot结合起来，在复杂任务推理中能获得更好的结果。</p>
<h4 id="Cot"><a href="#Cot" class="headerlink" title="Cot"></a>Cot</h4><p>论文<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.11903.pdf">《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》</a>：介绍了CoT，Paper Image:<br><img src="/images/prompt/prompt_cot.png" width="800px" /></p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Prompt</span>:</span><br><span class="line"><span class="attribute">The</span> odd numbers in this group add up to an even number: <span class="number">4</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">15</span>, <span class="number">12</span>, <span class="number">2</span>, <span class="number">1</span>.</span><br><span class="line"><span class="attribute">A</span>: Adding <span class="literal">all</span> the odd numbers (<span class="number">9</span>, <span class="number">15</span>, <span class="number">1</span>) gives <span class="number">25</span>. The answer is False.</span><br><span class="line"></span><br><span class="line"><span class="attribute">The</span> odd numbers in this group add up to an even number: <span class="number">15</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">13</span>, <span class="number">82</span>, <span class="number">7</span>, <span class="number">1</span>. </span><br><span class="line"><span class="attribute">A</span>:</span><br><span class="line"></span><br><span class="line"><span class="attribute">Output</span>:</span><br><span class="line"><span class="attribute">Adding</span> <span class="literal">all</span> the odd numbers (<span class="number">15</span>, <span class="number">5</span>, <span class="number">13</span>, <span class="number">7</span>, <span class="number">1</span>) gives <span class="number">41</span>. The answer is False.</span><br></pre></td></tr></table></figure>

<h4 id="Zero-Shot-CoT"><a href="#Zero-Shot-CoT" class="headerlink" title="Zero-Shot CoT"></a>Zero-Shot CoT</h4><p>论文<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.11916.pdf">《Large Language Models are Zero-Shot Reasoners》</a>：本质上是加了”<strong>Let’s think step by step</strong>“这句话，来让模型进一步思考。Paper Image:<br><img src="/images/prompt/prompt_cot_zero.png" width="800px" /></p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Prompt</span>:</span><br><span class="line"><span class="attribute">I</span> went to the market and bought <span class="number">10</span> apples. I gave <span class="number">2</span> apples to the neighbor and <span class="number">2</span> to the repairman. I then went and bought <span class="number">5</span> more apples and ate <span class="number">1</span>. How many apples did I remain with?</span><br><span class="line"></span><br><span class="line"><span class="attribute">Let</span>&#x27;s think step by step.</span><br><span class="line"></span><br><span class="line"><span class="attribute">Output</span>:</span><br><span class="line"><span class="attribute">First</span>, you started with <span class="number">10</span> apples.</span><br><span class="line"><span class="attribute">You</span> gave away <span class="number">2</span> apples to the neighbor and <span class="number">2</span> to the repairman, so you had <span class="number">6</span> apples left.</span><br><span class="line"><span class="attribute">Then</span> you bought <span class="number">5</span> more apples, so now you had <span class="number">11</span> apples.</span><br><span class="line"><span class="attribute">Finally</span>, you ate <span class="number">1</span> apple, so you would remain with <span class="number">10</span> apples.</span><br></pre></td></tr></table></figure>

<h4 id="Auto-CoT"><a href="#Auto-CoT" class="headerlink" title="Auto-CoT"></a>Auto-CoT</h4><p>论文<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2210.03493.pdf">《AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS》</a>：是对Zero-shot CoT的改进，认为Zero-shot CoT在每一步思维链中仍然会产生推理错误，而Manual CoT流程过于复杂并且手写成本巨大，而Auto-CoT是对不同的问题进行采样，并生成推理链来演示。主要有两步：</p>
<ul>
<li>Stage1: question clustering，聚类问题，将给定的问题划分为几个簇。</li>
<li>Stage2: demonstration sampling，示范抽样，从每个聚类簇中选择一个有代表性的问题，并使用简单的启发式算法生成推理链。</li>
</ul>
<blockquote><p>Auto-CoT consists of two main stages:<br>(i) question clustering: partition questions of a given dataset into a few clusters;<br>(ii) demonstration sampling: select a representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics.</p>
</blockquote>

<p>Paper Image:<br><img src="/images/prompt/prompt_cot_auto.png" width="800px" /></p>
<p>实验结果也表明，在10个公开的基准推理任务上，Auto-CoT与Manual-CoT相比，性能持平或超过，说明LLMs能够通过自动构建示例来执行CoT推理。</p>
<h3 id="Active-Prompt"><a href="#Active-Prompt" class="headerlink" title="Active-Prompt"></a>Active-Prompt</h3><p>论文<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.12246.pdf">《Active Prompting with Chain-of-Thought for Large Language Models》</a>：作者发现，LLM在某些特别任务上回答问题时，会存在不确定性。即生成的答案多样，不确定。为解决这一问题，<strong>通过主动学习的方式，从大量未标注的问题中选择最具有不确定性的问题进行人工注释，以此来构建有效的提示（prompt），从而提高大型语言模型（LLMs）解决复杂推理任务的能力。</strong></p>
<p>Paper Image:<br><img src="/images/prompt/prompt_cot_active.png" width="800px" /></p>
<p>步骤如上图所示，分四步：</p>
<ol>
<li>不确定性估计：让LLM尝试解决这个数据集中的问题，并观察它在哪些问题上表现得最不确定。比如，我们让模型尝试解答每个问题若干次，并记录下来它给出的不同答案。如果一个问题的答案在多次尝试中变化很大，那么我们认为模型在这个问题上的不确定性很高。</li>
<li>选择问题进行注释：根据不确定性的评估，选择一些最不确定的问题。</li>
<li>人工注释推理链：为这些选定的问题编写详细的推理链。</li>
<li>使用新的注释推理：再次使用LLM解答问题时，会在输入中包含这些人工注释的推理链示例。LLM在解答新的、未见过的问题时，就可以参考这些示例中的推理过程，从而提高解答问题的能力。</li>
</ol>
<h3 id="Multimodal-CoT-Prompting"><a href="#Multimodal-CoT-Prompting" class="headerlink" title="Multimodal CoT Prompting"></a>Multimodal CoT Prompting</h3><p>论文<a target="_blank" rel="noopener" href="https://www.promptingguide.ai/techniques">《Multimodal Chain-of-Thought Reasoning in Language Models》</a>，Paper Image: Multimodal CoT Task<br><img src="/images/prompt/prompt_cot_mul.png" width="600px" /></p>
<p>CoT比较注重语言的形式，处理文本比较有优势，Multimodal CoT主要是将文本和视觉结合到一个两阶段的框架中，两阶段共享想通的模型结构，但输入和输出不同。</p>
<ul>
<li>Stage1: (i) rationale generation，将输入问题在不同模态（文本和图像）下进行处理，生成中间的推理链；</li>
<li>Stage2: answer inference. 将这些推理链与原始问题一起用于推断最终的答案。</li>
</ul>
<blockquote><p>Both stages share the same model architecture but differ in the input and output. In the first stage, we feed the model with<br>language and vision inputs to generate rationales. In the second stage, we append the original language input with the rationale generated<br>from the first stage. Then, we feed the updated language input with the original vision input to the model to infer the answer.</p>
</blockquote>

<img src="/images/prompt/prompt_cot_mul1.png" width="800px" />

<h3 id="Self-Consistency-CoT"><a href="#Self-Consistency-CoT" class="headerlink" title="Self-Consistency CoT"></a>Self-Consistency CoT</h3><p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.11171.pdf">《SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS》</a><br>感觉没啥创意，核心思想就是先通过CoT的方式，生成多条推理链。再分别用多条推理链去去推理，选择概率较高的结果作为最终结果，虽然能提升准确率，但也比较浪费资源。</p>
<img src="/images/prompt/prompt_cot_self.png" width="800px" />

<blockquote><p>The self-consistency method contains three steps:<br>(1) prompt a language model usingchain-of-thought (CoT) prompting;<br>(2) replace the “greedy decode” in CoT prompting by sampling from the language model’s decoder to generate a diverse set of reasoning paths;<br>(3) marginalize out the reasoning paths and aggregate by choosing the most consistent answer in the final answer set.</p>
</blockquote>

<h3 id="Generated-Knowledge-Prompting"><a href="#Generated-Knowledge-Prompting" class="headerlink" title="Generated Knowledge Prompting"></a>Generated Knowledge Prompting</h3><p>论文<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.08387.pdf">《Generated Knowledge Prompting for Commonsense Reasoning》</a>，这篇论文比较早2022年的，其实后来的Few-shot Prompting跟此方法类似，或者说Few-shot Prompting包含了该Prompting的写法。核心思想就是在不需要访问结构化知识库或着进行特定任务微调的情况下，通过生成知识来提升大模型在常识推理任务上的性能。主要分两步：</p>
<ul>
<li>知识生成：使用语言模型根据问题生成知识声明，然后将这些知识作为输入提示与问题一起用于推断答案。这一步骤不需要任何结构化的知识库。</li>
<li>知识集成：生成的知识被整合到决策过程中，以增强语言模型用于推理的能力。这个过程通过提高零样本或微调模型在特定任务上的表现。</li>
</ul>
<img src="/images/prompt/prompt_knowledge.png" width="600px" />

<p>一个论文中的Prompt例子：</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Prompt：</span><br><span class="line"><span class="keyword">Generate</span> some knowledge <span class="keyword">about</span> the <span class="keyword">input</span>. Examples:</span><br><span class="line"><span class="keyword">Input</span>: What <span class="keyword">type</span> of water formation is formed <span class="keyword">by</span> clouds?</span><br><span class="line">Knowledge: Clouds are made of water vapor.</span><br><span class="line"><span class="keyword">Input</span>: What can prevent food spoilage?</span><br><span class="line">Knowledge: Dehydrating food is used <span class="keyword">for</span> preserving food.</span><br><span class="line"><span class="keyword">Input</span>: The process <span class="keyword">by</span> <span class="keyword">which</span> genes are passed is</span><br><span class="line">Knowledge: Genes are passed from parent to offspring.</span><br><span class="line"><span class="keyword">Input</span>: The stomach does what <span class="keyword">in</span> the body?</span><br><span class="line">Knowledge: The stomach is part of the digestive system.</span><br><span class="line"><span class="keyword">Input</span>: What can cause rocks to <span class="keyword">break</span> down?</span><br><span class="line">Knowledge: Mechanical weathering is when rocks are broken down <span class="keyword">by</span> mechanical <span class="keyword">means</span>.</span><br><span class="line"><span class="keyword">Input</span>: &#123;question&#125;</span><br><span class="line">Knowledge:</span><br></pre></td></tr></table></figure>

<h3 id="Prompt-Chaining"><a href="#Prompt-Chaining" class="headerlink" title="Prompt Chaining"></a>Prompt Chaining</h3><p>Prompt Chaining涉及使用一个提示的输出作为另一个提示的输入。通过链接提示在一起。通过引导LLMs一系列子任务，从而实现一个复杂的目标。这种Prompting的思想就是将任务拆分，每个子任务就是一个chain，在Langchain比较常见。</p>
<p>以下场景可以使用Prompt Chaining：</p>
<ol>
<li>Multi-step tasks: 如果任务需要多个不同的步骤，Workflow的方式，Chaining Prompts可以确保每一步执行完成后，完成一个复杂的任务。</li>
<li>Complex instructions: 指令比较负责，单个Prompt包含的指令和细节较多，LLMs难以始终如一的完成这些指令和细节。可以将任务拆解为一系列连锁的子任务，可以提高每个子任务的完成度。</li>
<li>Verifying outputs: 可以使用Chaining要求LLMs去重复验证其输出的结果，并可以要求对结果进行改进。</li>
<li>Parallel processing: 并行处理，如果任务有多个独立的子任务，可以为子任务创建单独的提示，并行运行。</li>
</ol>
<p>高效Prompt Chaining Tips：</p>
<ul>
<li>Keep subtasks simple and clear</li>
<li>Use XML tags</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://docs.anthropic.com/claude/docs/chain-prompts">https://docs.anthropic.com/claude/docs/chain-prompts</a></p>
</blockquote>
<h4 id="Multi-step"><a href="#Multi-step" class="headerlink" title="Multi-step"></a>Multi-step</h4><p>提取引用信息，一个task:</p>
<figure class="highlight handlebars"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml">Here is a document, in <span class="tag">&lt;<span class="name">document</span>&gt;</span><span class="tag">&lt;/<span class="name">document</span>&gt;</span> XML tags:</span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">document</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">&#123;&#123;<span class="name">DOCUMENT</span>&#125;&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">document</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">Please extract, word-for-word, any quotes relevant to the question </span><span class="template-variable">&#123;&#123;<span class="name">QUESTION</span>&#125;&#125;</span><span class="language-xml">. Please enclose the full list of quotes in <span class="tag">&lt;<span class="name">quotes</span>&gt;</span><span class="tag">&lt;/<span class="name">quotes</span>&gt;</span> XML tags. If there are no quotes in this document that seem relevant to this question, please say &quot;I can&#x27;t find any relevant quotes&quot;.</span></span><br></pre></td></tr></table></figure>

<p>将提取的引用信息结果，作为Knowledge，回答问题， 两个task：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">I want you <span class="keyword">to</span> use a<span class="built_in"> document</span> <span class="keyword">and</span> relevant quotes from the<span class="built_in"> document</span> <span class="keyword">to</span> answer a question.</span><br><span class="line"></span><br><span class="line">Here <span class="literal">is</span> the<span class="built_in"> document</span>:</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">document</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"></span><span class="language-xquery">&#123;&#123;DOCUMENT&#125;</span><span class="language-xml">&#125;</span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">document</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">Here are direct quotes from the<span class="built_in"> document</span> that are most relevant <span class="keyword">to</span> the question:</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">quotes</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"></span><span class="language-xquery">&#123;&#123;QUOTES&#125;</span><span class="language-xml">&#125;</span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">quotes</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">Please use these <span class="keyword">to</span> construct an answer <span class="keyword">to</span> the question <span class="string">&quot;&#123;&#123;QUESTION&#125;&#125;&quot;</span></span><br><span class="line"></span><br><span class="line">Ensure that your answer <span class="literal">is</span> accurate <span class="keyword">and</span> doesn<span class="string">&#x27;t contain any information not directly supported by the quotes.</span></span><br></pre></td></tr></table></figure>

<h4 id="Validating-outputs"><a href="#Validating-outputs" class="headerlink" title="Validating outputs"></a>Validating outputs</h4><p>比如一个task，先根据文章内容，来列举文章的语法错误：</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Here <span class="built_in">is</span> an article:</span><br><span class="line">&lt;article&gt;</span><br><span class="line">&#123;&#123;ARTICLE&#125;&#125;</span><br><span class="line">&lt;/article&gt;</span><br><span class="line"></span><br><span class="line">Please identify any grammatical <span class="built_in">errors</span> <span class="keyword">in</span> the article. Please only respond with the list of <span class="built_in">errors</span>, <span class="keyword">and</span> nothing <span class="keyword">else</span>. If there are no grammatical <span class="built_in">errors</span>, say <span class="string">&quot;There are no errors.&quot;</span></span><br></pre></td></tr></table></figure>

<p>两个task，分析文章的语法错误，与列举的语法Errors list作比较：</p>
<figure class="highlight handlebars"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml">Here is an article:</span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">article</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">&#123;&#123;<span class="name">ARTICLE</span>&#125;&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">article</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">Please identify any grammatical errors in the article that are missing from the following list:</span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">list</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">&#123;&#123;<span class="name">ERRORS</span>&#125;&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">list</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">If there are no errors in the article that are missing from the list, say &quot;There are no additional errors.&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="Parallel-processing"><a href="#Parallel-processing" class="headerlink" title="Parallel processing"></a>Parallel processing</h4><p>生成不同版本的文章：</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Here <span class="keyword">is</span> a concept: &#123;&#123;CONCEPT&#125;&#125;</span><br><span class="line"></span><br><span class="line">I want you <span class="keyword">to</span> <span class="built_in">write</span> a three sentence outline <span class="keyword">of</span> an essay <span class="keyword">about</span> this concept <span class="keyword">that</span> <span class="keyword">is</span> appropriate <span class="keyword">for</span> this level <span class="keyword">of</span> reader: &#123;&#123;LEVEL&#125;&#125;</span><br><span class="line"></span><br><span class="line">Please only respond <span class="keyword">with</span> your outline, one sentence per line, <span class="keyword">in</span> &lt;outline&gt;&lt;/outline&gt; XML tags. Don&#x27;t <span class="built_in">say</span> anything <span class="keyword">else</span>.</span><br></pre></td></tr></table></figure>

<h3 id="Tree-of-Thoughts-ToT"><a href="#Tree-of-Thoughts-ToT" class="headerlink" title="Tree of Thoughts (ToT)"></a>Tree of Thoughts (ToT)</h3><p>ToT的思想是模拟人的思考过程，构建思考树，来探索不同的解决方案路径。可以提高LLMs的能力，能更加有效的对复杂任务进行深入推理，找到更加合理的、有效的解决方案。</p>
<p>两篇论文，分别总结分析了ToT的思想，以及ToT区别于其他CoT在应用场景上的优势。<br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.08291.pdf">《Large Language Model Guided Tree-of-Thought》</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.10601.pdf">《Tree of Thoughts: Deliberate Problem Solving with Large Language Models》</a></p>
<p>ToT与CoT、CoT-SC的结构对比：<br><img src="/images/prompt/prompt_tot1.png" width="800px" /></p>
<p>ToT的思考过程，以及在System中的架构：<br><img src="/images/prompt/prompt_tot2.png" width="800px" /></p>
<p>ToT的流程非常有意思，可以结合BFS和DFS的算法来制定整个ToT的策略，当局部最优不等于全局最优时，可能会产生不同的结果。BFS的方案，认为是局部最优就决定了全局最优，会对每一步制定多个plan，再对每个plan的结果进行奇数次的选举，得票最高的plan会作为这一步的策略，再进行第二个任务的plan。直到最后，会生成一条plan链，推理出最终的结果。<br><img src="/images/prompt/prompt_tot_algo.png" width="800px" /><br><img src="/images/prompt/prompt_tot3.png" width="800px" /></p>
<p>论文官方代码：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/dave1010/tree-of-thought-prompting">https://github.com/dave1010/tree-of-thought-prompting</a></p>
</blockquote>
<p>但个人认为，ToT的这种策略在规定plan数量以及step数量时，不宜过长。plan为3就够了，step最好不超过5。</p>
<h3 id="Retrieval-Augmented-Generation-RAG-Prompting"><a href="#Retrieval-Augmented-Generation-RAG-Prompting" class="headerlink" title="Retrieval Augmented Generation (RAG) Prompting"></a>Retrieval Augmented Generation (RAG) Prompting</h3><p>RAG整体是一个研究领域，是比较有市场前景的，可以解决模型幻觉等一些问题，能提高输出的质量以及效果。</p>
<p>这里给出一篇RAG的综述：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.10997.pdf">《Retrieval-Augmented Generation for LargeLanguage Models: A Survey》</a>，分别分析了Naive RAG、Advanced RAG和Modular RAG的优劣势，有时间会专门写一篇RAG方向的文章。</p>
<p>RAG的一个基本流程：<br><img src="/images/prompt/prompt_rag_arch.png" width="800px" /></p>
<p>几种主流RAG方案的流程：<br><img src="/images/prompt/prompt_rag_diff.png" width="800px" /></p>
<p>文章同时介绍了，RAG与Fine-Tuning的区别：</p>
<ul>
<li>RAG在检索任务上，输出的信息会更加精准。</li>
<li>FT会让模型具备数据集的结构、风格等特点。</li>
</ul>
<blockquote>
<p> RAG can be likened to providing a model with a tailored textbook for information retrieval, ideal for precise information retrieval tasks. In contrast, FT is comparable to a student internalizing knowledge over time, suitable for scenarios requiring replication of specific structures, styles, or formats.</p>
</blockquote>
<p>比如Langchain中RAG的一个Prompt：</p>
<figure class="highlight handlebars"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml">You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don&#x27;t know the answer, just say that you don&#x27;t know. Use three sentences maximum and keep the answer concise.</span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">Question: </span><span class="template-variable">&#123;&#123;<span class="name">question</span>&#125;&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">Context: </span><span class="template-variable">&#123;&#123;<span class="name">context</span>&#125;&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">Answer:</span></span><br></pre></td></tr></table></figure>

<h3 id="Automatic-Reasoning-and-Tool-use-ART-Prompting"><a href="#Automatic-Reasoning-and-Tool-use-ART-Prompting" class="headerlink" title="Automatic Reasoning and Tool-use (ART) Prompting"></a>Automatic Reasoning and Tool-use (ART) Prompting</h3><p>本质上就是LLM+Tool相结合，目前可实现的框架很多，如Langchain，ChatGPT自带的Function Call等。</p>
<p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.09014.pdf">《ART: Automatic multi-step reasoning and tool-use for large language models》</a></p>
<img src="/images/prompt/prompt_art.png" width="800px" />

<p>Prompt如：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">You are <span class="keyword">an</span> assistant that <span class="built_in">has</span> access <span class="keyword">to</span> the following <span class="keyword">set</span> of tools. Here are the names <span class="built_in">and</span> descriptions <span class="keyword">for</span> each tool:</span><br><span class="line"></span><br><span class="line">&#123;tools&#125;</span><br><span class="line"></span><br><span class="line">Given the user <span class="built_in">input</span>, <span class="keyword">return</span> the name <span class="built_in">and</span> <span class="built_in">input</span> of the tool <span class="keyword">to</span> use. Return your response <span class="keyword">as</span> <span class="keyword">a</span> JSON blob with <span class="string">&#x27;name&#x27;</span> <span class="built_in">and</span> <span class="string">&#x27;arguments&#x27;</span> <span class="built_in">keys</span>.</span><br></pre></td></tr></table></figure>

<p>Langchain实现Tool，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain.tools.render <span class="keyword">import</span> render_text_description</span><br><span class="line"><span class="keyword">from</span> langchain_core.tools <span class="keyword">import</span> tool</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>(<span class="params">first_int: <span class="built_in">int</span>, second_int: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multiply two integers together.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> first_int * second_int</span><br><span class="line"></span><br><span class="line">rendered_tools = render_text_description([multiply])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">system_prompt = <span class="string">f&quot;&quot;&quot;You are an assistant that has access to the following set of tools. Here are the names and descriptions for each tool:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"><span class="subst">&#123;rendered_tools&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Given the user input, return the name and input of the tool to use. Return your response as a JSON blob with &#x27;name&#x27; and &#x27;arguments&#x27; keys.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">    [(<span class="string">&quot;system&quot;</span>, system_prompt), (<span class="string">&quot;user&quot;</span>, <span class="string">&quot;&#123;input&#125;&quot;</span>)]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>其他Tool实现：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/use_cases/tool_use/">https://python.langchain.com/docs/use_cases/tool_use/</a></p>
</blockquote>
<h3 id="Directional-Stimulus-Prompting"><a href="#Directional-Stimulus-Prompting" class="headerlink" title="Directional Stimulus Prompting"></a>Directional Stimulus Prompting</h3><p>论文<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.11520.pdf">《Guiding Large Language Models via Directional Stimulus Prompting》</a></p>
<p>大致的思想就是，通过一些实体标签，来约束住LLMs要回答的范围以及内容。从而起到对人工控制关键信息，生成高质量摘要的目的。</p>
<img src="/images/prompt/prompt_dmp.png" width="800px" />

<h3 id="Program-Aided-Language-Models-PAL-Prompting"><a href="#Program-Aided-Language-Models-PAL-Prompting" class="headerlink" title="Program-Aided Language Models(PAL) Prompting"></a>Program-Aided Language Models(PAL) Prompting</h3><p>论文<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.10435.pdf">《PAL: Program-aided Language Models》</a><br>论文比较早了，2022年发表。本质上还是一种CoT，而且这种Coding的任务现在用LLMs+Tool，挂一个解释器会更准确。所以可能在实际应用场景中，作用不大。但是论文的思想还是值得借鉴的。比如对Programing任务进行拆解，通过CoT的方式最终给出答案。 可以用于苏格拉底式的启发式编程使用。</p>
<img src="/images/prompt/prompt_pal.png" width="800px" />

<h3 id="ReAct-Prompting"><a href="#ReAct-Prompting" class="headerlink" title="ReAct Prompting"></a>ReAct Prompting</h3><p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2210.03629.pdf">《REAC T: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS》</a><br>核心思想还是通过提出多个thought-action-observation的步骤，用于解决任务轨迹。<br>其实就是针对某一个任务，按照thought-action-observation这一套流程，步骤，分别执行了n次，然后统计n次执行后的结果，输出最优解。复杂度也挺高，但思想比较简单。</p>
<h3 id="Reflection"><a href="#Reflection" class="headerlink" title="Reflection"></a>Reflection</h3><p>Reflection这种Prompting思想，我认为非常有用。会对AGI的技术发展起到关键作用。相关论文：</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.11366.pdf">《Reflexion: Language Agents with Verbal Reinforcement Learning》</a></p>
<p>步骤：</p>
<ol>
<li>生成轨迹</li>
<li>评估</li>
<li>执行反思</li>
<li>生成下个轨迹</li>
</ol>
<img src="/images/prompt/prompt_reflection.png" width="500px" />

<p>如在decision-making, programming, and reasoning方面的Reflection过程。<br><img src="/images/prompt/prompt_reflection2.png" width="800px" /></p>
<p>代码：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/noahshinn/reflexion">https://github.com/noahshinn/reflexion</a></p>
</blockquote>
<p>其中在西部小镇的项目中，创建的25个智能体，都具备了Reflection的能力。<br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.03442v1.pdf">《Generative Agents: Interactive Simulacra of Human Behavior》</a></p>
<img src="/images/prompt/prompt_agi.png" width="800px" />
<img src="/images/prompt/prompt_agi2.png" width="800px" />

<p>Klaus这个Agent的反思树，观察-&gt;反思-&gt;观察-&gt;反思</p>
<blockquote>
<p>A reflection tree for Klaus Mueller. The agent’s observations of the world, represented in the leaf nodes, are recursively synthesized to derive Klaus’s self-notion that he is highly dedicated to his research.</p>
</blockquote>
<h3 id="Useful-Prompt"><a href="#Useful-Prompt" class="headerlink" title="Useful Prompt"></a>Useful Prompt</h3><ul>
<li>Let’s think step by step.</li>
<li>We should think about this step by step. </li>
<li>First, </li>
<li>Before we dive into the answer, </li>
<li>Proof followed by the answer. </li>
<li>Let’s think step by step in a realistic way.</li>
<li>Let’s think step by step using common sense and knowledge. </li>
<li>Let’s think like a detective step by step. </li>
<li>Let’s think about this logically.</li>
<li>Let’s think step by step. First, </li>
<li>Let’s think</li>
<li>Let’s solve this problem by splitting it into steps. </li>
<li>The answer is after the proof. </li>
<li>Let’s be realistic and think step by step. </li>
<li>APE: Let’s work this out in a step by step way to be<br>sure we have the right answer</li>
<li>Now, please write…, following the formatting of the examples above.</li>
</ul>
<p>Directory structure reference<a target="_blank" rel="noopener" href="https://www.promptingguide.ai/techniques">1</a></p>
<hr>
</div><script type="text/javascript" src="../../../../js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://deeplearner.top/2024/04/09/AIGC-LLM-Prompt%E6%8A%80%E8%83%BD/" data-id="clusmmuv80001o8fyfpng1vl1" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACvElEQVR42u3aQW7cMAwF0Nz/0lOgqxaNlU9SzKTA0yowxrKeFhJD8uMjHq/f4+nJ64/x72/O7+ZzPr11YeDh4eG1lv408g9X50kY53nO38LDw8Pb5iXHdPXIjj4/uIqSt/Dw8PDeyzsvrhc6V68HPDw8vP+LlyQFqhdA/i08PDy8n8ZLkgjn4Di/PHophvVcCx4eHl7M2z64N/5erO/h4eHhDarqk3EuX+UphtEa8PDw8BZ4vfTBXjL3bkoXDw8Pb4OXH9x5M9Z5Efnvq9vxyRM8PDy8b+TlKdpqu1V1g6oX0mMwjYeHh7fAS2LRyRKrJbSkrNX8XwEPDw9vwDuXuJKX8wXl8+eXUKF1AA8PD+8qb76slaO8lZUdvYaHh4cX8Jr7ESdqq8f6vKiGh4eHt807XwDVlG6+6Gqr1koyAg8PD2+Qxu2V6nvXQPXCqF4qfz3Bw8PDW+DlAXc1KE8w1QasvBEBDw8Pb5s3aajKw/Q8WO8V5KICGB4eHt4CL2+xmrRP9RIW1UAcDw8Pb4/XSyUkAXQvFO61FxRmwMPDwxvwklRC9WP50X8OnfOr6IssNR4eHt5VXl5wuhXO3i1rRWvGw8PDu8rLD+XqgV6tvOVvFbYeDw8Pb5k3GXnxLN+avAj3OAMeHh7eAq8aKFfbCyaBeC/Vi4eHh/edvPO/93lzwKQ81kttNJsG8PDw8Aa8alCbHM2TLaimQkYVOTw8PLwBb35J9Mr/+WY1LXh4eHhXea/i6H0+T3PkT8o9EXh4eHiXeL0Dt1rmL+RCWpfT/Ct4eHh4VV7eMtVrC6huWTVpi4eHh/cuXvVQ7rVG9cppeRr6MSWBh4eH92N4T0uZb02+cV9cLXh4eHhv5U1aV/MG1l4LAh4eHt738/IpqundXhlscp3g4eHhbfPuFqKqm5gkLzZSGHh4eHgt3i+fql6TZiZEfwAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="../../../../tags/AIGC/"><i class="fa fa-tag"></i>AIGC</a><a href="../../../../tags/LLM/"><i class="fa fa-tag"></i>LLM</a><a href="../../../../tags/Prompt/"><i class="fa fa-tag"></i>Prompt</a></div><div class="post-nav"><a class="pre" href="../../../09/27/LLama32%E8%B0%83%E7%A0%94/">LLama3.2调研报告</a><a class="next" href="../../../03/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-BERT/">Deeeplearning模型-NLP-BERT</a></div><div class="nofancybox" id="waline"></div><script src="//unpkg.com/@waline/client@v2/dist/waline.js"></script><link rel="stylesheet" type="text/css" href="//unpkg.com/@waline/client@v2/dist/waline.css"><script>let metaInfo = ['nick', 'mail', 'link']
let requiredMeta = 'nick'.split(',').filter(item => {
  return metaInfo.indexOf(item) > -1
})
Waline.init({
  el: '#waline',
  comment: true,
  serverURL: 'https://waline-deeplearner.vercel.app',
  pageSize: '10',
  wordLimit: '500',
  requiredMeta,
})
</script></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/images/blog/avatar_new.png"/></a><p>心外无物，知行合一</p><a class="info-icon" href="/resume/" title="个人简历" target="_blank" style="margin-inline:5px"> <i class="fa fa-share-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:conghaoyuan@gmail.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/conghaoyuan" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/AIGC/">AIGC</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Algorithm/">Algorithm</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/CV/">CV</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/GUI/">GUI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/NLP/">NLP</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Python/">Python</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%97%E4%BA%BA/">数字人</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%A6/">数学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/">深度学习模型</a><span class="category-list-count">11</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/06/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8BPrompt%20Engineering/">AIGC-LLM-辟邪剑谱之Prompt Engineering</a></li><li class="post-list-item"><a class="post-list-link" href="../../../11/18/AIGC-Agent-%E4%BD%9C%E6%96%87%E6%89%B9%E6%94%B9%E6%A0%87%E6%B3%A8/">AIGC-Agent-作文批改标注</a></li><li class="post-list-item"><a class="post-list-link" href="../../../09/27/LLama32%E8%B0%83%E7%A0%94/">LLama3.2调研报告</a></li><li class="post-list-item"><a class="post-list-link" href="">AIGC-LLM-Prompt工程技能综述</a></li><li class="post-list-item"><a class="post-list-link" href="../../../03/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-BERT/">Deeeplearning模型-NLP-BERT</a></li><li class="post-list-item"><a class="post-list-link" href="../../../01/16/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E8%AF%AD%E6%96%99%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/">Deeeplearning模型-NLP-大语言模型微调语料生成工具</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-Transformer%E6%A8%A1%E5%9E%8B/">Deeeplearning模型-NLP-Transformer模型</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/01/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-NLP%E5%9F%BA%E7%A1%80/">Deeeplearning模型-NLP-NLP基础</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/10/26/%E6%95%B0%E5%AD%97%E4%BA%BA-%E6%95%B0%E5%AD%97%E4%BA%BA%E6%8A%80%E6%9C%AF%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94/">数字人-数字人技术效果对比</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/10/13/GUI-%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E6%89%93%E5%8C%85%E4%B8%8E%E5%AE%89%E8%A3%85/">GUI-应用程序打包与安装</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://conghaoyuan.github.io" title="Fern[个人早期博客]" target="_blank">Fern[个人早期博客]</a></div><div class="widget"><div class="widget-title"><i class="fa fa-star"> 天文学</i></div><ul></ul><a href="/tools/tianwenli.html" title="天文历" target="_self">天文历</a></div><div class="widget"><div class="widget-title"><i class="fa fa-plus"> 数学小工具</i></div><ul></ul><a href="/tools/buy_house.html" title="丐版买房计算器" target="_self">丐版买房计算器</a><ul></ul><a href="/tools/equation_12.html" title="一元二次方程" target="_self">一元二次方程</a><ul></ul><a href="/tools/equation_line.html" title="直线方程" target="_self">直线方程</a><ul></ul><a href="/tools/equation_polynomial.html" title="多项式方程" target="_self">多项式方程</a><ul></ul><a href="/tools/regression_ridge.html" title="岭回归" target="_self">岭回归</a><ul></ul><a href="/tools/fourier_transform.html" title="傅里叶变换" target="_self">傅里叶变换</a><ul></ul><a href="/tools/gradient_descent.html" title="梯度下降" target="_self">梯度下降</a><ul></ul><a href="/tools/function_e.html" title="指数函数" target="_self">指数函数</a><ul></ul><a href="/tools/function_inverse_scale.html" title="反比例函数" target="_self">反比例函数</a><ul></ul><a href="/tools/function_log.html" title="对数函数" target="_self">对数函数</a><ul></ul><a href="/tools/function_sin.html" title="正弦函数" target="_self">正弦函数</a><ul></ul><a href="/tools/coord_grid.html" title="网格化坐标" target="_self">网格化坐标</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="../../../../." rel="nofollow">DeepLearner.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="../../../../js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="../../../../js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="../../../../css/search.css?v=1.0.0"><script type="text/javascript" src="../../../../js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="/static/js/MathJax.js?config=TeX-MML-AM_CHTML" async></script><div id="script" type="text/javascript" src="../../../../js/mathjaxs.js" async></div><script type="text/javascript" src="../../../../js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="../../../../js/smartresize.js?v=1.0.0"></script></div><!-- hexo injector body_end start -->
<script src="assets/prism-bundle.js"></script>
<script src="assets/prism-plus.js" data-pjax=""></script>
<!-- hexo injector body_end end --></body></html>