<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Just Do It"><title>Deeeplearning模型-NLP-Transformer模型 | DeepLearner</title><link rel="stylesheet" type="text/css" href="../../../../css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/static/css/normalize.css"><link rel="stylesheet" type="text/css" href="/static/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/static/css/grids-responsive-min.css"><link rel="stylesheet" href="/static/css/font-awesome.min.css"><script type="text/javascript" src="/static/js/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="../../../../favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="../../../../favicon.ico"><link rel="apple-touch-icon" href="../../../../apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="../../../../apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="../../../../atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-83251621-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + 'b207d9febf3295d375e6e9b42006f9eb';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/static/js/clipboard.min.js"></script><script type="text/javascript" src="/static/js/toastr.min.js"></script><link rel="stylesheet" href="/static/css/toastr.min.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Deeeplearning模型-NLP-Transformer模型</h1><a id="logo" href="../../../../.">DeepLearner</a><p class="description">Deep Learning Notes</p></div><div id="nav-menu"><a class="current" href="../../../../."><i class="fa fa-home"> 首页</i></a><a href="../../../../archives/"><i class="fa fa-archive"> 归档</i></a><a href="../../../../tagcloud/"><i class="fa fa-tags"> 标签</i></a><a href="../../../../resource/"><i class="fa fa-cloud"> 资源</i></a><a href="../../../../guestbook/"><i class="fa fa-wechat"> 留言</i></a><a href="../../../../codeline/"><i class="fa fa-circle"> 码迹</i></a><a href="../../../../resume/"><i class="fa fa-user"> 个人简历</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Deeeplearning模型-NLP-Transformer模型</h1><div class="post-meta">created:2023-12-05</div><div class="post-meta">updated:2024-03-02<span> | </span><span class="category"><a href="../../../../categories/NLP/">NLP</a></span></div><a class="disqus-comment-count" href="#waline"><span class="waline-comment-count" id=""></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer-Arch"><span class="toc-number">1.</span> <span class="toc-text">Transformer Arch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention-Multi-Head-Attention-Arch"><span class="toc-number">2.</span> <span class="toc-text">Attention &amp; Multi-Head Attention Arch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention-Multi-Head-Attention-Functions"><span class="toc-number">3.</span> <span class="toc-text">Attention &amp; Multi-Head Attention Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder-Block-Arch-Calc-Process"><span class="toc-number">4.</span> <span class="toc-text">Encoder Block Arch Calc Process</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AD%97%E5%90%91%E9%87%8F%E4%B8%8E%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">4.1.</span> <span class="toc-text">1. 字向量与位置编码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">4.2.</span> <span class="toc-text">2. 自注意力机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Self-Attention%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8ELayer-Normalization"><span class="toc-number">4.3.</span> <span class="toc-text">3. Self-Attention残差连接与Layer Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-FeedForward"><span class="toc-number">4.4.</span> <span class="toc-text">4. FeedForward</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-FeedForward%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8ELayer-Normalization"><span class="toc-number">4.5.</span> <span class="toc-text">5. FeedForward残差连接与Layer Normalization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder-Block-Arch-Calc-Process"><span class="toc-number">5.</span> <span class="toc-text">Decoder Block Arch Calc Process</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AD%97%E5%90%91%E9%87%8F%E4%B8%8E%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-1"><span class="toc-number">5.1.</span> <span class="toc-text">1. 字向量与位置编码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%B8%A6Masked%E7%9A%84%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">5.2.</span> <span class="toc-text">2. 带Masked的自注意力机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Self-Attention%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8ELayer-Normalization-1"><span class="toc-number">5.3.</span> <span class="toc-text">3. Self-Attention残差连接与Layer Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-Multi-Head-Encoder-Decoder-Attention%E4%B8%8EFeedForward"><span class="toc-number">5.4.</span> <span class="toc-text">4. Multi-Head Encoder-Decoder Attention与FeedForward</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-FeedForward%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8ELayer-Normalization-1"><span class="toc-number">5.5.</span> <span class="toc-text">5. FeedForward残差连接与Layer Normalization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">6.</span> <span class="toc-text">代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Multi-Head-Attention"><span class="toc-number">6.1.</span> <span class="toc-text">Multi-Head Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Encoder"><span class="toc-number">6.2.</span> <span class="toc-text">Encoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Decoder"><span class="toc-number">6.3.</span> <span class="toc-text">Decoder</span></a></li></ol></li></ol></div></div><div class="post-content"><p>Transformer是2017年Google论文《Attention is All You Need》提出的一种模型结构，颠覆了通过RNN进行序列建模的思路，已被广泛应用于NLP各个领域，如出色的Bert、GPT都是基于Transformer模型。</p>
<p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All Your Need</a></p>
<span id="more"></span>

<img src="/images/nlp/transformer_architectures.png" alt="Transformer家族" width="400px"/>

<h3 id="Transformer-Arch"><a href="#Transformer-Arch" class="headerlink" title="Transformer Arch"></a>Transformer Arch</h3><table>
<tr align="center">
<td><img src="/images/nlp/transformer_arch.png" width="500px"/></td>
<td><img src="/images/nlp/transformer_arch1.png" width="650px"/></td>
</tr>
</table>

<h3 id="Attention-Multi-Head-Attention-Arch"><a href="#Attention-Multi-Head-Attention-Arch" class="headerlink" title="Attention &amp; Multi-Head Attention Arch"></a>Attention &amp; Multi-Head Attention Arch</h3><img src="/images/nlp/attention.png" alt="Attention & Multi-Attention" width="600px"/>

<img src="/images/nlp/attention1.png" alt="Attention & Multi-Attention Contact" width="600px"/>

<h3 id="Attention-Multi-Head-Attention-Functions"><a href="#Attention-Multi-Head-Attention-Functions" class="headerlink" title="Attention &amp; Multi-Head Attention Functions"></a>Attention &amp; Multi-Head Attention Functions</h3><p>$$ Attention(Q, K, V) &#x3D; softmax \left( \frac{Q K^T}{\sqrt{d_k}} \right)V $$</p>
<p>$$<br>\begin{matrix}<br> MultiHead(Q, K, V) &#x3D; Concat(head_1, …, head_h)W^O \\<br>\text{ where }head_i &#x3D; Attention(QW_{i}^{Q}, KW_{i}^{K}, VW_{i}^{V})<br>\end{matrix}<br>$$</p>
<h3 id="Encoder-Block-Arch-Calc-Process"><a href="#Encoder-Block-Arch-Calc-Process" class="headerlink" title="Encoder Block Arch Calc Process"></a>Encoder Block Arch Calc Process</h3><img src="/images/nlp/transformer_encoder.png" width="600px"/>

<h4 id="1-字向量与位置编码"><a href="#1-字向量与位置编码" class="headerlink" title="1. 字向量与位置编码"></a>1. 字向量与位置编码</h4><p>$$ X &#x3D; Embedding-Lookup(X) + Positional-Encoding $$</p>
<h4 id="2-自注意力机制"><a href="#2-自注意力机制" class="headerlink" title="2. 自注意力机制"></a>2. 自注意力机制</h4><p>$$<br>\begin{matrix}<br>Q &#x3D; Linear_q(X) &#x3D; XW_Q \\<br>K &#x3D; Linear_k(X) &#x3D; XW_K \\<br>V &#x3D; Linear_v(X) &#x3D; XW_V \\<br>X_{attention} &#x3D; \text{Self-Attention}(Q, K, V)<br>\end{matrix}<br>$$</p>
<h4 id="3-Self-Attention残差连接与Layer-Normalization"><a href="#3-Self-Attention残差连接与Layer-Normalization" class="headerlink" title="3. Self-Attention残差连接与Layer Normalization"></a>3. Self-Attention残差连接与Layer Normalization</h4><p>$$<br>\begin{matrix}<br>X_{attention} &#x3D; X + X_{attention} \\<br>X_{attention} &#x3D; LayerNorm(X_{attention})<br>\end{matrix}<br>$$</p>
<h4 id="4-FeedForward"><a href="#4-FeedForward" class="headerlink" title="4. FeedForward"></a>4. FeedForward</h4><p>结构中的第4部分，两层线性变换并用激活函数ReLU。</p>
<p>$$ X_{hidden} &#x3D; Linear(ReLU(Linear(X_{attention}))) $$</p>
<h4 id="5-FeedForward残差连接与Layer-Normalization"><a href="#5-FeedForward残差连接与Layer-Normalization" class="headerlink" title="5. FeedForward残差连接与Layer Normalization"></a>5. FeedForward残差连接与Layer Normalization</h4><p>$$<br>\begin{matrix}<br>X_{hidden} &#x3D; X_{attention} + X_{hidden} \\<br>X_{hidden} &#x3D; LayerNorm(X_{hidden})<br>\end{matrix}<br>$$</p>
<p>其中，$$ X_{hidden}\in \mathbb{R}^{batch\_size \ * \ seq\_len \ * \ embd\_dim} $$</p>
<h3 id="Decoder-Block-Arch-Calc-Process"><a href="#Decoder-Block-Arch-Calc-Process" class="headerlink" title="Decoder Block Arch Calc Process"></a>Decoder Block Arch Calc Process</h3><img src="/images/nlp/transformer_decoder.png" width="600px"/>

<h4 id="1-字向量与位置编码-1"><a href="#1-字向量与位置编码-1" class="headerlink" title="1. 字向量与位置编码"></a>1. 字向量与位置编码</h4><p>$$ X &#x3D; Embedding-Lookup(X) + Positional-Encoding $$</p>
<h4 id="2-带Masked的自注意力机制"><a href="#2-带Masked的自注意力机制" class="headerlink" title="2. 带Masked的自注意力机制"></a>2. 带Masked的自注意力机制</h4><p>$$ Self-Attention(Q, K, V) &#x3D; softmax \left( \frac{Q K^T}{\sqrt{d_k}} + Mask \right)V $$</p>
<p>Mask setting to $ - \propto $ all values.</p>
<p>$$<br>\begin{matrix}<br>Q &#x3D; Linear_q(X) &#x3D; XW_Q \\<br>K &#x3D; Linear_k(X) &#x3D; XW_K \\<br>V &#x3D; Linear_v(X) &#x3D; XW_V \\<br>X_{attention} &#x3D; \text{Self-Attention}(Q, K, V)<br>\end{matrix}<br>$$</p>
<h4 id="3-Self-Attention残差连接与Layer-Normalization-1"><a href="#3-Self-Attention残差连接与Layer-Normalization-1" class="headerlink" title="3. Self-Attention残差连接与Layer Normalization"></a>3. Self-Attention残差连接与Layer Normalization</h4><p>$$<br>\begin{matrix}<br>X_{attention} &#x3D; X + X_{attention} \\<br>X_{attention} &#x3D; LayerNorm(X_{attention})<br>\end{matrix}<br>$$</p>
<h4 id="4-Multi-Head-Encoder-Decoder-Attention与FeedForward"><a href="#4-Multi-Head-Encoder-Decoder-Attention与FeedForward" class="headerlink" title="4. Multi-Head Encoder-Decoder Attention与FeedForward"></a>4. Multi-Head Encoder-Decoder Attention与FeedForward</h4><p>与Encoder的Multi-Head Attention计算一样，只不过K，V为Encoder的输出。Q为Masked Self-Attention的输出。</p>
<p>$$ Attention(Q, K, V) &#x3D; softmax \left( \frac{Q K^T}{\sqrt{d_k}} \right)V $$</p>
<p>$$<br>\begin{matrix}<br>Q &#x3D; X_{attention} \\<br>K &#x3D; X_{k-hidden} \\<br>V &#x3D; V_{k-hidden} \\<br>X_{second-attention} &#x3D; \text{Attention}(Q, K, V) \\<br>X_{second-attention} &#x3D; X_{second-attention} + X_{attention} \\<br>X_{second-attention} &#x3D; LayerNorm(X_{second-attention})<br>\end{matrix}<br>$$</p>
<h4 id="5-FeedForward残差连接与Layer-Normalization-1"><a href="#5-FeedForward残差连接与Layer-Normalization-1" class="headerlink" title="5. FeedForward残差连接与Layer Normalization"></a>5. FeedForward残差连接与Layer Normalization</h4><p>$$<br>\begin{matrix}<br>X_{second-hidden} &#x3D; X_{second-attention} + X_{attention} \\<br>X_{second-hidden} &#x3D; LayerNorm(X_{second-hidden})<br>\end{matrix}<br>$$</p>
<p>其中，$$ X_{second-hidden}\in \mathbb{R}^{batch\_size \ * \ seq\_len \ * \ embd\_dim} $$</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, emb_size, q_k_size, v_size, head</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.emb_size = emb_size</span><br><span class="line">        self.q_k_size = q_k_size</span><br><span class="line">        self.v_size = v_size</span><br><span class="line">        self.head = head</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 多头</span></span><br><span class="line">        self.w_q = nn.Linear(emb_size, head * q_k_size)</span><br><span class="line">        self.w_k = nn.Linear(emb_size, head * q_k_size)</span><br><span class="line">        self.w_v = nn.Linear(emb_size, head * v_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_q, x_k_v, attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        forward</span></span><br><span class="line"><span class="string">        :param x_q: (batch_size, seq_len, emb_size)</span></span><br><span class="line"><span class="string">        :param x_k_v:</span></span><br><span class="line"><span class="string">        :param attn_mask:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        q = self.w_q(x_q)   <span class="comment"># q: (batch_size, seq_len, head*q_k_size)</span></span><br><span class="line">        k = self.w_k(x_k_v) <span class="comment"># k: (batch_size, seq_len, head*q_k_size)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 多头兼容</span></span><br><span class="line">        <span class="comment"># q: (batch_size, head, seq_len, q_k_size)</span></span><br><span class="line">        q = q.view(q.size()[<span class="number">0</span>], q.size()[<span class="number">1</span>], self.head, self.q_k_size).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># k: (batch_size, head, q_k_size, seq_len)</span></span><br><span class="line">        k = k.view(k.size()[<span class="number">0</span>], k.size()[<span class="number">1</span>], self.head, self.q_k_size).transpose(<span class="number">1</span>, <span class="number">2</span>).transpose(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 注意力矩阵</span></span><br><span class="line">        <span class="comment"># (batch_size, head, seq_len, seq_len) row是q, col是k</span></span><br><span class="line">        attn = torch.matmul(q, k) / math.sqrt(self.q_k_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 注意力分值处理</span></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).expand(-<span class="number">1</span>, self.head, -<span class="number">1</span>, -<span class="number">1</span>)   <span class="comment"># attn_mask: (batch_size, head, seq_len, seq_len)</span></span><br><span class="line">        attn_mask = attn_mask.to(torch.<span class="built_in">bool</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;attn_mask1:&quot;</span>, attn_mask)</span><br><span class="line">        attn = attn.masked_fill(attn_mask, -<span class="number">1e9</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;attn:&quot;</span>, attn)</span><br><span class="line">        attn = torch.softmax(attn, dim=-<span class="number">1</span>)  <span class="comment"># scores: (batch_size, head, seq_len, seq_len)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 注意力与V相乘</span></span><br><span class="line">        v = self.w_v(x_k_v) <span class="comment"># v: (batch_size, seq_len, head*v_size)</span></span><br><span class="line">        v = v.view(v.size()[<span class="number">0</span>], v.size()[<span class="number">1</span>], self.head, self.v_size).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># v: (batch_size, head, seq_len, v_size)</span></span><br><span class="line">        z = torch.matmul(attn, v) <span class="comment"># z: (batch_size, head, seq_len, v_size)</span></span><br><span class="line">        z = z.transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># z: (batch_size, seq_len, head, v_size)</span></span><br><span class="line">        <span class="keyword">return</span> z.reshape(z.size()[<span class="number">0</span>], z.size()[<span class="number">1</span>], -<span class="number">1</span>)  <span class="comment"># z: (batch_size, seq_len, head * v_size)</span></span><br></pre></td></tr></table></figure>

<h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderBlock</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, emb_size, q_k_size, v_size, f_size, head</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 多头注意力</span></span><br><span class="line">        self.multihead_attn = MultiHeadAttention(emb_size, q_k_size, v_size, head)</span><br><span class="line">        <span class="comment"># 调整多头输出尺寸为emb_size</span></span><br><span class="line">        self.z_linear = nn.Linear(head * v_size, emb_size)</span><br><span class="line">        <span class="comment"># 按last dim做norm</span></span><br><span class="line">        self.addnorm1 = nn.LayerNorm(emb_size)</span><br><span class="line">        <span class="comment"># feed-forward结构</span></span><br><span class="line">        self.feedforward = nn.Sequential(nn.Linear(emb_size, f_size), nn.ReLU(), nn.Linear(f_size, emb_size))</span><br><span class="line">        <span class="comment"># 按last dim做norm</span></span><br><span class="line">        self.addnorm2 = nn.LayerNorm(emb_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        forward</span></span><br><span class="line"><span class="string">        :param x:   (batch_size, seq_len, emb_size)</span></span><br><span class="line"><span class="string">        :param attn_mask:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        z = self.multihead_attn(x, x, attn_mask)    <span class="comment"># z: (batch_size, seq_len, head*v_size)</span></span><br><span class="line">        z = self.z_linear(z)    <span class="comment"># z: (batch_size, seq_len, emb_size)</span></span><br><span class="line">        output1 = self.addnorm1(z + x)  <span class="comment"># z: (batch_size, seq_len, emb_size)</span></span><br><span class="line"></span><br><span class="line">        z = self.feedforward(output1)   <span class="comment"># z: (batch_size, seq_len, emb_size)</span></span><br><span class="line">        <span class="keyword">return</span> self.addnorm2(z + output1)   <span class="comment"># z: (batch_size, seq_len, emb_size)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, emb_size, q_k_size, v_size, f_size, head, nblocks, dropout=<span class="number">0.1</span>, seq_max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.emb = EmbeddingWithPosition(vocab_size, emb_size, dropout, seq_max_len)</span><br><span class="line"></span><br><span class="line">        self.encoder_blocks = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(nblocks):</span><br><span class="line">            self.encoder_blocks.append(EncoderBlock(emb_size, q_k_size, v_size, f_size, head))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):   <span class="comment"># x: (batch_size, seq_len)</span></span><br><span class="line">        pad_mask = (x == PAD_IDX).unsqueeze(<span class="number">1</span>)  <span class="comment"># pad_mask: (batch_size, 1, seq_len)</span></span><br><span class="line">        pad_mask = pad_mask.expand(x.size()[<span class="number">0</span>], x.size()[<span class="number">1</span>], x.size()[<span class="number">1</span>])   <span class="comment"># pad_mask:(batch_size, seq_len, seq_len)</span></span><br><span class="line"></span><br><span class="line">        pad_mask = pad_mask.to(DEVICE)</span><br><span class="line"></span><br><span class="line">        x = self.emb(x)</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.encoder_blocks:</span><br><span class="line">            x = block(x, pad_mask)  <span class="comment">#x :(batch_size, seq_len, emb_size)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, emb_size, q_k_size, v_size, f_size, head</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 第1个多头注意力</span></span><br><span class="line">        self.first_multihead_attn = MultiHeadAttention(emb_size, q_k_size, v_size, head)</span><br><span class="line">        self.z_linear1 = nn.Linear(head * v_size, emb_size)</span><br><span class="line">        self.addnorm1 = nn.LayerNorm(emb_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第2个多头注意力</span></span><br><span class="line">        self.second_multihead_attn = MultiHeadAttention(emb_size, q_k_size, v_size, head)</span><br><span class="line">        self.z_linear2 = nn.Linear(head * v_size, emb_size)</span><br><span class="line">        self.addnorm2 = nn.LayerNorm(emb_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># feed-forward结构</span></span><br><span class="line">        self.feedforward = nn.Sequential(nn.Linear(emb_size, f_size), nn.ReLU(), nn.Linear(f_size, emb_size))</span><br><span class="line">        self.addnorm3 = nn.LayerNorm(emb_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, encoder_z, first_attn_mask, second_attn_mask</span>):</span><br><span class="line">        <span class="comment"># 第1个多头</span></span><br><span class="line">        z = self.first_multihead_attn(x, x, first_attn_mask)</span><br><span class="line">        z = self.z_linear1(z)</span><br><span class="line">        output1 = self.addnorm1(z + x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第2个多头</span></span><br><span class="line">        z = self.second_multihead_attn(output1, encoder_z, second_attn_mask)</span><br><span class="line">        z = self.z_linear2(z)</span><br><span class="line">        output2 = self.addnorm2(z + output1)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最后feed forward</span></span><br><span class="line">        z = self.feedforward(output2)</span><br><span class="line">        <span class="keyword">return</span> self.addnorm3(z + output2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, emb_size, q_k_size, v_size, f_size, head, nblocks, dropout=<span class="number">0.1</span>, seq_mex_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.emb = EmbeddingWithPosition(vocab_size, emb_size, dropout, seq_mex_len)</span><br><span class="line"></span><br><span class="line">        self.decoder_blocks = nn.ModuleList()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(nblocks):</span><br><span class="line">            self.decoder_blocks.append(DecoderBlock(emb_size, q_k_size, v_size, f_size, head))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出向量词概率logits</span></span><br><span class="line">        self.linear = nn.Linear(emb_size, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, encoder_z, encoder_x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        forward</span></span><br><span class="line"><span class="string">        :param x: (batch_size, seq_len)</span></span><br><span class="line"><span class="string">        :param encoder_z:</span></span><br><span class="line"><span class="string">        :param encoder_x:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 目标序列的pad掩码</span></span><br><span class="line">        first_attn_mask = (x == PAD_IDX).unsqueeze(<span class="number">1</span>).expand(x.size()[<span class="number">0</span>], x.size()[<span class="number">1</span>], x.size()[<span class="number">1</span>]).to(DEVICE)</span><br><span class="line">        <span class="comment"># 目标序列的向后看掩码</span></span><br><span class="line">        first_attn_mask = first_attn_mask | torch.triu(torch.ones(x.size()[<span class="number">1</span>], x.size()[<span class="number">1</span>]), diagonal=<span class="number">1</span>).<span class="built_in">bool</span>().unsqueeze(<span class="number">0</span>).expand(x.size()[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>).to(DEVICE)</span><br><span class="line">        <span class="comment"># 根据来源序列的pad掩码，遮盖decoder对其pad部分的注意力</span></span><br><span class="line">        <span class="comment"># (batch_size, target_len, src_len)</span></span><br><span class="line">        second_attn_mask = (encoder_x == PAD_IDX).unsqueeze(<span class="number">1</span>).expand(encoder_x.size()[<span class="number">0</span>], x.size()[<span class="number">1</span>], encoder_x.size()[<span class="number">1</span>]).to(DEVICE)</span><br><span class="line"></span><br><span class="line">        x = self.emb(x)</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.decoder_blocks:</span><br><span class="line">            x = block(x, encoder_z, first_attn_mask, second_attn_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.linear(x)   <span class="comment"># (batch_size, target_len, vocab_size)</span></span><br></pre></td></tr></table></figure>


</div><script type="text/javascript" src="../../../../js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://deeplearner.top/2023/12/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-Transformer%E6%A8%A1%E5%9E%8B/" data-id="clr50jvwv000drw3e7v05091c" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADKElEQVR42u3aQY7bQAwEwP3/px0gpwCB7W5SCaxx6WR4vdLU6EAMmz8/8fX4ff39+dk3r79/fZ8/r2QNr5+yurCxsbFvwn68vGbUZInPfvka/3qDEgs2Njb2qeykaM2+abejLWn5fbCxsbGx2+KRLD0/lrQbjY2NjY3dlqtk0flxou0FYWNjY2PPSkVbqBJMHkskxfWCXho2Njb2x7M3Qe+nff5P+TY2Njb2x7Af5ZU0gK5tFeWRQ6HAxsbGPoi9GcfZh76zQc/ZSorZT2xsbOxbsdsHtAHwLBKu31vw8p6WLmxsbOwj2Hv8fvRnExK3/4WNjY19HnvTiN+P+7T4CyJnbGxs7OPYeVCa/DLfpryYtXd4U/CwsbGxD2K30yz5cvPINhmRbIto25DCxsbGvjs7P2DkRWt/h1lZKqJlbGxs7CPYeYmatW82o5abu73ZFGxsbOzj2PmxYdZ4au+zGf2JXgk2Njb2oew9Y1ZgZq2ivHS9KWDY2NjYB7FnBaxt6O8PIZvtw8bGxj6VnYesdfIQHznamHazZmxsbOyT2LMgdj9Y2Y74zAaGNsEzNjY29l3Yq7j00uHL2bOGa8PGxsb+MnbbAGrx7SEkHx5aTSphY2Nj34q9CVNnhWoT7m6G8i/LkLGxsbE/mH3V0OS10UI+jhNtPTY2NvZx7FlrJm8kzcLjdkjognwbGxsb+4bszXL3A5rJc2cjQcV/YWNjYx/ETprssyHIzVbmWzP7KzY2NvYZ7E0AkMcJs4ZUW97qQwg2Njb2zdn5aOO/Ky1t+cxjgKdPxMbGxj6I3Q5Htp2ZWXC7OdgUsQE2Njb2Eew84t038WdR7uw3Ud3GxsbGPoidBLdtwdgcP4bDN/nWYGNjYx/H3jRuorNOuTXtUaduMGFjY2MfxJ51oWYHmGQ7ki3eHIRqPDY2NvZN2O3wzSxOuGoop40Nnq4QGxsb+zj2pqGft41mBW/TWnqTb2NjY2N/GXtGbUcz98+KmlzY2NjYX8/O2z0zdjsuXwcD2NjY2Mex2yNBvtB9JJAch+q2FDY2NvZB7NUg4+jgcVWcnL82bGxs7EPZvwCGR9SYH3cRMQAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="../../../../tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><i class="fa fa-tag"></i>深度学习</a><a href="../../../../tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"><i class="fa fa-tag"></i>深度学习模型</a><a href="../../../../tags/LLM/"><i class="fa fa-tag"></i>LLM</a><a href="../../../../tags/NLP/"><i class="fa fa-tag"></i>NLP</a><a href="../../../../tags/Transformer/"><i class="fa fa-tag"></i>Transformer</a><a href="../../../../tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><i class="fa fa-tag"></i>大模型</a></div><div class="post-nav"><a class="pre" href="../../../../2024/01/16/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E8%AF%AD%E6%96%99%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/">Deeeplearning模型-NLP-大语言模型微调语料生成工具</a><a class="next" href="../../01/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-NLP%E5%9F%BA%E7%A1%80/">Deeeplearning模型-NLP-NLP基础</a></div><div class="nofancybox" id="waline"></div><script src="//unpkg.com/@waline/client@v2/dist/waline.js"></script><link rel="stylesheet" type="text/css" href="//unpkg.com/@waline/client@v2/dist/waline.css"><script>let metaInfo = ['nick', 'mail', 'link']
let requiredMeta = 'nick'.split(',').filter(item => {
  return metaInfo.indexOf(item) > -1
})
Waline.init({
  el: '#waline',
  comment: true,
  serverURL: 'https://waline-deeplearner.vercel.app',
  pageSize: '10',
  wordLimit: '500',
  requiredMeta,
})
</script></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/images/blog/avatar_new.png"/></a><p>心外无物，知行合一</p><a class="info-icon" href="/resume/" title="个人简历" target="_blank" style="margin-inline:5px"> <i class="fa fa-share-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:conghaoyuan@gmail.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/conghaoyuan" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/AIGC/">AIGC</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Algorithm/">Algorithm</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/CV/">CV</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/GUI/">GUI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/NLP/">NLP</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Python/">Python</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%97%E4%BA%BA/">数字人</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%A6/">数学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/">深度学习模型</a><span class="category-list-count">11</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/10/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8B%E9%80%9A%E4%B9%89Prompt%E5%9B%9B%E5%8D%81%E5%BC%8F/">AIGC-LLM-辟邪剑谱之通义Prompt四十式</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/06/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8BPrompt%20Engineering/">AIGC-LLM-辟邪剑谱之Prompt Engineering</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/11/18/AIGC-Agent-%E4%BD%9C%E6%96%87%E6%89%B9%E6%94%B9%E6%A0%87%E6%B3%A8/">AIGC-Agent-作文批改标注</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/09/27/LLama32%E8%B0%83%E7%A0%94/">LLama3.2调研报告</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/04/09/AIGC-LLM-Prompt%E6%8A%80%E8%83%BD/">AIGC-LLM-Prompt工程技能综述</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/03/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-BERT/">Deeeplearning模型-NLP-BERT</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/01/16/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E8%AF%AD%E6%96%99%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/">Deeeplearning模型-NLP-大语言模型微调语料生成工具</a></li><li class="post-list-item"><a class="post-list-link" href="">Deeeplearning模型-NLP-Transformer模型</a></li><li class="post-list-item"><a class="post-list-link" href="../../01/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-NLP%E5%9F%BA%E7%A1%80/">Deeeplearning模型-NLP-NLP基础</a></li><li class="post-list-item"><a class="post-list-link" href="../../../10/26/%E6%95%B0%E5%AD%97%E4%BA%BA-%E6%95%B0%E5%AD%97%E4%BA%BA%E6%8A%80%E6%9C%AF%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94/">数字人-数字人技术效果对比</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://conghaoyuan.github.io" title="Fern[个人早期博客]" target="_blank">Fern[个人早期博客]</a></div><div class="widget"><div class="widget-title"><i class="fa fa-star"> 天文学</i></div><ul></ul><a href="/tools/tianwenli.html" title="天文历" target="_self">天文历</a></div><div class="widget"><div class="widget-title"><i class="fa fa-plus"> 数学小工具</i></div><ul></ul><a href="/tools/buy_house.html" title="丐版买房计算器" target="_self">丐版买房计算器</a><ul></ul><a href="/tools/equation_12.html" title="一元二次方程" target="_self">一元二次方程</a><ul></ul><a href="/tools/equation_line.html" title="直线方程" target="_self">直线方程</a><ul></ul><a href="/tools/equation_polynomial.html" title="多项式方程" target="_self">多项式方程</a><ul></ul><a href="/tools/regression_ridge.html" title="岭回归" target="_self">岭回归</a><ul></ul><a href="/tools/fourier_transform.html" title="傅里叶变换" target="_self">傅里叶变换</a><ul></ul><a href="/tools/gradient_descent.html" title="梯度下降" target="_self">梯度下降</a><ul></ul><a href="/tools/function_e.html" title="指数函数" target="_self">指数函数</a><ul></ul><a href="/tools/function_inverse_scale.html" title="反比例函数" target="_self">反比例函数</a><ul></ul><a href="/tools/function_log.html" title="对数函数" target="_self">对数函数</a><ul></ul><a href="/tools/function_sin.html" title="正弦函数" target="_self">正弦函数</a><ul></ul><a href="/tools/coord_grid.html" title="网格化坐标" target="_self">网格化坐标</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="../../../../." rel="nofollow">DeepLearner.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="../../../../js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="../../../../js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="../../../../css/search.css?v=1.0.0"><script type="text/javascript" src="../../../../js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="/static/js/MathJax.js?config=TeX-MML-AM_CHTML" async></script><div id="script" type="text/javascript" src="../../../../js/mathjaxs.js" async></div><script type="text/javascript" src="../../../../js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="../../../../js/smartresize.js?v=1.0.0"></script></div><!-- hexo injector body_end start -->
<script src="assets/prism-bundle.js"></script>
<script src="assets/prism-plus.js" data-pjax=""></script>
<!-- hexo injector body_end end --></body></html>