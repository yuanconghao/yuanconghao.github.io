<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Just Do It"><title>资源 | DeepLearner</title><link rel="stylesheet" type="text/css" href="../css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/static/css/normalize.css"><link rel="stylesheet" type="text/css" href="/static/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/static/css/grids-responsive-min.css"><link rel="stylesheet" href="/static/css/font-awesome.min.css"><script type="text/javascript" src="/static/js/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="../favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="../favicon.ico"><link rel="apple-touch-icon" href="../apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="../apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="../atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-83251621-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + 'b207d9febf3295d375e6e9b42006f9eb';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/static/js/clipboard.min.js"></script><script type="text/javascript" src="/static/js/toastr.min.js"></script><link rel="stylesheet" href="/static/css/toastr.min.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">资源</h1><a id="logo" href="../.">DeepLearner</a><p class="description">Deep Learning Notes</p></div><div id="nav-menu"><a href="../."><i class="fa fa-home"> 首页</i></a><a href="../archives/"><i class="fa fa-archive"> 归档</i></a><a href="../tagcloud/"><i class="fa fa-tags"> 标签</i></a><a class="current" href=""><i class="fa fa-cloud"> 资源</i></a><a href="../guestbook/"><i class="fa fa-wechat"> 留言</i></a><a href="../codeline/"><i class="fa fa-circle"> 码迹</i></a><a href="../resume/"><i class="fa fa-user"> 个人简历</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">资源</h1><div class="post-content"><p><font color="#FF1E1E"><b>对学习过程中的数据集、框架、模型、论文、代码等进行整理，提供统一入口。</b></font><br><font color="#31C6D4"><b>Keep Learning, Keep Growing, Keep Succeeding</b></font></p>
<p><font color="#FA16D4"><b>欢迎大家在底部留言，提供其他资源链接，一起共建。</b></font></p>
<h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><h3 id="AIGC"><a href="#AIGC" class="headerlink" title="AIGC"></a>AIGC</h3><table>
<thead>
<tr>
<th><div style="width: 110px;">Datasets</div></th>
<th>Source</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>LLama2</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://huggingface.co/meta-llama">https://huggingface.co/meta-llama</a></td>
<td>Meta开源LLama2</td>
</tr>
<tr>
<td>OPENAI-ChatGPT</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://openai.com/blog/chatgpt">https://openai.com/blog/chatgpt</a></td>
<td>文本生成、对话</td>
</tr>
<tr>
<td>LLama</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama">https://github.com/facebookresearch/llama</a></td>
<td>Facebook大大语言模型</td>
</tr>
<tr>
<td>Alpaca</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a></td>
<td>LLama改进版</td>
</tr>
<tr>
<td>Chinese-LLaMA-Alpaca</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca">https://github.com/ymcui/Chinese-LLaMA-Alpaca</a></td>
<td>中文LLaMA模型和指令精调的Alpaca大模型</td>
</tr>
<tr>
<td>ChatGLM</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://huggingface.co/THUDM">https://huggingface.co/THUDM</a></td>
<td>清华ChatGLM</td>
</tr>
</tbody></table>
<h3 id="IMAGE"><a href="#IMAGE" class="headerlink" title="IMAGE"></a>IMAGE</h3><table>
<thead>
<tr>
<th><div style="width: 110px;">Datasets</div></th>
<th>Source</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>DALL·E2</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://openai.com/product/dall-e-2/">https://openai.com/product/dall-e-2</a></td>
<td>文字生成图片，效果还可以。</td>
</tr>
<tr>
<td>Midjourney</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://www.midjourney.com/showcase/recent/">https://www.midjourney.com/</a></td>
<td>生成的图片比较精致。</td>
</tr>
<tr>
<td>Stable Diffusion</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://huggingface.co/blog/stable_diffusion">stable_diffusion blog</a></td>
<td>多模态图片生成</td>
</tr>
</tbody></table>
<h3 id="AUDIO"><a href="#AUDIO" class="headerlink" title="AUDIO"></a>AUDIO</h3><table>
<thead>
<tr>
<th><div style="width: 110px;">Datasets</div></th>
<th>Source</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>Whisper</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://openai.com/research/whisper/">https://openai.com/research/whisper</a></td>
<td>语音识别，语音转文字。</td>
</tr>
</tbody></table>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h3 id="数据集网站"><a href="#数据集网站" class="headerlink" title="数据集网站"></a>数据集网站</h3><table>
<thead>
<tr>
<th><div style="width: 110px;">Website</div></th>
<th>Address</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>超神经</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://hyper.ai/datasets/">https://hyper.ai/datasets/</a></td>
<td>收集了数百个公开数据集资源，整理并提供相应的存储服务免费提供给相关从业人员进行下载</td>
</tr>
<tr>
<td>baai</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://data.baai.ac.cn/data">https://data.baai.ac.cn/data</a></td>
<td>若干数据集</td>
</tr>
</tbody></table>
<h3 id="指令微调-FineTuning"><a href="#指令微调-FineTuning" class="headerlink" title="指令微调(FineTuning)"></a>指令微调(FineTuning)</h3><table>
<thead>
<tr>
<th><div style="width: 110px;">Dataset</div></th>
<th>Source</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>openai human feedback</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://github.com/openai/following-instructions-human-feedback/tree/main/automatic-eval-samples">https://github.com/openai/following-instructions-human-feedback/tree/main/automatic-eval-samples</a></td>
<td>自动摘要、数学计算式问答、机器翻译、阅读理解、情感计算、推理、问答系统</td>
</tr>
<tr>
<td>HuggingFace</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://huggingface.co/datasets">https://huggingface.co/datasets</a></td>
<td>HuggingFace Datasets</td>
</tr>
</tbody></table>
<h3 id="计算机视觉-CV"><a href="#计算机视觉-CV" class="headerlink" title="计算机视觉(CV)"></a>计算机视觉(CV)</h3><table>
<thead>
<tr>
<th><div style="width: 110px;">Datasets</div></th>
<th>Source</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>MNIST</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></td>
<td>手写数字识别；计算机视觉入门级数据集，包含各种手写数字图片。</td>
</tr>
<tr>
<td>Fashion-MNIST</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://github.com/zalandoresearch/fashion-mnist">https://github.com/zalandoresearch/fashion-mnist</a></td>
<td>服饰识别；MNIST数据集过于简单，Fashion-MNIST可替代MNIST数据集，作为机器学习与深度学习算法基准。</td>
</tr>
<tr>
<td>ImageNet</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="http://www.image-net.org/">http://www.image-net.org</a></td>
<td>图像识别；大规模数据集，几大经典CNN模型，AlexNet、VGG、GoogleNet、ResNet在ILSVRC大赛数据集。</td>
</tr>
<tr>
<td>MS-COCO</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://cocodataset.org">https://cocodataset.org</a></td>
<td>目标检测、语义分割、图像标题生成；大规模的数据集。</td>
</tr>
<tr>
<td>CIFAR-10</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~kriz/cifar.html">http://www.cs.toronto.edu/~kriz&#x2F;cifar.html</a></td>
<td>图像分类；10个类别，每个类别6000张图片，5w个训练图片、1w个测试图片。</td>
</tr>
<tr>
<td>SVHN</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="http://ufldl.stanford.edu/housenumbers/">http://ufldl.stanford.edu/housenumbers/</a></td>
<td>目标检测、文字检测；街景门牌号数据集，来源于谷歌街景图片。</td>
</tr>
<tr>
<td>Open Images</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://storage.googleapis.com/openimages/web/factsfigures_v7.html">https://storage.googleapis.com/</a></td>
<td>语义分割、目标检测、图像分类；V4-V7</td>
</tr>
<tr>
<td>LAION-5B</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://laion.ai/blog/laion-5b/">https://laion.ai/blog/laion-5b/</a></td>
<td>the largest, freely accessible multi-modal dataset that currently exists.(目前最大的多模态开源数据集)</td>
</tr>
</tbody></table>
<h3 id="自然语言处理-NLP"><a href="#自然语言处理-NLP" class="headerlink" title="自然语言处理(NLP)"></a>自然语言处理(NLP)</h3><table>
<thead>
<tr>
<th><div style="width: 110px;">Datasets</div></th>
<th>Source</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>ACL-IMDB</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="http://ai.stanford.edu/~amaas/data/sentiment/">http://ai.stanford.edu/~amaas&#x2F;data&#x2F;sentiment&#x2F;</a></td>
<td>电影评论数据集；大规模情感二分类数据集。</td>
</tr>
<tr>
<td>WordNet</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://wordnet.princeton.edu/">https://wordnet.princeton.edu/</a></td>
<td>英语词库数据集</td>
</tr>
<tr>
<td>CLUECorpus2020</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/CLUECorpus2020">https://github.com/CLUEbenchmark/CLUECorpus2020/</a></td>
<td>通过对Common Crawl的中文部分进行语料清洗，最终得到100GB的高质量中文预训练语料</td>
</tr>
<tr>
<td>MNBVC</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://github.com/esbatmop/MNBVC">https://github.com/esbatmop/MNBVC</a></td>
<td>超大规模中文语料集，包括新闻、作文、小说、书籍、杂志、论文、台词、帖子、wiki、古诗、歌词、商品介绍、笑话、糗事、聊天记录等一切形式的纯文本中文数据。数据均来源于互联网收集。</td>
</tr>
</tbody></table>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="经典卷积神经网络模型-CNN"><a href="#经典卷积神经网络模型-CNN" class="headerlink" title="经典卷积神经网络模型(CNN)"></a>经典卷积神经网络模型(CNN)</h3><table>
<thead>
<tr>
<th><div style="width: 110px;">Model</div></th>
<th>Paper</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>LeNet-5</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">Gradient-Based Learning Applied to Document Recognition</a></td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/lenet/index.html">Yann LeCun（杨立昆）官网LetNet-5介绍；</a>1998年提出的CNN模型，主要用于手写字体识别，目前的CNN模型都没有逃出LetNet-5的卷积、池化、全连接架构。顶级大牛！！！</td>
</tr>
<tr>
<td>AlexNet</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a></td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~kriz/">Alex官网AlexNet介绍；</a>2012年提出的CNN模型， ImageNet LSVRC-2010竞赛冠军，具有划时代意义，再次之前主要用传统机器学习方法SVM，此后，深度学习发展迅速。</td>
</tr>
<tr>
<td>VGG</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.1556.pdf">Very Deep Convolutional Networks for Large-scale Image Recognition</a></td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~karen/">Karen Simonyan</a>，ImageNet LSVRC-2014竞赛亚军，VGG结构简单，应用性强，广受喜爱。<a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~vgg/research/very_deep/">VGG-16、VGG-19效果较好。</a></td>
</tr>
<tr>
<td>GoogleNet</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.4842.pdf">Going deeper with convolutions</a></td>
<td>ImageNet LSVRC-2014竞赛冠军，22层网络，Top5错误率比VGG低约0.6个百分点。（结构有点复杂，不如VGG通用）</td>
</tr>
<tr>
<td>ResNet</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.03385.pdf">Deep Residual Learning for Image Recognition</a></td>
<td>大名鼎鼎的残差神经网络，ImageNet LSVRC-2015竞赛冠军，152层残差网络结构，将Top5错误率降到3.57，已经超过人眼水平，此后ImageNet大赛不再举办。</td>
</tr>
</tbody></table>
<h3 id="目标检测模型-ObjectDetection"><a href="#目标检测模型-ObjectDetection" class="headerlink" title="目标检测模型(ObjectDetection)"></a>目标检测模型(ObjectDetection)</h3><table>
<thead>
<tr>
<th><div style="width: 110px;">Model</div></th>
<th>Paper</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>R-CNN</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1311.2524.pdf">Rich feature hierarchies for accurate object detection and semantic segmentation</a></td>
<td>Two Stage开山之作，深度学习与传统机器学习结合，先选中2000个候选区域，AlexNet提取特征向量，SVM二分类，识别区域是否有目标；训练回归器，选中区域目标位置。测试集上能达到58.5%准确率，当时的王者，缺点是慢，且空间消耗大。</td>
</tr>
<tr>
<td>Fast R-CNN</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1504.08083.pdf">Fast R-CNN</a></td>
<td>基于R-CNN和SPPNets，进行模型改进。不需要再生成2000个候选区域，只需要特征提取一次，使用selective search生成2000个区域候选框，再CNN卷积，Rol池化形成特定长度特征向量，送入全连接FC，Softmax，输出定位信息。速度较R-CNN有提升，但依旧慢。</td>
</tr>
<tr>
<td>Faster R-CNN</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1506.01497.pdf">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td>
<td>结构上将特征抽取，region proposal提取，bbox regression（包围边框回归），分类都整合到了一个网络中，综合性能有较大提高，检测速度提升较大。</td>
</tr>
<tr>
<td>YOLO v1</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1506.02640.pdf">You Only Look Once: Unified, Real-Time Object Detection</a></td>
<td>One Stage开山之作，将检测任务当做回归问题处理。优点是速度快，但精度下降。</td>
</tr>
<tr>
<td>YOLO v2</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1612.08242.pdf">YOLO9000: Better, Faster, Stronger</a></td>
<td>YOLO v1改进，使用了新的网络模型Darknet-19，加入了BN层，起到正则化效果；使用了高分辨率分类器；带Anchor Boxes的卷积；对边框进行K-Means聚类，可以直接定位预测。速度快，准确率较YOLO V1有提升，精度比SSD差。</td>
</tr>
<tr>
<td>YOLO v3</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.02767.pdf">YOLOv3: An Incremental Improvement</a></td>
<td>YOLO v2改进，使用新网络结构DarkNet-53，使用逻辑回归替代softmax作为分类器，融合FPN，实现多尺度检测。比较经典，在速度和准确率上都有提升，性能较好。也是作者的封笔之作，最后作者的自述比较有意思。In closing, do not @ me.(quit twitter)[破涕为笑]</td>
</tr>
<tr>
<td>YOLO v4</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.10934.pdf">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></td>
<td>将CV界大量研究成果进行了集成，提出了一套目标检测框架：输入、骨干、特征融合、输出；速度和精度上都有较大提升。</td>
</tr>
<tr>
<td>YOLO v5</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5">YOLOv5 Source</a></td>
<td>YOLO4发布两个月后，Glenn Jocher发布YOLO5，只有框架源码，无论文。架构上无创新，但号称模型大小比YOLO4小了近90%，但速度与YOLOv4不分伯仲。</td>
</tr>
<tr>
<td>YOLO v6</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.02976.pdf">YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications</a></td>
<td><a target="_blank" rel="noopener" href="https://tech.meituan.com/2022/06/23/yolov6-a-fast-and-accurate-target-detection-framework-is-opening-source.html">美团视觉智能部</a>研发的目标检测框架，致力于工业应用。专注于检测精度和推理效率。<a target="_blank" rel="noopener" href="https://github.com/meituan/YOLOv6">Github&#x2F;YOLOV6</a></td>
</tr>
<tr>
<td>YOLO v7</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.02696.pdf">YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</a></td>
<td>总结就是目前(2022)模型最小、速度最快、精度最高的目标检测模型。<a target="_blank" rel="noopener" href="https://github.com/WongKinYiu/yolov7">Github&#x2F;YOLOV7</a></td>
</tr>
<tr>
<td>SSD</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.02325.pdf">SSD: Single Shot MultiBox Detector</a></td>
<td>是一种One Stage的检测模型，相比于R-CNN系列模型上要简单许多。其精度与Faster R-CNN相匹敌，而速度达到59FPS，速度上超过Faster R-CNN</td>
</tr>
</tbody></table>
<h3 id="生成对抗网络-GAN"><a href="#生成对抗网络-GAN" class="headerlink" title="生成对抗网络(GAN)"></a>生成对抗网络(GAN)</h3><!-- 参考：[生成对抗网络GAN论文TOP 10][1] -->

<table>
<thead>
<tr>
<th><div style="width: 110px;">Model</div></th>
<th>Paper</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>DCGANs</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.06434.pdf">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a></td>
<td>卷积层与GAN相结合；并讨论了GAN特征的可视化、潜在空间插值、利用判别器特征来训练分类器、评估结果等问题。</td>
</tr>
<tr>
<td>Improved Techniques for Training GANs</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.03498.pdf">Improved Techniques for Training GANs</a></td>
<td>分析了DCGAN，改进GAN训练的技术。</td>
</tr>
<tr>
<td>Conditional GANs</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1411.1784.pdf">Conditional Generative Adversarial Nets</a></td>
<td>条件GAN是最先进的GAN之一，论文展示了如何整合数据的类标签，从而使 GAN 训练更加稳定。</td>
</tr>
<tr>
<td>PG-GAN</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1710.10196.pdf">Progressive Growing of GANs for Improved Quality, Stability, and Variation</a></td>
<td>作者表示，这种方式不仅稳定了训练，GAN 生成的图像也是迄今为止质量最好的。</td>
</tr>
<tr>
<td>BigGAN</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.11096.pdf">Large Scale GAN Training for High Fidelity Natural Image Synthesis</a></td>
<td>BigGAN模型基于ImageNet生成图像质量最高的模型之一。</td>
</tr>
<tr>
<td>StyleGAN</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.04948.pdf">A Style-Based Generator Architecture for Generative Adversarial Networks</a></td>
<td>StyleGAN 模型非常先进，利用了潜在空间控制。</td>
</tr>
<tr>
<td>CycleGAN</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.10593.pdf">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a></td>
<td>CycleGAN更具体地处理了没有成对训练样本的image-to-image转换的情况。CycleGAN有很多很酷的应用，比如超分辨率，风格转换，例如将马的图像变成斑马。</td>
</tr>
<tr>
<td>Pix2Pix</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1611.07004.pdf">Image-to-Image Translation with Conditional Adversarial Networks</a></td>
<td>Pix2Pix是另一种图像到图像转换的GAN模型。Pix2Pix有很多很酷的应用，比如将草图转换成逼真的照片。</td>
</tr>
<tr>
<td>StackGAN</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1612.03242.pdf">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</a></td>
<td>与Conditional GAN 和Progressively Growing GANs最为相似。StackGAN是从自然语言文本生成图像。（牛）</td>
</tr>
<tr>
<td>GAN</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1406.2661.pdf">Generative Adversarial Networks</a></td>
<td>定义了GAN框架，并讨论了“非饱和”损失函数。论文在MNIST、TFD和CIFAR-10图像数据集上对GAN的有效性进行了实验验证。</td>
</tr>
</tbody></table>
<h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><h3 id="标注工具"><a href="#标注工具" class="headerlink" title="标注工具"></a>标注工具</h3><table>
<thead>
<tr>
<th><div style="width: 110px;">Name</div></th>
<th>Source</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>labelImg</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://github.com/heartexlabs/labelImg">labelImg</a></td>
<td>开源的图像标注工具，标签可用于分类和目标检测。</td>
</tr>
<tr>
<td>labelme</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://github.com/wkentaro/labelme">labelme</a></td>
<td>图像语义分割标注工具</td>
</tr>
<tr>
<td>VIA</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~vgg/software/via/">VGG Image Annotator (VIA)</a></td>
<td>VGG图像注释器，可对图像、音频、视频标注。</td>
</tr>
<tr>
<td>EasyDL</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://ai.baidu.com/easydl/">EasyDL</a></td>
<td>百度推出AI开发平台，可采集、标注、清洗、训练。</td>
</tr>
</tbody></table>
<h3 id="分词工具"><a href="#分词工具" class="headerlink" title="分词工具"></a>分词工具</h3><table>
<thead>
<tr>
<th><div style="width: 110px;">Name</div></th>
<th>Source</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>jieba</td>
<td><a style="text-decoration:underline;" target="_blank" rel="noopener" href="https://github.com/fxsjy/jieba">jieba</a></td>
<td>中文分词（精确模式、全模式、搜索引擎模式）、标注组件</td>
</tr>
</tbody></table>
<h2 id="顶会"><a href="#顶会" class="headerlink" title="顶会"></a>顶会</h2><table>
<thead>
<tr>
<th><div style="width: 110px;">Conference</div></th>
<th>Description</th>
<th>Field</th>
</tr>
</thead>
<tbody><tr>
<td>ACL</td>
<td>Association of Computational Linguistics，每年开；计算语言学&#x2F;自然语言处理方面最好的会议</td>
<td>人工智能&#x2F;计算语言学</td>
</tr>
<tr>
<td>IJCAI</td>
<td>International Joint Conference on Artificial Intelligence, 人工智能领域顶级国际会议，论文接受率18％左右</td>
<td>人工智能</td>
</tr>
<tr>
<td>AAAI</td>
<td>American Association for Artificial Intelligence, 美国人工智能学会AAAI的年会，该领域的顶级会议</td>
<td>人工智能</td>
</tr>
<tr>
<td>PRICAI</td>
<td>Pacific Rim International Conference on Artificial Intelligence, 亚太人工智能国际会议</td>
<td>人工智能</td>
</tr>
<tr>
<td>ECCV</td>
<td>European Conference on Computer Vision, 领域顶级国际会议，录取率25%左右，2年一次，中国大陆每年论文数不超过20篇</td>
<td>模式识别&#x2F;计算机视觉&#x2F;多媒体计算</td>
</tr>
<tr>
<td>ICML</td>
<td>International Conference on Machine Learning, 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少</td>
<td>模式识别&#x2F;计算机学习</td>
</tr>
<tr>
<td>NIPS</td>
<td>Neural Information Processing Systems, 领域顶级国际会议，录取率20%左右，每年一次，目前完全国内论文极少（不超过5篇）</td>
<td>神经计算&#x2F;机器学习</td>
</tr>
<tr>
<td>ACM MM</td>
<td>ACM Multimedia Conference, 领域顶级国际会议，全文的录取率极低，Poster较容易</td>
<td>多媒体技术&#x2F;数据压缩</td>
</tr>
<tr>
<td>IEEE ICCV</td>
<td>International Conference on Computer Vision, 领域顶级国际会议，录取率20%左右，2年一次，中国大陆每年论文数不超过10篇</td>
<td>计算机视觉&#x2F;模式识别&#x2F;多媒体计算</td>
</tr>
<tr>
<td>IEEE CVPR</td>
<td>International Conference on Computer Vision and Pattern Recognition, 领域顶级国际会议，录取率25%左右，每年一次，中国大陆每年论文数不超过20篇</td>
<td>模式识别&#x2F;计算机视觉&#x2F;多媒体计算</td>
</tr>
<tr>
<td>IEEE ICIP</td>
<td>International conference on Image Processing, 图像处理领域最具影响力国际会议，一年一次</td>
<td>图像处理</td>
</tr>
<tr>
<td>IEEE ICME</td>
<td>International Conference on Multimedia and Expo, 多媒体领域重要国际会议，一年一次</td>
<td>多媒体技术</td>
</tr>
</tbody></table>
<h2 id="优秀网站"><a href="#优秀网站" class="headerlink" title="优秀网站"></a>优秀网站</h2><h3 id="优秀学习网站"><a href="#优秀学习网站" class="headerlink" title="优秀学习网站"></a>优秀学习网站</h3><table>
<thead>
<tr>
<th><div style="width: 110px;">网站</div></th>
<th>Description</th>
<th>Field</th>
</tr>
</thead>
<tbody><tr>
<td>书栈网</td>
<td><a target="_blank" rel="noopener" href="https://www.bookstack.cn/">https://www.bookstack.cn</a></td>
<td>计算机领域相关书籍、文档资料，很齐全</td>
</tr>
</tbody></table>
<h3 id="优秀工具网站"><a href="#优秀工具网站" class="headerlink" title="优秀工具网站"></a>优秀工具网站</h3><table>
<thead>
<tr>
<th><div style="width: 110px;">网站</div></th>
<th>Description</th>
<th>Field</th>
</tr>
</thead>
<tbody><tr>
<td>即时工具</td>
<td><a target="_blank" rel="noopener" href="https://www.67tool.com/category/5f56fb0164935e78271fd5a1">https://www.67tool.com</a></td>
<td>工具集合网站，200多个工具，图片处理、视频处理、文档处理等。</td>
</tr>
<tr>
<td>LaTeX公式编辑器</td>
<td><a target="_blank" rel="noopener" href="https://www.latexlive.com/">https://www.latexlive.com</a></td>
<td>LaTeX公式编辑器</td>
</tr>
<tr>
<td>CNN Explainer</td>
<td><a target="_blank" rel="noopener" href="https://poloclub.github.io/cnn-explainer/">cnn-explainer</a></td>
<td>Learn Convolutional Neural Network (CNN) in your browser!</td>
</tr>
</tbody></table>
</div></div><div class="nofancybox" id="waline"></div><script src="//unpkg.com/@waline/client@v2/dist/waline.js"></script><link rel="stylesheet" type="text/css" href="//unpkg.com/@waline/client@v2/dist/waline.css"><script>let metaInfo = ['nick', 'mail', 'link']
let requiredMeta = 'nick'.split(',').filter(item => {
  return metaInfo.indexOf(item) > -1
})
Waline.init({
  el: '#waline',
  comment: true,
  serverURL: 'https://waline-deeplearner.vercel.app',
  pageSize: '10',
  wordLimit: '500',
  requiredMeta,
})
</script></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/images/blog/avatar_new.png"/></a><p>心外无物，知行合一</p><a class="info-icon" href="/resume/" title="个人简历" target="_blank" style="margin-inline:5px"> <i class="fa fa-share-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:conghaoyuan@gmail.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/conghaoyuan" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../categories/AIGC/">AIGC</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="../categories/Algorithm/">Algorithm</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../categories/CV/">CV</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../categories/GUI/">GUI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../categories/NLP/">NLP</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../categories/Python/">Python</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../categories/%E6%95%B0%E5%AD%97%E4%BA%BA/">数字人</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../categories/%E6%95%B0%E5%AD%A6/">数学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/">深度学习模型</a><span class="category-list-count">11</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="../2025/03/06/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8BPrompt%20Engineering/">AIGC-LLM-辟邪剑谱之Prompt Engineering</a></li><li class="post-list-item"><a class="post-list-link" href="../2024/11/18/AIGC-Agent-%E4%BD%9C%E6%96%87%E6%89%B9%E6%94%B9%E6%A0%87%E6%B3%A8/">AIGC-Agent-作文批改标注</a></li><li class="post-list-item"><a class="post-list-link" href="../2024/09/27/LLama32%E8%B0%83%E7%A0%94/">LLama3.2调研报告</a></li><li class="post-list-item"><a class="post-list-link" href="../2024/04/09/AIGC-LLM-Prompt%E6%8A%80%E8%83%BD/">AIGC-LLM-Prompt工程技能综述</a></li><li class="post-list-item"><a class="post-list-link" href="../2024/03/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-BERT/">Deeeplearning模型-NLP-BERT</a></li><li class="post-list-item"><a class="post-list-link" href="../2024/01/16/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E8%AF%AD%E6%96%99%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/">Deeeplearning模型-NLP-大语言模型微调语料生成工具</a></li><li class="post-list-item"><a class="post-list-link" href="../2023/12/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-Transformer%E6%A8%A1%E5%9E%8B/">Deeeplearning模型-NLP-Transformer模型</a></li><li class="post-list-item"><a class="post-list-link" href="../2023/12/01/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-NLP%E5%9F%BA%E7%A1%80/">Deeeplearning模型-NLP-NLP基础</a></li><li class="post-list-item"><a class="post-list-link" href="../2023/10/26/%E6%95%B0%E5%AD%97%E4%BA%BA-%E6%95%B0%E5%AD%97%E4%BA%BA%E6%8A%80%E6%9C%AF%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94/">数字人-数字人技术效果对比</a></li><li class="post-list-item"><a class="post-list-link" href="../2023/10/13/GUI-%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E6%89%93%E5%8C%85%E4%B8%8E%E5%AE%89%E8%A3%85/">GUI-应用程序打包与安装</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://conghaoyuan.github.io" title="Fern[个人早期博客]" target="_blank">Fern[个人早期博客]</a></div><div class="widget"><div class="widget-title"><i class="fa fa-star"> 天文学</i></div><ul></ul><a href="/tools/tianwenli.html" title="天文历" target="_self">天文历</a></div><div class="widget"><div class="widget-title"><i class="fa fa-plus"> 数学小工具</i></div><ul></ul><a href="/tools/buy_house.html" title="丐版买房计算器" target="_self">丐版买房计算器</a><ul></ul><a href="/tools/equation_12.html" title="一元二次方程" target="_self">一元二次方程</a><ul></ul><a href="/tools/equation_line.html" title="直线方程" target="_self">直线方程</a><ul></ul><a href="/tools/equation_polynomial.html" title="多项式方程" target="_self">多项式方程</a><ul></ul><a href="/tools/regression_ridge.html" title="岭回归" target="_self">岭回归</a><ul></ul><a href="/tools/fourier_transform.html" title="傅里叶变换" target="_self">傅里叶变换</a><ul></ul><a href="/tools/gradient_descent.html" title="梯度下降" target="_self">梯度下降</a><ul></ul><a href="/tools/function_e.html" title="指数函数" target="_self">指数函数</a><ul></ul><a href="/tools/function_inverse_scale.html" title="反比例函数" target="_self">反比例函数</a><ul></ul><a href="/tools/function_log.html" title="对数函数" target="_self">对数函数</a><ul></ul><a href="/tools/function_sin.html" title="正弦函数" target="_self">正弦函数</a><ul></ul><a href="/tools/coord_grid.html" title="网格化坐标" target="_self">网格化坐标</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="../." rel="nofollow">DeepLearner.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="../js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="../js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="../css/search.css?v=1.0.0"><script type="text/javascript" src="../js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="../js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="../js/smartresize.js?v=1.0.0"></script></div><!-- hexo injector body_end start -->
<script src="assets/prism-bundle.js"></script>
<script src="assets/prism-plus.js" data-pjax=""></script>
<!-- hexo injector body_end end --></body></html>