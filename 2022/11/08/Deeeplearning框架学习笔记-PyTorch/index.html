<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Just Do It"><title>Deeeplearning框架-PyTorch | DeepLearner</title><link rel="stylesheet" type="text/css" href="../../../../css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/static/css/normalize.css"><link rel="stylesheet" type="text/css" href="/static/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/static/css/grids-responsive-min.css"><link rel="stylesheet" href="/static/css/font-awesome.min.css"><script type="text/javascript" src="/static/js/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="../../../../favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="../../../../favicon.ico"><link rel="apple-touch-icon" href="../../../../apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="../../../../apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="../../../../atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-83251621-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + 'b207d9febf3295d375e6e9b42006f9eb';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/static/js/clipboard.min.js"></script><script type="text/javascript" src="/static/js/toastr.min.js"></script><link rel="stylesheet" href="/static/css/toastr.min.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Deeeplearning框架-PyTorch</h1><a id="logo" href="../../../../.">DeepLearner</a><p class="description">Deep Learning Notes</p></div><div id="nav-menu"><a class="current" href="../../../../."><i class="fa fa-home"> 首页</i></a><a href="../../../../archives/"><i class="fa fa-archive"> 归档</i></a><a href="../../../../tagcloud/"><i class="fa fa-tags"> 标签</i></a><a href="../../../../resource/"><i class="fa fa-cloud"> 资源</i></a><a href="../../../../guestbook/"><i class="fa fa-wechat"> 留言</i></a><a href="../../../../codeline/"><i class="fa fa-circle"> 码迹</i></a><a href="../../../../resume/"><i class="fa fa-user"> 个人简历</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Deeeplearning框架-PyTorch</h1><div class="post-meta">created:2022-11-08</div><div class="post-meta">updated:2023-10-30<span> | </span><span class="category"><a href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a></span></div><a class="disqus-comment-count" href="#waline"><span class="waline-comment-count" id=""></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85"><span class="toc-number">1.</span> <span class="toc-text">安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E5%8C%85"><span class="toc-number">2.</span> <span class="toc-text">PyTorch常用工具包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F"><span class="toc-number">3.</span> <span class="toc-text">张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%BD%A2%E7%8A%B6%E6%9C%AF%E8%AF%AD"><span class="toc-number">3.1.</span> <span class="toc-text">张量形状术语</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%88%9B%E5%BB%BA"><span class="toc-number">3.2.</span> <span class="toc-text">张量创建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8ENumpy%E6%95%B0%E6%8D%AE%E7%9B%B8%E4%BA%92%E8%BD%AC%E5%8C%96"><span class="toc-number">3.3.</span> <span class="toc-text">与Numpy数据相互转化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">4.</span> <span class="toc-text">张量基本操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%87%E7%89%87"><span class="toc-number">4.1.</span> <span class="toc-text">索引与切片</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD"><span class="toc-number">4.2.</span> <span class="toc-text">广播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-number">4.3.</span> <span class="toc-text">数学运算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="toc-number">5.</span> <span class="toc-text">自动求导</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#autograd%E6%A8%A1%E5%9D%97"><span class="toc-number">5.1.</span> <span class="toc-text">autograd模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%87%E9%87%8F%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">5.2.</span> <span class="toc-text">标量反向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9D%9E%E6%A0%87%E9%87%8F%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">5.3.</span> <span class="toc-text">非标量反向传播</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">6.</span> <span class="toc-text">数据加载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">7.</span> <span class="toc-text">PyTorch神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-number">7.1.</span> <span class="toc-text">核心组件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PyTorch%E7%9A%84nn%E6%A8%A1%E5%9D%97"><span class="toc-number">7.2.</span> <span class="toc-text">PyTorch的nn模块</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">7.2.1.</span> <span class="toc-text">常用卷积层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">7.2.2.</span> <span class="toc-text">常用池化层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">7.2.3.</span> <span class="toc-text">激活函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">7.2.4.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="toc-number">7.2.5.</span> <span class="toc-text">全连接层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%E5%87%BD%E6%95%B0"><span class="toc-number">7.2.6.</span> <span class="toc-text">防止过拟合函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">7.2.7.</span> <span class="toc-text">其他</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%B9%E5%99%A8"><span class="toc-number">7.3.</span> <span class="toc-text">构建神经网络容器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">8.</span> <span class="toc-text">PyTorch优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-optim-SGD"><span class="toc-number">8.1.</span> <span class="toc-text">torch.optim.SGD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-optim-Adam-AMSGrad"><span class="toc-number">8.2.</span> <span class="toc-text">torch.optim.Adam(AMSGrad)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-optim-Adagrad"><span class="toc-number">8.3.</span> <span class="toc-text">torch.optim.Adagrad</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA%E6%AD%A5%E9%AA%A4"><span class="toc-number">9.</span> <span class="toc-text">模型搭建步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E4%B8%80%EF%BC%9ACNN%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E4%B9%8B%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B"><span class="toc-number">10.</span> <span class="toc-text">项目一：CNN回归模型之波士顿房价预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E4%BA%8C%EF%BC%9AMNIST%E6%89%8B%E5%86%99%E5%AD%97%E4%BD%93%E8%AF%86%E5%88%AB"><span class="toc-number">11.</span> <span class="toc-text">项目二：MNIST手写字体识别</span></a></li></ol></div></div><div class="post-content"><p>PyTorch是一个建立在Torch库之上的Python包，是由Facebook开源的神经网络框架。它提供一种类似NumPy的抽象方法来表征张量（或多维数组），可利用GPU来加速训练。Torch是一个经典的对多维矩阵数据进行操作的张量（tensor ）库，包含自动求导系统的深度神经网络，提供了高度灵活性和效率的深度学习实验性平台。与Tensorflow的静态计算图不同，PyTorch的计算图是动态的，可以根据计算需要实时改变计算图。</p>
<span id="more"></span>

<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CPU版本的</span></span><br><span class="line">pip3 install torch torchvision torchaudio --index-url https:<span class="regexp">//</span>download.pytorch.org<span class="regexp">/whl/</span>cpu</span><br><span class="line"><span class="comment"># CUDA 11.1</span></span><br><span class="line">pip3 install torch torchvision torchaudio</span><br></pre></td></tr></table></figure>

<h3 id="PyTorch常用工具包"><a href="#PyTorch常用工具包" class="headerlink" title="PyTorch常用工具包"></a>PyTorch常用工具包</h3><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html">https://pytorch.org/docs/stable/torch.html</a></p>
<ul>
<li>torch: 类似于Numpy的通用数组库，可将张量类型转为torch.cuda.TensorFloat，并在GPU上进行计算。</li>
<li>torch.autograd: 用于构建计算图形并自动获取梯度的包。</li>
<li>torch.nn: 具有共享层和损失函数的神经网络库。</li>
<li>torch.optim: 具有通用优化算法（如SGD，Adam等）的优化包。</li>
<li>torch.utils: 数据载入器。具有训练器和其他便利功能；</li>
</ul>
<p>   </p>
<h3 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h3><h4 id="张量形状术语"><a href="#张量形状术语" class="headerlink" title="张量形状术语"></a>张量形状术语</h4><ul>
<li><strong>形状</strong>：张量的每个维度的长度（元素数量）。</li>
<li><strong>秩</strong>：张量的维度数量。标量的秩为 0，向量的秩为 1，矩阵的秩为 2。</li>
<li><strong>轴</strong>或<strong>维度</strong>：张量的一个特殊维度。</li>
<li><strong>大小</strong>：张量的总项数，即乘积形状向量。</li>
</ul>
<h4 id="张量创建"><a href="#张量创建" class="headerlink" title="张量创建"></a>张量创建</h4><table>
<thead>
<tr>
<th>method</th>
<th>desc</th>
</tr>
</thead>
<tbody><tr>
<td>torch.tensor()</td>
<td>创建张量</td>
</tr>
<tr>
<td>torch.ones_like(x) </br> torch.zeros_like(x) </br> torch.rand_like(x) </br></td>
<td>创建一个与张量X具有相同维度的全1、全0或者是服从[0,1]区间上均匀分布的张量。</td>
</tr>
<tr>
<td>torch.normal(mean, std)</td>
<td>随机数生成张量，通过传入指定的均值张量和标准差张量，从而生成一个对应满足该分布的随机数张量。</td>
</tr>
<tr>
<td>torch.zeros(shape) </br> torch.ones(shape) </br> torch.eye(shape) </br>torch.full(shape, fill_value) </br>torch.empty(shape)</td>
<td>按照数值内容创建张量，可以通过指定shape来创建一个全0、全1、全为fill_value或是完全随机的一个张量。</td>
</tr>
<tr>
<td>torch.arange(start, end, step) </br> torch.linspace(start, end, step) </br> torch.logspace(start, end, step)</td>
<td>按照某种规则生成张量，可以通过指定start,end以及step参数来在某个范围内基于固定步长、等长间隔或对数间隔的张量。</td>
</tr>
</tbody></table>
<h4 id="与Numpy数据相互转化"><a href="#与Numpy数据相互转化" class="headerlink" title="与Numpy数据相互转化"></a>与Numpy数据相互转化</h4><table>
<thead>
<tr>
<th>method</th>
<th>desc</th>
</tr>
</thead>
<tbody><tr>
<td>torch.as_tensor(ndarray) </br> torch.from_numpy(ndarray)</td>
<td>将Numpy数组转化为PyTorch张量</td>
</tr>
<tr>
<td>tensor.numpy()</td>
<td>将PyTorch张量转化为Numpy数组</td>
</tr>
</tbody></table>
<h3 id="张量基本操作"><a href="#张量基本操作" class="headerlink" title="张量基本操作"></a>张量基本操作</h3><h4 id="索引与切片"><a href="#索引与切片" class="headerlink" title="索引与切片"></a>索引与切片</h4><p>遵循Python索引规则</p>
<ul>
<li>索引从 <code>0</code> 开始编制</li>
<li>负索引表示按倒序编制索引</li>
<li>冒号 <code>:</code> 用于切片 <code>start:stop:step</code></li>
</ul>
<h4 id="广播"><a href="#广播" class="headerlink" title="广播"></a>广播</h4><p>广播是从 NumPy 中的等效功能借用的一个概念。简而言之，在一定条件下，对一组张量执行组合运算时，为了适应大张量，会对小张量进行“扩展”。<br>广播主要发生在两种情况，一种是两个张量的维数不相等，但是它们的后缘维度的轴长相符，另外一种是有一方的长度为1。</p>
<h4 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h4><p>| func       	| desc           		| func           	| desc                	|<br>| :————	| :——————–	| :—————- | :——————– |<br>| t.abs()     	| 绝对值       			| torch.add(t, t) 或 + 	| 相加 				|<br>| t.sum() 		| 返回tensor所有元素的和  | torch.log(t) 		| 对数 					|<br>| t.ceil()      | 向上取整       		| torch.median(t)   | 中位数               	|<br>| t.floor()     | 向下取整       		| torch.mean(t)     | 均值                 	|<br>| t.exp()       | 指数          			| torch.sqrt()      | 开根号               	|<br>| t.prod()   	| 返回tensor所有元素的积  | torch.sign()      | 取符号               	|<br>| t.pow()       | 幂            			| torch.mul() 或 *  	| 逐元素相乘           	|<br>| torch.dot(t1,t2) | 计算一维张量的点积  	| torch.mm(mat1,mat2)&#x2F;matmul(mat1,mat2)&#x2F;@ | 计算矩阵的乘法 |<br>| t.t()     	| 转置            		| torch.cumprod(t,axis) | 在指定维度对t进行累积 |<br>| torch.cumsum() | 在指定维度对t进行累加 	| torch.std&#x2F;var&#x2F;sum | 标准差&#x2F;方差&#x2F;和 			|</p>
<h3 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h3><h4 id="autograd模块"><a href="#autograd模块" class="headerlink" title="autograd模块"></a>autograd模块</h4><p>autograd包为对tensor进行自动求导，为实现对tensor自动求导，需考虑如下事项：</p>
<ol>
<li>创建叶子节点(leaf node)的tensor，使用requires_grad参数指定是否记录对其的操作，以便之后利用backward()方法进行梯度求解。requires_grad参数缺省值为False，如果要对其求导需设置为True。</li>
<li>可利用requires_grad_()方法修改tensor的requires_grad属性。可以调用.detach()或with torch.no_grad():将不再计算张量的梯度，跟踪张量的历史记录。这点在评估模型、测试模型阶段常常使用。</li>
<li>通过运算创建的tensor（即非叶子节点），会自动被赋于grad_fn属性。该属性表示梯度函数。叶子节点的grad_fn为None。</li>
<li>最后得到的tensor执行backward()函数，此时自动计算各变量的梯度，并将累加结果保存grad属性中。计算完成后，非叶子节点的梯度自动释放。</li>
<li>backward()函数接受参数，该参数应和调用backward()函数的Tensor的维度相同。如果求导的tensor为标量（即一个数字），backward中参数可省略。</li>
<li>反向传播的中间缓存会被清空，如果需要进行多次反向传播，需要指定backward中的参数retain_graph&#x3D;True。多次反向传播时，梯度是累加的。</li>
<li>非叶子节点的梯度backward调用后即被清空。</li>
<li>可以通过用torch.no_grad()包裹代码块来阻止autograd去跟踪那些标记为.requesgrad&#x3D;True的张量的历史记录。这步在测试阶段经常使用。<br>整个过程中，Pytorch采用计算图的形式进行组织，该计算图为动态图，它的计算图在每次前向传播时，将重新构建。其他深度学习架构，如TensorFlow、Keras一般为静态图。</li>
</ol>
<h4 id="标量反向传播"><a href="#标量反向传播" class="headerlink" title="标量反向传播"></a>标量反向传播</h4><p>假设x、w、b都是标量，z&#x3D;wx+b，对标量z调用backward()，无需对backward()传入参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(b.grad)</span><br></pre></td></tr></table></figure>

<h4 id="非标量反向传播"><a href="#非标量反向传播" class="headerlink" title="非标量反向传播"></a>非标量反向传播</h4><p>Pytorch有个简单的规定，不让张量(tensor)对张量求导，只允许标量对张量求导，因此，如果目标张量对一个非标量调用backward()，需要传入一个gradient参数,该参数也是张量，而且需要与调用backward()的张量形状相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> 非标量反向传播</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 定义叶子节点张量x,y</span></span><br><span class="line">a = torch.tensor([<span class="number">2</span>, <span class="number">3</span>], dtype=torch.<span class="built_in">float</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([<span class="number">5</span>, <span class="number">6</span>], dtype=torch.<span class="built_in">float</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义f与a,b之间的映射关系：</span></span><br><span class="line">f = a * b</span><br><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line"><span class="comment"># 这里f是非标量张量所以我们需要把梯度参数传递给和张量f维数相同的反向传播函数</span></span><br><span class="line">gradients = torch.ones_like(f)</span><br><span class="line">f.backward(gradient=gradients)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a.grad)   <span class="comment"># tensor([5., 6.])</span></span><br><span class="line"><span class="built_in">print</span>(b.grad)   <span class="comment"># tensor([2., 3.])</span></span><br></pre></td></tr></table></figure>

<p><strong>tensorflow的forward只会根据第一次模型前向传播来构建一个静态的计算图, 后面的梯度自动求导都是根据这个计算图来计算的, 但是pytorch则不是, 它会为每次forward计算都构建一个动态图的计算图, 后续的每一次迭代都是使用一个新的计算图进行计算的.非常灵活，易调节</strong></p>
<h3 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h3><ul>
<li><code>torch.utils.data.Dataset</code>: 实现对数据的加载。</li>
<li><code>torch.utils.data.TensorDataset</code>: 对tensor进行打包</li>
<li><code>torch.utils.data.DataLoader</code>: 数据加载器，结合了数据集和取样器，并且可以提供多个线程处理数据集。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.utils.data.DataLoader(dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>, drop_last=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h3 id="PyTorch神经网络"><a href="#PyTorch神经网络" class="headerlink" title="PyTorch神经网络"></a>PyTorch神经网络</h3><h4 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h4><ul>
<li>层：神经网络的基本结构，将输入张量转换为输出张量。</li>
<li>模型：层构成的网络。</li>
<li>损失函数：参数学习的目标函数，通过最小化损失函数来学习各种参数。</li>
<li>优化器：如何使得损失函数最小，这就涉及到优化器。</li>
</ul>
<img src="/images/pytorch/flow.png" width="400px">

<h4 id="PyTorch的nn模块"><a href="#PyTorch的nn模块" class="headerlink" title="PyTorch的nn模块"></a>PyTorch的nn模块</h4><p>nn全称为neural network,意思是神经网络，是torch中构建神经网络的模块，导入为<code>torch.nn</code>。</p>
<p>[<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html][https://pytorch.org/docs/stable/nn.html]">https://pytorch.org/docs/stable/nn.html][https://pytorch.org/docs/stable/nn.html]</a></p>
<h5 id="常用卷积层"><a href="#常用卷积层" class="headerlink" title="常用卷积层"></a>常用卷积层</h5><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#convolution-layers">https://pytorch.org/docs/stable/nn.html#convolution-layers</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>, padding_mode=<span class="string">&#x27;zeros&#x27;</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>in_channels (int)</strong> – Number of channels in the input image</li>
<li><strong>out_channels (int)</strong> – Number of channels produced by the convolution</li>
<li><strong>kernel_size (int or tuple)</strong> – Size of the convolving kernel</li>
<li><strong>stride (int or tuple, optional)</strong> – Stride of the convolution. Default: 1</li>
<li><strong>padding (int, tuple or str, optional)</strong> – Padding added to all four sides of the input. Default: 0</li>
<li><strong>padding_mode (str, optional)</strong> – ‘zeros’, ‘reflect’, ‘replicate’ or ‘circular’. Default: ‘zeros’</li>
<li><strong>dilation (int or tuple, optional)</strong> – Spacing between kernel elements. Default: 1</li>
<li><strong>groups (int, optional)</strong> – Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias (bool, optional)</strong> – If True, adds a learnable bias to the output. Default: True</li>
</ul>
<h5 id="常用池化层"><a href="#常用池化层" class="headerlink" title="常用池化层"></a>常用池化层</h5><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#pooling-layers">https://pytorch.org/docs/stable/nn.html#pooling-layers</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MaxPool2d(kernel_size, stride=<span class="literal">None</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, return_indices=<span class="literal">False</span>, ceil_mode=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>kernel_size (Union[int, Tuple[int, int]])</strong> – the size of the window to take a max over</li>
<li><strong>stride (Union[int, Tuple[int, int]])</strong> – the stride of the window. Default value is kernel_size</li>
<li><strong>padding (Union[int, Tuple[int, int]])</strong> – Implicit negative infinity padding to be added on both sides</li>
<li><strong>dilation (Union[int, Tuple[int, int]])</strong> – a parameter that controls the stride of elements in the window</li>
<li><strong>return_indices (bool)</strong> – if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later</li>
<li><strong>ceil_mode (bool)</strong> – when True, will use ceil instead of floor to compute the output shape</li>
</ul>
<h5 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h5><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU</span></span><br><span class="line">torch.nn.ReLU()</span><br><span class="line"><span class="comment"># https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid</span></span><br><span class="line">torch.nn.Sigmoid()</span><br><span class="line"><span class="comment"># https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax</span></span><br><span class="line">torch.nn.Softmax()</span><br><span class="line"><span class="comment"># https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh</span></span><br><span class="line">torch.nn.Tanh()</span><br></pre></td></tr></table></figure>

<h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#loss-functions">https://pytorch.org/docs/stable/nn.html#loss-functions</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个衡量输入`x`(`模型预测输出`)和目标`y`之间均方误差标准。常用于回归模型。</span></span><br><span class="line">torch.nn.MSELoss(size_average=<span class="literal">True</span>)</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 交叉熵损失函数，又称对数似然损失函数，常用于分类模型。</span></span><br><span class="line">torch.nn.CrossEntropyLoss(weight=<span class="literal">None</span>, size_average=<span class="literal">True</span>)  </span><br></pre></td></tr></table></figure>

<h5 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h5><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#linear-layers">https://pytorch.org/docs/stable/nn.html#linear-layers</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 线性变换</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">in_features - 每个输入样本的大小</span></span><br><span class="line"><span class="string">out_features - 每个输出样本的大小</span></span><br><span class="line"><span class="string">bias - 若设置为False，这层不会学习偏置。默认值：True</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">torch.nn.Linear(in_features, out_features, bias=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h5 id="防止过拟合函数"><a href="#防止过拟合函数" class="headerlink" title="防止过拟合函数"></a>防止过拟合函数</h5><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#normalization-layers">https://pytorch.org/docs/stable/nn.html#normalization-layers</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">对小批量(mini-batch)数据进行批标准化(Batch Normalization)操作</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">num_features： 来自期望输入的特征数</span></span><br><span class="line"><span class="string">eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。</span></span><br><span class="line"><span class="string">momentum： 动态均值和动态方差所使用的动量。默认为0.1。</span></span><br><span class="line"><span class="string">affine： 一个布尔值，当设为true，给该层添加可学习的仿射变换参数。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">torch.nn.BatchNorm2d(num_features, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机将输入张量中部分元素设置为0。对于每次前向调用，被置0的元素都是随机的。</span></span><br><span class="line"><span class="comment"># p - 将元素置0的概率。默认值：0.5</span></span><br><span class="line">torch.nn.Dropout(p=<span class="number">0.5</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#transformer-layers">https://pytorch.org/docs/stable/nn.html#transformer-layers</a></p>
<h4 id="构建神经网络容器"><a href="#构建神经网络容器" class="headerlink" title="构建神经网络容器"></a>构建神经网络容器</h4><p><strong>自定义神经网络。构建网络层可以基于Module类，即torch.nn.Module，它是所有网络的基类。</strong></p>
<p>nn.Module与nn.functional的区别：<br>nn中的层，</p>
<ul>
<li>一类是继承了nn.Module，其命名一般为nn.Xxx(第一个是大写),如nn.Linear、nn.Conv2d、nn.CrossEntropyLoss等。</li>
<li>另一类是nn.functional中的函数，其名称一般为nn.functional.xxx，如nn.functional.linear、nn.functional.conv2d、nn.functional.cross_entropy等。</li>
</ul>
<p>从功能来说两者相当，基于nn.Mudle能实现的层，使用nn.functional也可实现，反之亦然，而且性能方面两者也没有太大差异。但PyTorch官方推荐：具有学习参数的（例如，conv2d, linear, batch_norm)采用nn.Xxx方式。没有学习参数的（例如，maxpool, loss func, activation func）等根据个人选择使用nn.functional.xxx或者nn.Xxx方式。</p>
<h3 id="PyTorch优化器"><a href="#PyTorch优化器" class="headerlink" title="PyTorch优化器"></a>PyTorch优化器</h3><h4 id="torch-optim-SGD"><a href="#torch-optim-SGD" class="headerlink" title="torch.optim.SGD"></a>torch.optim.SGD</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">可实现SGD优化算法，带动量SGD优化算法，带NAG(Nesterov accelerated gradient)动量SGD优化算法,并且均可拥有weight_decay项。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">params(iterable)- 参数组，优化器要管理的那部分参数。</span></span><br><span class="line"><span class="string">lr(float)- 初始学习率，可按需随着训练过程不断调整学习率。</span></span><br><span class="line"><span class="string">momentum(float)- 动量，通常设置为0.9，0.8</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">torch.optim.SGD(params, lr, momentum=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h4 id="torch-optim-Adam-AMSGrad"><a href="#torch-optim-Adam-AMSGrad" class="headerlink" title="torch.optim.Adam(AMSGrad)"></a>torch.optim.Adam(AMSGrad)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">torch.optim.Adam(params, lr=<span class="number">0.001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), eps=<span class="number">1e-08</span>)</span><br></pre></td></tr></table></figure>

<h4 id="torch-optim-Adagrad"><a href="#torch-optim-Adagrad" class="headerlink" title="torch.optim.Adagrad"></a>torch.optim.Adagrad</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">实现Adagrad优化方法(Adaptive Gradient)，Adagrad是一种自适应优化方法，是自适应的为各个参数分配不同的学习率。这个学习率的变化，会受到梯度的大小和迭代次数的影响。梯度越大，学习率越小；梯度越小，学习率越大。缺点是训练后期，学习率过小，因为Adagrad累加之前所有的梯度平方作为分母。AdaGrad算法是通过参数来调整合适的学习率λ，能独立地自动调整模型参数的学习率，对稀疏参数进行大幅更新和对频繁参数进行小幅更新。因此，Adagrad方法非常适合处理稀疏数据。AdaGrad算法在某些深度学习模型上效果不错。但还有些不足，可能因其累积梯度平方导致学习率过早或过量的减少所致。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">torch.optim.Adagrad(params, lr=<span class="number">0.01</span>, lr_decay=<span class="number">0</span>, weight_decay=<span class="number">0</span>, initial_accumulator_value=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>


<h3 id="模型搭建步骤"><a href="#模型搭建步骤" class="headerlink" title="模型搭建步骤"></a>模型搭建步骤</h3>
<div class="markmap-container" style="height:400px">
  <svg data="{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:2,&quot;p&quot;:{&quot;lines&quot;:[0,1]},&quot;v&quot;:&quot;神经网络搭建步骤&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[1,2]},&quot;v&quot;:&quot;获取数据集&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:6,&quot;p&quot;:{&quot;lines&quot;:[2,3]},&quot;v&quot;:&quot;图片、文本、视频等&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[3,4]},&quot;v&quot;:&quot;数据预处理，建立数据迭代器&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:6,&quot;p&quot;:{&quot;lines&quot;:[4,5]},&quot;v&quot;:&quot;torchvision、torchtext对数据预处理&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[5,6]},&quot;v&quot;:&quot;可视化源数据&quot;},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[6,7]},&quot;v&quot;:&quot;利用nn工具箱构建神经网络模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:6,&quot;p&quot;:{&quot;lines&quot;:[7,8]},&quot;v&quot;:&quot;nn.Module或着nn.Sequential&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[8,9]},&quot;v&quot;:&quot;实例化模型，并定义损失函数和优化器&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:6,&quot;p&quot;:{&quot;lines&quot;:[9,10]},&quot;v&quot;:&quot;torch.optim和Loss&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[10,11]},&quot;v&quot;:&quot;定义训练参数，学习率，批次等&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:6,&quot;p&quot;:{&quot;lines&quot;:[11,12]},&quot;v&quot;:&quot;batch_size,lr,epoch等参数&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[12,13]},&quot;v&quot;:&quot;训练模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:6,&quot;p&quot;:{&quot;lines&quot;:[13,14]},&quot;v&quot;:&quot;for循环迭代训练，正、反向传播，梯度清零&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[14,15]},&quot;v&quot;:&quot;获取训练结果，可视化训练结果&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:6,&quot;p&quot;:{&quot;lines&quot;:[15,16]},&quot;v&quot;:&quot;绘图可视化&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[16,17]},&quot;v&quot;:&quot;保存模型，测试并验证模型&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:6,&quot;p&quot;:{&quot;lines&quot;:[17,18]},&quot;v&quot;:&quot;测试集测试数据，验证模型&quot;}]},{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:4,&quot;p&quot;:{&quot;lines&quot;:[18,19]},&quot;v&quot;:&quot;模型优化&quot;,&quot;c&quot;:[{&quot;t&quot;:&quot;list_item&quot;,&quot;d&quot;:6,&quot;p&quot;:{&quot;lines&quot;:[19,20]},&quot;v&quot;:&quot;过拟合，欠拟合问题&quot;}]}]}"></svg>
</div>



<h3 id="项目一：CNN回归模型之波士顿房价预测"><a href="#项目一：CNN回归模型之波士顿房价预测" class="headerlink" title="项目一：CNN回归模型之波士顿房价预测"></a>项目一：CNN回归模型之波士顿房价预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 导入数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line">data_url = <span class="string">&quot;http://lib.stat.cmu.edu/datasets/boston&quot;</span></span><br><span class="line">raw_df = pd.read_csv(data_url, sep=<span class="string">&quot;\s+&quot;</span>, skiprows=<span class="number">22</span>, header=<span class="literal">None</span>)</span><br><span class="line">data = np.hstack([raw_df.values[::<span class="number">2</span>, :], raw_df.values[<span class="number">1</span>::<span class="number">2</span>, :<span class="number">2</span>]])</span><br><span class="line">target = raw_df.values[<span class="number">1</span>::<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X = data    <span class="comment"># 特征提取</span></span><br><span class="line">Y = target  <span class="comment"># 目标target提取</span></span><br><span class="line">Y = Y.reshape(-<span class="number">1</span>, <span class="number">1</span>)    <span class="comment"># 将y转换为一个列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据归一化</span></span><br><span class="line">ss = MinMaxScaler()</span><br><span class="line">X = ss.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把x，y从ndarray格式转成tensor格式</span></span><br><span class="line">X = torch.from_numpy(X).<span class="built_in">type</span>(torch.FloatTensor)</span><br><span class="line">Y = torch.from_numpy(Y).<span class="built_in">type</span>(torch.FloatTensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分数据集</span></span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建网络</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">13</span>, <span class="number">16</span>), <span class="comment"># 13*16 输入train_x的维度是(404,13)</span></span><br><span class="line">    nn.ReLU(),  <span class="comment"># ReLU()层  输出的数据的维度(404,16)</span></span><br><span class="line">    nn.Linear(<span class="number">16</span>, <span class="number">1</span>)   <span class="comment"># 再加一层全连接层 就输出y了  维度是(404,1)</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建优化器和损失函数</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.08</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">max_epoch = <span class="number">500</span></span><br><span class="line">iter_loss = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">    <span class="comment"># 向前传播</span></span><br><span class="line">    y_pred = model(train_x)</span><br><span class="line">    <span class="comment"># 计算loss</span></span><br><span class="line">    loss = criterion(y_pred, train_y)</span><br><span class="line">    <span class="keyword">if</span> (i % <span class="number">50</span> == <span class="number">0</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;第&#123;&#125;次迭代的loss是:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, loss))</span><br><span class="line">    </span><br><span class="line">    iter_loss.append(loss.item())</span><br><span class="line">    <span class="comment"># 清空之前的梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 权重调整</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">output = model(test_x)</span><br><span class="line">predict_list = output.detach().numpy()</span><br><span class="line"><span class="built_in">print</span>(predict_list[:<span class="number">10</span>])    <span class="comment"># 只输出10行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制不同的iteration的loss</span></span><br><span class="line">x = np.arange(max_epoch)</span><br><span class="line">y = np.array(iter_loss)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.title(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;nums of iter&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss func&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看真实值与预测值的散点图</span></span><br><span class="line">x = np.arange(test_x.shape[<span class="number">0</span>])</span><br><span class="line">y1 = np.array(predict_list) <span class="comment"># 测试集的预测值</span></span><br><span class="line">y2 = np.array(test_y) <span class="comment"># 测试集的实际值</span></span><br><span class="line">line1 = plt.scatter(x, y1, c=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">line2 = plt.scatter(x, y2, c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.legend([line1, line2], [<span class="string">&#x27;y_predict&#x27;</span>, <span class="string">&#x27;y_true&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&quot;loss of true value and predict value&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;boston hoursing price&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>outputs:</p>
<figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">第0次迭代的loss是:596.5716552734375</span><br><span class="line">第50次迭代的loss是:46.23166275024414</span><br><span class="line">第100次迭代的loss是:24.163291931152344</span><br><span class="line">第150次迭代的loss是:17.612567901611328</span><br><span class="line">第200次迭代的loss是:15.434951782226562</span><br><span class="line">第250次迭代的loss是:14.168197631835938</span><br><span class="line">第300次迭代的loss是:13.293211936950684</span><br><span class="line">第350次迭代的loss是:12.607232093811035</span><br><span class="line">第400次迭代的loss是:12.067901611328125</span><br><span class="line">第450次迭代的loss是:11.588809967041016</span><br><span class="line"><span class="comment">[<span class="comment">[12.9814005]</span></span></span><br><span class="line"><span class="comment"> <span class="comment">[21.10761  ]</span></span></span><br><span class="line"><span class="comment"> <span class="comment">[16.77605  ]</span></span></span><br><span class="line"><span class="comment"> <span class="comment">[21.558886 ]</span></span></span><br><span class="line"><span class="comment"> <span class="comment">[21.683792 ]</span></span></span><br><span class="line"><span class="comment"> <span class="comment">[20.60617  ]</span></span></span><br><span class="line"><span class="comment"> <span class="comment">[20.92336  ]</span></span></span><br><span class="line"><span class="comment"> <span class="comment">[25.620314 ]</span></span></span><br><span class="line"><span class="comment"> <span class="comment">[15.320004 ]</span></span></span><br><span class="line"><span class="comment"> <span class="comment">[17.533672 ]</span>]</span></span><br></pre></td></tr></table></figure>
<table><tr>
<td><img src="/images/pytorch/boston_loss.png" width="400px"></td>
<td><img src="/images/pytorch/boston_legend.png" width="400px"></td>
</tr></table>


<h3 id="项目二：MNIST手写字体识别"><a href="#项目二：MNIST手写字体识别" class="headerlink" title="项目二：MNIST手写字体识别"></a>项目二：MNIST手写字体识别</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">EPOCH = <span class="number">3</span></span><br><span class="line">BATCH_SIZE=<span class="number">50</span></span><br><span class="line">LR = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">root_dataset = <span class="string">&#x27;./dataset&#x27;</span></span><br><span class="line">train_data = torchvision.datasets.MNIST(</span><br><span class="line">    root=root_dataset,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor(),</span><br><span class="line">    download=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显式图片</span></span><br><span class="line"><span class="comment"># print(train_data.train_data.size())</span></span><br><span class="line"><span class="comment"># print(train_data.train_labels.size())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.imshow(train_data.train_data[66].numpy(), cmap=&#x27;Greys&#x27;)</span></span><br><span class="line"><span class="comment"># plt.title(&quot;%i&quot; % train_data.train_labels[66])</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练数据</span></span><br><span class="line">train_loader = Data.DataLoader(</span><br><span class="line">    dataset=train_data,</span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    shuffle=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载测试集数据</span></span><br><span class="line">test_data = torchvision.datasets.MNIST(</span><br><span class="line">    root=root_dataset,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_x = torch.unsqueeze(test_data.test_data, dim=<span class="number">1</span>).<span class="built_in">type</span>(torch.FloatTensor)[:<span class="number">2000</span>]/<span class="number">255</span></span><br><span class="line">test_y = test_data.test_labels[:<span class="number">2000</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            <span class="comment"># 卷积 维度变换（1,28,28）--&gt; (16,28,28)</span></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">1</span>,  <span class="comment"># 输入信号通道</span></span><br><span class="line">                     out_channels=<span class="number">16</span>, <span class="comment"># 输出信号通道</span></span><br><span class="line">                     kernel_size=<span class="number">3</span>,   <span class="comment"># 卷积核尺寸</span></span><br><span class="line">                     stride=<span class="number">1</span>,        <span class="comment"># 卷积步长</span></span><br><span class="line">                     padding=<span class="number">1</span>        <span class="comment"># 每一条边补充0的层数</span></span><br><span class="line">                     ),</span><br><span class="line">            <span class="comment"># 激活函数</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            <span class="comment"># 最大池化</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>) <span class="comment"># 维度变换（16,28,28）--&gt; (16,14,14)</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            <span class="comment"># 卷积 维度变换(16,14,14) --&gt; (32,14,14)</span></span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">16</span>,</span><br><span class="line">                     out_channels=<span class="number">32</span>,</span><br><span class="line">                     kernel_size=<span class="number">3</span>,</span><br><span class="line">                     stride=<span class="number">1</span>,</span><br><span class="line">                     padding=<span class="number">1</span></span><br><span class="line">                     ),</span><br><span class="line">            <span class="comment"># 激活函数</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            <span class="comment"># 最大池化</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>)  <span class="comment"># 维度变换(32,14,14) --&gt; (32,7,7)</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 全连接</span></span><br><span class="line">        self.output = nn.Linear(<span class="number">32</span>*<span class="number">7</span>*<span class="number">7</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.output(out)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cnn = CNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        <span class="comment"># 计算模型输出值</span></span><br><span class="line">        output = cnn(b_x) </span><br><span class="line">        <span class="comment"># 计算损失函数</span></span><br><span class="line">        loss = loss_func(output, b_y)</span><br><span class="line">        <span class="comment"># 优化器清零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># loss反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 优化器优化</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            test_output = cnn(test_x)</span><br><span class="line">            pred_y = torch.<span class="built_in">max</span>(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.numpy()</span><br><span class="line">            accuracy = <span class="built_in">float</span>((pred_y == test_y.data.numpy()).astype(<span class="built_in">int</span>).<span class="built_in">sum</span>()) / <span class="built_in">float</span>(test_y.size(<span class="number">0</span>))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#123;&#125; step: &#123;&#125; train loss: &#123;&#125; test accuracy: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, step, loss.data.numpy(), accuracy))</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">torch.save(cnn, <span class="string">&#x27;./checkpoint/cnn_minist.pkl&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;training finished&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>outputs:</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">0</span> train loss: <span class="number">2</span>.<span class="number">2989518642425537</span> test accuracy: <span class="number">0</span>.<span class="number">124</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">50</span> train loss: <span class="number">0</span>.<span class="number">6227200627326965</span> test accuracy: <span class="number">0</span>.<span class="number">761</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">100</span> train loss: <span class="number">0</span>.<span class="number">255100816488266</span> test accuracy: <span class="number">0</span>.<span class="number">873</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">150</span> train loss: <span class="number">0</span>.<span class="number">12506981194019318</span> test accuracy: <span class="number">0</span>.<span class="number">9035</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">200</span> train loss: <span class="number">0</span>.<span class="number">23605869710445404</span> test accuracy: <span class="number">0</span>.<span class="number">916</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">250</span> train loss: <span class="number">0</span>.<span class="number">10597402602434158</span> test accuracy: <span class="number">0</span>.<span class="number">9245</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">300</span> train loss: <span class="number">0</span>.<span class="number">19649431109428406</span> test accuracy: <span class="number">0</span>.<span class="number">9365</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">350</span> train loss: <span class="number">0</span>.<span class="number">09695011377334595</span> test accuracy: <span class="number">0</span>.<span class="number">945</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">400</span> train loss: <span class="number">0</span>.<span class="number">169553741812706</span> test accuracy: <span class="number">0</span>.<span class="number">9445</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">450</span> train loss: <span class="number">0</span>.<span class="number">07386567443609238</span> test accuracy: <span class="number">0</span>.<span class="number">956</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">500</span> train loss: <span class="number">0</span>.<span class="number">09177689254283905</span> test accuracy: <span class="number">0</span>.<span class="number">9525</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">550</span> train loss: <span class="number">0</span>.<span class="number">249346062541008</span> test accuracy: <span class="number">0</span>.<span class="number">957</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">600</span> train loss: <span class="number">0</span>.<span class="number">3329342305660248</span> test accuracy: <span class="number">0</span>.<span class="number">956</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">650</span> train loss: <span class="number">0</span>.<span class="number">2073971927165985</span> test accuracy: <span class="number">0</span>.<span class="number">9625</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">700</span> train loss: <span class="number">0</span>.<span class="number">07403621822595596</span> test accuracy: <span class="number">0</span>.<span class="number">964</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">750</span> train loss: <span class="number">0</span>.<span class="number">05281904339790344</span> test accuracy: <span class="number">0</span>.<span class="number">9615</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">800</span> train loss: <span class="number">0</span>.<span class="number">07417619973421097</span> test accuracy: <span class="number">0</span>.<span class="number">961</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">850</span> train loss: <span class="number">0</span>.<span class="number">1186814084649086</span> test accuracy: <span class="number">0</span>.<span class="number">9705</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">900</span> train loss: <span class="number">0</span>.<span class="number">11796750128269196</span> test accuracy: <span class="number">0</span>.<span class="number">965</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">950</span> train loss: <span class="number">0</span>.<span class="number">020582687109708786</span> test accuracy: <span class="number">0</span>.<span class="number">9715</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">1000</span> train loss: <span class="number">0</span>.<span class="number">028191855177283287</span> test accuracy: <span class="number">0</span>.<span class="number">9655</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">1050</span> train loss: <span class="number">0</span>.<span class="number">10790297389030457</span> test accuracy: <span class="number">0</span>.<span class="number">967</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">1100</span> train loss: <span class="number">0</span>.<span class="number">037335075438022614</span> test accuracy: <span class="number">0</span>.<span class="number">9635</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">0</span> step: <span class="number">1150</span> train loss: <span class="number">0</span>.<span class="number">1390017867088318</span> test accuracy: <span class="number">0</span>.<span class="number">9655</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">0</span> train loss: <span class="number">0</span>.<span class="number">07538899779319763</span> test accuracy: <span class="number">0</span>.<span class="number">9715</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">50</span> train loss: <span class="number">0</span>.<span class="number">06148580089211464</span> test accuracy: <span class="number">0</span>.<span class="number">9745</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">100</span> train loss: <span class="number">0</span>.<span class="number">059671998023986816</span> test accuracy: <span class="number">0</span>.<span class="number">9635</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">150</span> train loss: <span class="number">0</span>.<span class="number">0580081082880497</span> test accuracy: <span class="number">0</span>.<span class="number">9745</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">200</span> train loss: <span class="number">0</span>.<span class="number">012145531363785267</span> test accuracy: <span class="number">0</span>.<span class="number">97</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">250</span> train loss: <span class="number">0</span>.<span class="number">03478122875094414</span> test accuracy: <span class="number">0</span>.<span class="number">972</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">300</span> train loss: <span class="number">0</span>.<span class="number">03409402817487717</span> test accuracy: <span class="number">0</span>.<span class="number">9675</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">350</span> train loss: <span class="number">0</span>.<span class="number">03502879664301872</span> test accuracy: <span class="number">0</span>.<span class="number">9725</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">400</span> train loss: <span class="number">0</span>.<span class="number">03113013319671154</span> test accuracy: <span class="number">0</span>.<span class="number">978</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">450</span> train loss: <span class="number">0</span>.<span class="number">13314753770828247</span> test accuracy: <span class="number">0</span>.<span class="number">978</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">500</span> train loss: <span class="number">0</span>.<span class="number">018445264548063278</span> test accuracy: <span class="number">0</span>.<span class="number">97</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">550</span> train loss: <span class="number">0</span>.<span class="number">011954346671700478</span> test accuracy: <span class="number">0</span>.<span class="number">973</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">600</span> train loss: <span class="number">0</span>.<span class="number">0324656218290329</span> test accuracy: <span class="number">0</span>.<span class="number">9775</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">650</span> train loss: <span class="number">0</span>.<span class="number">046384457498788834</span> test accuracy: <span class="number">0</span>.<span class="number">9745</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">700</span> train loss: <span class="number">0</span>.<span class="number">05520062893629074</span> test accuracy: <span class="number">0</span>.<span class="number">979</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">750</span> train loss: <span class="number">0</span>.<span class="number">30722805857658386</span> test accuracy: <span class="number">0</span>.<span class="number">9765</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">800</span> train loss: <span class="number">0</span>.<span class="number">10514361411333084</span> test accuracy: <span class="number">0</span>.<span class="number">9765</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">850</span> train loss: <span class="number">0</span>.<span class="number">04414287954568863</span> test accuracy: <span class="number">0</span>.<span class="number">981</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">900</span> train loss: <span class="number">0</span>.<span class="number">1443830281496048</span> test accuracy: <span class="number">0</span>.<span class="number">978</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">950</span> train loss: <span class="number">0</span>.<span class="number">026100166141986847</span> test accuracy: <span class="number">0</span>.<span class="number">979</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">1000</span> train loss: <span class="number">0</span>.<span class="number">11013159900903702</span> test accuracy: <span class="number">0</span>.<span class="number">9795</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">1050</span> train loss: <span class="number">0</span>.<span class="number">10890169441699982</span> test accuracy: <span class="number">0</span>.<span class="number">977</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">1100</span> train loss: <span class="number">0</span>.<span class="number">07636130601167679</span> test accuracy: <span class="number">0</span>.<span class="number">98</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">1</span> step: <span class="number">1150</span> train loss: <span class="number">0</span>.<span class="number">12814483046531677</span> test accuracy: <span class="number">0</span>.<span class="number">9795</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">0</span> train loss: <span class="number">0</span>.<span class="number">15829549729824066</span> test accuracy: <span class="number">0</span>.<span class="number">977</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">50</span> train loss: <span class="number">0</span>.<span class="number">022714903578162193</span> test accuracy: <span class="number">0</span>.<span class="number">976</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">100</span> train loss: <span class="number">0</span>.<span class="number">11526823043823242</span> test accuracy: <span class="number">0</span>.<span class="number">98</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">150</span> train loss: <span class="number">0</span>.<span class="number">042159732431173325</span> test accuracy: <span class="number">0</span>.<span class="number">979</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">200</span> train loss: <span class="number">0</span>.<span class="number">10546272993087769</span> test accuracy: <span class="number">0</span>.<span class="number">979</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">250</span> train loss: <span class="number">0</span>.<span class="number">030654177069664</span> test accuracy: <span class="number">0</span>.<span class="number">98</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">300</span> train loss: <span class="number">0</span>.<span class="number">0656307265162468</span> test accuracy: <span class="number">0</span>.<span class="number">9795</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">350</span> train loss: <span class="number">0</span>.<span class="number">08443894982337952</span> test accuracy: <span class="number">0</span>.<span class="number">978</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">400</span> train loss: <span class="number">0</span>.<span class="number">035659801214933395</span> test accuracy: <span class="number">0</span>.<span class="number">976</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">450</span> train loss: <span class="number">0</span>.<span class="number">08835320174694061</span> test accuracy: <span class="number">0</span>.<span class="number">979</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">500</span> train loss: <span class="number">0</span>.<span class="number">02448669634759426</span> test accuracy: <span class="number">0</span>.<span class="number">976</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">550</span> train loss: <span class="number">0</span>.<span class="number">023640567436814308</span> test accuracy: <span class="number">0</span>.<span class="number">98</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">600</span> train loss: <span class="number">0</span>.<span class="number">0034627816639840603</span> test accuracy: <span class="number">0</span>.<span class="number">9775</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">650</span> train loss: <span class="number">0</span>.<span class="number">10121780633926392</span> test accuracy: <span class="number">0</span>.<span class="number">9785</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">700</span> train loss: <span class="number">0</span>.<span class="number">0032379289623349905</span> test accuracy: <span class="number">0</span>.<span class="number">9795</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">750</span> train loss: <span class="number">0</span>.<span class="number">053985223174095154</span> test accuracy: <span class="number">0</span>.<span class="number">9805</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">800</span> train loss: <span class="number">0</span>.<span class="number">014309514313936234</span> test accuracy: <span class="number">0</span>.<span class="number">9805</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">850</span> train loss: <span class="number">0</span>.<span class="number">030016331002116203</span> test accuracy: <span class="number">0</span>.<span class="number">9785</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">900</span> train loss: <span class="number">0</span>.<span class="number">08062826097011566</span> test accuracy: <span class="number">0</span>.<span class="number">983</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">950</span> train loss: <span class="number">0</span>.<span class="number">005085643380880356</span> test accuracy: <span class="number">0</span>.<span class="number">9755</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">1000</span> train loss: <span class="number">0</span>.<span class="number">046167097985744476</span> test accuracy: <span class="number">0</span>.<span class="number">9805</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">1050</span> train loss: <span class="number">0</span>.<span class="number">002461640862748027</span> test accuracy: <span class="number">0</span>.<span class="number">979</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">1100</span> train loss: <span class="number">0</span>.<span class="number">04258471354842186</span> test accuracy: <span class="number">0</span>.<span class="number">9835</span></span><br><span class="line"><span class="attribute">epoch</span>: <span class="number">2</span> step: <span class="number">1150</span> train loss: <span class="number">0</span>.<span class="number">0026853044982999563</span> test accuracy: <span class="number">0</span>.<span class="number">9805</span></span><br><span class="line"><span class="attribute">training</span> finished</span><br></pre></td></tr></table></figure>

<p>可以看到准确率已经达到了98%。调整训练轮数、批次大小、学习率继续训练，发现准确率提升到98.4%。</p>
<figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">EPOCH = 20</span><br><span class="line">BATCH_SIZE=100</span><br><span class="line">LR = 0.0002</span><br><span class="line"></span><br><span class="line">epoch:<span class="number"> 0 </span>step:<span class="number"> 0 </span>train loss: 2.3289575576782227 test accuracy: 0.073</span><br><span class="line">epoch:<span class="number"> 0 </span>step:<span class="number"> 50 </span>train loss: 2.133652448654175 test accuracy: 0.596</span><br><span class="line">epoch:<span class="number"> 0 </span>step:<span class="number"> 100 </span>train loss: 1.4860273599624634 test accuracy: 0.736</span><br><span class="line">epoch:<span class="number"> 0 </span>step:<span class="number"> 150 </span>train loss: 0.8417261242866516 test accuracy: 0.7835</span><br><span class="line">epoch:<span class="number"> 0 </span>step:<span class="number"> 200 </span>train loss: 0.6684989929199219 test accuracy: 0.821</span><br><span class="line">epoch:<span class="number"> 0 </span>step:<span class="number"> 250 </span>train loss: 0.5492635369300842 test accuracy: 0.8485</span><br><span class="line">epoch:<span class="number"> 0 </span>step:<span class="number"> 300 </span>train loss: 0.42627930641174316 test accuracy: 0.866</span><br><span class="line">epoch:<span class="number"> 0 </span>step:<span class="number"> 350 </span>train loss: 0.2847326695919037 test accuracy: 0.876</span><br><span class="line">epoch:<span class="number"> 0 </span>step:<span class="number"> 400 </span>train loss: 0.29851987957954407 test accuracy: 0.886</span><br><span class="line">...............</span><br><span class="line">epoch:<span class="number"> 19 </span>step:<span class="number"> 0 </span>train loss: 0.14779159426689148 test accuracy: 0.985</span><br><span class="line">epoch:<span class="number"> 19 </span>step:<span class="number"> 50 </span>train loss: 0.0102002564817667 test accuracy: 0.9825</span><br><span class="line">epoch:<span class="number"> 19 </span>step:<span class="number"> 100 </span>train loss: 0.04536750167608261 test accuracy: 0.984</span><br><span class="line">epoch:<span class="number"> 19 </span>step:<span class="number"> 150 </span>train loss: 0.011938444338738918 test accuracy: 0.9825</span><br><span class="line">epoch:<span class="number"> 19 </span>step:<span class="number"> 200 </span>train loss: 0.026133885607123375 test accuracy: 0.983</span><br><span class="line">epoch:<span class="number"> 19 </span>step:<span class="number"> 250 </span>train loss: 0.00792818795889616 test accuracy: 0.9815</span><br><span class="line">epoch:<span class="number"> 19 </span>step:<span class="number"> 300 </span>train loss: 0.07967539131641388 test accuracy: 0.985</span><br><span class="line">epoch:<span class="number"> 19 </span>step:<span class="number"> 350 </span>train loss: 0.02291872352361679 test accuracy: 0.985</span><br><span class="line">epoch:<span class="number"> 19 </span>step:<span class="number"> 400 </span>train loss: 0.02513641119003296 test accuracy: 0.9835</span><br><span class="line">epoch:<span class="number"> 19 </span>step:<span class="number"> 450 </span>train loss: 0.06369250267744064 test accuracy: 0.9835</span><br><span class="line">epoch:<span class="number"> 19 </span>step:<span class="number"> 500 </span>train loss: 0.017594920471310616 test accuracy: 0.9845</span><br><span class="line">epoch:<span class="number"> 19 </span>step:<span class="number"> 550 </span>train loss: 0.020058415830135345 test accuracy: 0.984</span><br><span class="line">training finished</span><br></pre></td></tr></table></figure>


<p>模型使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">cnn = torch.load(<span class="string">&#x27;./checkpoint/cnn_minist.pkl&#x27;</span>)</span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line">test_output = cnn(test_x[:<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">pred_y = torch.<span class="built_in">max</span>(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.numpy()</span><br><span class="line"><span class="built_in">print</span>(pred_y, <span class="string">&#x27;prediction number&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(test_y[:<span class="number">20</span>].numpy(), <span class="string">&#x27;real number&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得出模型的准确度</span></span><br><span class="line">test_output1 = cnn(test_x)</span><br><span class="line">pred_y1 = torch.<span class="built_in">max</span>(test_output1, <span class="number">1</span>)[<span class="number">1</span>].data.numpy()</span><br><span class="line">accuracy = <span class="built_in">float</span>((pred_y1 == test_y.data.numpy()).astype(<span class="built_in">int</span>).<span class="built_in">sum</span>()) / <span class="built_in">float</span>(test_y.size(<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;accuracy&#x27;</span>,accuracy)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4] prediction number</span></span><br><span class="line"><span class="string">[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4] real number</span></span><br><span class="line"><span class="string">accuracy 0.9855</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<hr>
</div><script type="text/javascript" src="../../../../js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://deeplearner.top/2022/11/08/Deeeplearning%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-PyTorch/" data-id="clkor69ox002ec6s6cta6cp92" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQ4AAAEOCAAAAABd2qZ5AAAD6klEQVR42u3aSW7cQBAEwPn/p23AJxvSkJlVPbQNBE+CxKU7KKBYy+sVHz9+Hfnvv57z9czff5Ofc33tu3MOHzhw4MCBI17u9a2TRV/fOdlMQpDcP3nKNyvHgQMHDhyHOK6DaBvwkq3mr6TFyoMuDhw4cOD4uxxtAMs52ue24R8HDhw4cPzLHHly1ZYRZ8W+9kMBBw4cOHA8w9EmS6dKhG1pLw/JH6+V4sCBAweONId65UHrf/n5g/MdOHDgwIFj0ey5fkAbnvN2VD54tzlw4MCBA8eeY9bmmZXqZiXC2bV5KnhTK8WBAwcOHGuONkF6skS4KRQOs1UcOHDgwFFyzBKqWbto0zSapXb1gB0OHDhw4Fhz5EnXZomzO88oc+jofwQHDhw4cIw4ksLfrEU0mxzYr2HDhwMHDhw4Nhz5WEA7UrBH2W+4Lm7iwIEDB45DHG05b3P+JrRvmJJRDBw4cODAcYpjljJtNpCkbe1VeUB9u0IcOHDgwLHmmI0CtAnVJtFqr82TyZuGEw4cOHDgOMTRpmezrZ4qQQ6LgNe7w4EDBw4cC45NQG03lg/YtW2qU00pHDhw4MCx5zg1JDdrMrXEs/JlXYjEgQMHDhwLjiTVycPeLPE7W0CcHThw4MCB4yxHMnyWn3M23M5KjW0YLr4UcODAgQPHuoPfLiJnmgXUvL00DMw4cODAgeMQRx4IT7V5ZslevpL8/lGtFAcOHDhwlByzsmAehtun5MGyDag3K8GBAwcOHIc4NqW3vGiYjE20ZcQ25N+8Hhw4cODAseDIh8/altLsqqTY1z49x8KBAwcOHHuOvFF0bFygbErlHPlriO6DAwcOHDhGa8tPbZOozXhB8tf8DsVrxoEDBw4ca4724jaV2gTmljUvQUbJGw4cOHDgGHE80xxqRyL2Yw35kAQOHDhw4DjLsRlcyEt1baCtk65R2+ktMQ4cOHDgOMTRhrF8WGFWzms7QXmJ82Z3OHDgwIFjzZEU49q0bX9Oe+amYVZ8F+DAgQMHjqN9lhYub24l65l9CnywVooDBw4cOBZDaadSu3wcoW0mzdpjf/wVBw4cOHCsOdqlzJpMr9GRhPkZ2c1rxoEDBw4cC47NAMGssTQbrZsV++pUEAcOHDhwrDk2Ya8dU2gHF/Jgn9xztXkcOHDgwFFyJMG1DYoJ39ni46kyJQ4cOHDgOMVxvZR2hC4v9u3bXW1AvXmpOHDgwIHjcY7NcFvbTJqNg89SPhw4cODA8TxHHlDbwe72DqfG7HDgwIEDx+c42kXsGz/X22iLiXn4v/k4wIEDBw4ca47NQENOM3tWnkbmSV0+kIEDBw4cOEqOn8HkRF66wK3cAAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="../../../../tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"><i class="fa fa-tag"></i>深度学习框架</a><a href="../../../../tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><i class="fa fa-tag"></i>深度学习</a><a href="../../../../tags/PyTorch/"><i class="fa fa-tag"></i>PyTorch</a></div><div class="post-nav"><a class="pre" href="../../12/Deeeplearning%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-CNN-LeNet5/">Deeeplearning模型-CNN-LeNet5</a><a class="next" href="../../07/Deeeplearning%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-TensorFlow/">Deeeplearning框架-TensorFlow</a></div><div class="nofancybox" id="waline"></div><script src="//unpkg.com/@waline/client@v2/dist/waline.js"></script><link rel="stylesheet" type="text/css" href="//unpkg.com/@waline/client@v2/dist/waline.css"><script>let metaInfo = ['nick', 'mail', 'link']
let requiredMeta = 'nick'.split(',').filter(item => {
  return metaInfo.indexOf(item) > -1
})
Waline.init({
  el: '#waline',
  comment: true,
  serverURL: 'https://waline-deeplearner.vercel.app',
  pageSize: '10',
  wordLimit: '500',
  requiredMeta,
})
</script></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/images/blog/avatar_new.png"/></a><p>心外无物，知行合一</p><a class="info-icon" href="/resume/" title="个人简历" target="_blank" style="margin-inline:5px"> <i class="fa fa-share-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:conghaoyuan@gmail.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/conghaoyuan" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/AIGC/">AIGC</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Algorithm/">Algorithm</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/CV/">CV</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/GUI/">GUI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/NLP/">NLP</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Python/">Python</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%97%E4%BA%BA/">数字人</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%A6/">数学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/">深度学习模型</a><span class="category-list-count">11</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/06/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8BPrompt%20Engineering/">AIGC-LLM-辟邪剑谱之Prompt Engineering</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/11/18/AIGC-Agent-%E4%BD%9C%E6%96%87%E6%89%B9%E6%94%B9%E6%A0%87%E6%B3%A8/">AIGC-Agent-作文批改标注</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/09/27/LLama32%E8%B0%83%E7%A0%94/">LLama3.2调研报告</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/04/09/AIGC-LLM-Prompt%E6%8A%80%E8%83%BD/">AIGC-LLM-Prompt工程技能综述</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/03/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-BERT/">Deeeplearning模型-NLP-BERT</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/01/16/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E8%AF%AD%E6%96%99%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/">Deeeplearning模型-NLP-大语言模型微调语料生成工具</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-Transformer%E6%A8%A1%E5%9E%8B/">Deeeplearning模型-NLP-Transformer模型</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/01/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-NLP%E5%9F%BA%E7%A1%80/">Deeeplearning模型-NLP-NLP基础</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/10/26/%E6%95%B0%E5%AD%97%E4%BA%BA-%E6%95%B0%E5%AD%97%E4%BA%BA%E6%8A%80%E6%9C%AF%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94/">数字人-数字人技术效果对比</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/10/13/GUI-%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E6%89%93%E5%8C%85%E4%B8%8E%E5%AE%89%E8%A3%85/">GUI-应用程序打包与安装</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://conghaoyuan.github.io" title="Fern[个人早期博客]" target="_blank">Fern[个人早期博客]</a></div><div class="widget"><div class="widget-title"><i class="fa fa-star"> 天文学</i></div><ul></ul><a href="/tools/tianwenli.html" title="天文历" target="_self">天文历</a></div><div class="widget"><div class="widget-title"><i class="fa fa-plus"> 数学小工具</i></div><ul></ul><a href="/tools/buy_house.html" title="丐版买房计算器" target="_self">丐版买房计算器</a><ul></ul><a href="/tools/equation_12.html" title="一元二次方程" target="_self">一元二次方程</a><ul></ul><a href="/tools/equation_line.html" title="直线方程" target="_self">直线方程</a><ul></ul><a href="/tools/equation_polynomial.html" title="多项式方程" target="_self">多项式方程</a><ul></ul><a href="/tools/regression_ridge.html" title="岭回归" target="_self">岭回归</a><ul></ul><a href="/tools/fourier_transform.html" title="傅里叶变换" target="_self">傅里叶变换</a><ul></ul><a href="/tools/gradient_descent.html" title="梯度下降" target="_self">梯度下降</a><ul></ul><a href="/tools/function_e.html" title="指数函数" target="_self">指数函数</a><ul></ul><a href="/tools/function_inverse_scale.html" title="反比例函数" target="_self">反比例函数</a><ul></ul><a href="/tools/function_log.html" title="对数函数" target="_self">对数函数</a><ul></ul><a href="/tools/function_sin.html" title="正弦函数" target="_self">正弦函数</a><ul></ul><a href="/tools/coord_grid.html" title="网格化坐标" target="_self">网格化坐标</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="../../../../." rel="nofollow">DeepLearner.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="../../../../js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="../../../../js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="../../../../css/search.css?v=1.0.0"><script type="text/javascript" src="../../../../js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="/static/js/MathJax.js?config=TeX-MML-AM_CHTML" async></script><div id="script" type="text/javascript" src="../../../../js/mathjaxs.js" async></div><script type="text/javascript" src="../../../../js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="../../../../js/smartresize.js?v=1.0.0"></script></div><!-- hexo injector body_end start -->
<script src="assets/prism-bundle.js"></script>
<script src="assets/prism-plus.js" data-pjax=""></script>
<!-- hexo injector body_end end --><script src="/js/markmap.js"></script></body></html>