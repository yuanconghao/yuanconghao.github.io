<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Just Do It"><title>DeepLearning-4.数值计算 | DeepLearner</title><link rel="stylesheet" type="text/css" href="../../../../css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/static/css/normalize.css"><link rel="stylesheet" type="text/css" href="/static/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/static/css/grids-responsive-min.css"><link rel="stylesheet" href="/static/css/font-awesome.min.css"><script type="text/javascript" src="/static/js/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="../../../../favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="../../../../favicon.ico"><link rel="apple-touch-icon" href="../../../../apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="../../../../apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="../../../../atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-83251621-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + 'b207d9febf3295d375e6e9b42006f9eb';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/static/js/clipboard.min.js"></script><script type="text/javascript" src="/static/js/toastr.min.js"></script><link rel="stylesheet" href="/static/css/toastr.min.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">DeepLearning-4.数值计算</h1><a id="logo" href="../../../../.">DeepLearner</a><p class="description">Deep Learning Notes</p></div><div id="nav-menu"><a class="current" href="../../../../."><i class="fa fa-home"> 首页</i></a><a href="../../../../archives/"><i class="fa fa-archive"> 归档</i></a><a href="../../../../tagcloud/"><i class="fa fa-tags"> 标签</i></a><a href="../../../../resource/"><i class="fa fa-cloud"> 资源</i></a><a href="../../../../guestbook/"><i class="fa fa-wechat"> 留言</i></a><a href="../../../../codeline/"><i class="fa fa-circle"> 码迹</i></a><a href="../../../../resume/"><i class="fa fa-user"> 个人简历</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">DeepLearning-4.数值计算</h1><div class="post-meta">created:2022-01-04</div><div class="post-meta">updated:2023-07-30<span> | </span><span class="category"><a href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><a class="disqus-comment-count" href="#waline"><span class="waline-comment-count" id=""></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8A%E6%BA%A2%E5%92%8C%E4%B8%8B%E6%BA%A2"><span class="toc-number">1.</span> <span class="toc-text">上溢和下溢</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.1.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8E%A8%E7%AE%97"><span class="toc-number">1.2.</span> <span class="toc-text">手动推算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%97%85%E6%80%81%E7%9F%A9%E9%98%B5%E4%B8%8E%E6%9D%A1%E4%BB%B6%E6%95%B0"><span class="toc-number">2.</span> <span class="toc-text">病态矩阵与条件数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">基于梯度的优化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">3.1.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8E%A8%E7%AE%97-1"><span class="toc-number">3.2.</span> <span class="toc-text">手动推算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Jacobian%E7%9F%A9%E9%98%B5%E5%92%8CHessian%E7%9F%A9%E9%98%B5"><span class="toc-number">3.3.</span> <span class="toc-text">Jacobian矩阵和Hessian矩阵</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%9B%E9%A1%BF%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">牛顿法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8E%A8%E7%AE%97-2"><span class="toc-number">4.1.</span> <span class="toc-text">手动推算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96"><span class="toc-number">5.</span> <span class="toc-text">约束优化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8E%A8%E7%AE%97-3"><span class="toc-number">5.1.</span> <span class="toc-text">手动推算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98"><span class="toc-number">6.</span> <span class="toc-text">线性最小二乘</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8E%A8%E7%AE%97-4"><span class="toc-number">6.1.</span> <span class="toc-text">手动推算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">6.2.</span> <span class="toc-text">代码实现</span></a></li></ol></li></ol></div></div><div class="post-content"><p>机器学习算法需要大量数值计算，通常是指通过<strong>迭代</strong>过程更新解得估计值来解决数学问题的算法，而不是通过解析过程推导出公式来提供正确解的方法。常见的操作包括<strong>优化</strong>和<strong>线性方程组</strong>的求解。</p>
<span id="more"></span>

<h3 id="上溢和下溢"><a href="#上溢和下溢" class="headerlink" title="上溢和下溢"></a>上溢和下溢</h3><p>&emsp;&emsp;无限多的实数无法在计算机内精确保存，因此计算机保存实数时，几乎总会引入一些近似误差，单纯的舍入误差会导致一些问题，特别是当操作复合时，即使是理论上可行的算法，如果没有考虑最小化舍入误差的累积，在实践时也会导致算法失效。</p>
<p>&emsp;&emsp;<strong>下溢（Underflow）</strong> ： 当接近零的数被四舍五入为零时发生下溢。<br>&emsp;&emsp;<strong>上溢（Overflow）</strong> ： 当大量级的数被近似为$\propto $或$-\propto $时发生上溢。</p>
<p>&emsp;&emsp;必须对上溢和下溢进行数值稳定的一个例子是softmax函数（解决上溢和下溢）。softmax函数经常用于预测和范畴分布相关联的概率，定义为：</p>
<center>$softmax(x)_{i}=\frac{exp(x_{i})}{\sum_{j=1}^{n}exp(x_{j})}$</center>

<p>&emsp;&emsp;当所有$x_{i}$都等于某个常数$c$时，所有的输出都应该为$\frac{1}{n}$，当$$是很小的负数，$exp(c)$就会下溢，函数分母会变为0，所以结果是未定义的。当$c$是非常大的正数时，$exp(c)$的上溢再次导致整个表达式未定义。<br>&emsp;&emsp;两个困难都通过计算$softmax(z)$同时解决，其中$z&#x3D;x-max_{i}x_{i}$ 。</p>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numpy.linalg <span class="keyword">as</span> la</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1e7</span>, <span class="number">1e8</span>, <span class="number">2e5</span>, <span class="number">2e7</span>])</span><br><span class="line">y = np.exp(x) / <span class="built_in">sum</span>(np.exp(x))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;上溢：&quot;</span>, y)</span><br><span class="line">x = x - np.<span class="built_in">max</span>(x) <span class="comment"># 减去最大值</span></span><br><span class="line">y = np.exp(x) / <span class="built_in">sum</span>(np.exp(x))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;上溢处理：&quot;</span>, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">上溢： [nan nan nan nan]</span><br><span class="line">上溢处理： [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([-<span class="number">1e10</span>, -<span class="number">1e9</span>, -<span class="number">2e10</span>, -<span class="number">1e10</span>])</span><br><span class="line">y = np.exp(x) / <span class="built_in">sum</span>(np.exp(x))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;下溢：&quot;</span>, y)</span><br><span class="line">x = x - np.<span class="built_in">max</span>(x) <span class="comment"># 减去最大值</span></span><br><span class="line">y = np.exp(x) / <span class="built_in">sum</span>(np.exp(x))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;下溢处理：&quot;</span>, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;log softmax(x):&quot;</span>, np.log(y))</span><br><span class="line"><span class="comment"># 对 log softmax 下溢的处理：</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logsoftmax</span>(<span class="params">x</span>):</span><br><span class="line">    y = x - np.log(<span class="built_in">sum</span>(np.exp(x)))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;logsoftmax(x):&quot;</span>, logsoftmax(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">下溢： [nan nan nan nan]</span><br><span class="line">下溢处理： [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">log softmax(x): [-inf   <span class="number">0.</span> -inf -inf]</span><br><span class="line">logsoftmax(x): [-<span class="number">9.0e+09</span>  <span class="number">0.0e+00</span> -<span class="number">1.9e+10</span> -<span class="number">9.0e+09</span>]</span><br></pre></td></tr></table></figure>

<h4 id="手动推算"><a href="#手动推算" class="headerlink" title="手动推算"></a>手动推算</h4><h3 id="病态矩阵与条件数"><a href="#病态矩阵与条件数" class="headerlink" title="病态矩阵与条件数"></a>病态矩阵与条件数</h3><p>&emsp;&emsp;在求解方程组时，如果对数据进行较小的扰动，则结果有很大的波动，这样的矩阵称为<strong>病态矩阵</strong> 。病态矩阵是一种特殊矩阵。指条件数很大的非奇异矩阵。病态矩阵的逆和以其为系数矩阵的方程组的界对微小扰动十分敏感，对数值求解会带来很大困难。<font color="#ff0000"><sup>[<a target="_blank" rel="noopener" href="https://www.pianshen.com/article/459993619/">1</a>]</sup></font></p>
<p>例如：现在有线性方程组，$Ax &#x3D; b$， 解方程：</p>
<center>
$$\begin{bmatrix}
 400& -201 \\\\
 -800& 401
\end{bmatrix}
\begin{bmatrix}
x_{1} \\\\
x_{2}
\end{bmatrix} = 
\begin{bmatrix}
200 \\\\
-200
\end{bmatrix}$$
</center>

<p>很容易得到解为：$x1 &#x3D; -100, x2 &#x3D; -200$。如果在样本采集时存在一个微小的误差，比如，将$A$矩阵的系数<strong>400</strong>改变成<strong>401</strong>：则得到一个截然不同的解： $x1 &#x3D; 40000, x2 &#x3D; 79800$.</p>
<center>
$$\begin{bmatrix}
 401& -201 \\\\
 -800& 401
\end{bmatrix}
\begin{bmatrix}
x_{1} \\\\
x_{2}
\end{bmatrix} = 
\begin{bmatrix}
200 \\\\
-200
\end{bmatrix}$$
</center>

<p>当解集 $x$ 对 $A$ 和 $b$ 的系数高度敏感，那么这样的方程组就是病态的 (ill-conditioned)。</p>
<p>&emsp;&emsp;<strong>条件数</strong> ：判断矩阵是否病态以及衡量矩阵的病态程度通常看矩阵$A$的条件数$K(A)$的大小：</p>
<center>$K(A)=\frac{\left\|A^{-1}\right\|}{\left\|A \right\|}$</center>

<p>&emsp;&emsp;$K(A)$称为$A$的条件数，很大时，称$A$为病态，否则称良态；$K(A)$俞大，$A$的病态程度俞严重。</p>
<h3 id="基于梯度的优化方法"><a href="#基于梯度的优化方法" class="headerlink" title="基于梯度的优化方法"></a>基于梯度的优化方法</h3><p>&emsp;&emsp;深度学习算法都涉及到某种形式的优化，优化指改变$x$以最小化或最大化某个函数$f(x)$的任务。<strong>目标函数（Objective Function）</strong> ：把要最小化或最大化的函数称为目标函数。对其进行最小化时，称为<strong>损失函数（Loss Function）</strong> 或 <strong>误差函数（Error Function）</strong>。梯度下降时无约束优化最常用的方法之一，另一种为最小二乘法。</p>
<p>&emsp;&emsp;<strong>梯度下降（Gradient Descent）</strong> <font color="#ff0000"><sup>[<a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/5970503.html">2</a>]</sup></font><font color="#ff0000"><sup>[<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/305638940">3</a>]</sup></font>： 梯度下降简单来说就是一种寻找目标函数最小化的方法，<strong>导数</strong>对于最小化一个函数很有用，代表更改$x$来略微改善$y$，因此可以将$x$往导数的反方向移动一小步来减小$f(x)$，这种技术称为梯度下降。如图所示，梯度下降算法，沿着函数的下坡方向（导数反方向）直到最小。</p>
<p><img src="/images/gradient_descent.png" width="70%"></img></p>
<ul>
<li>对于$x&gt;0$，有$f’(x)&gt;0$，左移来减小$f$。</li>
<li>对于$x&lt;0$，有$f’(x)&lt;0$，右移来减小$f$。</li>
<li>当$f’(x)&#x3D;0$时，导数无法提供往哪个方向移动，$f’(x)&#x3D;0$的点称为<strong>临界点（Critical Point）</strong> 。</li>
</ul>
<p>&emsp;&emsp;临界点分局部极小点、局部极大点、鞍点。当存在多个局部极小点或平坦区域时，优化算法可能无法找到<strong>全局最小点</strong>，在深度学习背景下，即使找到的解不是真正最小的，但只要对应于代价函数显著低的值，通常可以接受这样的解。</p>
<p>&emsp;&emsp;针对多维输入的函数，需要用到<strong>偏导数（Partial Detrivatice）</strong>，<strong>梯度（Gradient）</strong>是相对于一个向量求导的导数：$f$的梯度是包含所有偏导数的向量，记为$\bigtriangledown_{x}f(x)$。在多维情况下，临界点是梯度中所有元素都为零点。</p>
<p>&emsp;&emsp;梯度下降建议新的点为:</p>
<center>$x'=x-\epsilon \bigtriangledown_{x}f(x)$</center>

<p>&emsp;&emsp;其中$\epsilon$为<strong>学习率（Learning Rate）</strong>，是一个确定步长大小的正标量。$\epsilon$的选择方法：选择一个小常数；根据几个$\epsilon$计算$x’$，选择能产生最小目标函数值的$\epsilon$，称为在线搜索。</p>
<h4 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上图中函数公式 f(x)=0.5x^2 梯度下降算法演示</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>): <span class="comment"># f(x)</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * x**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">df</span>(<span class="params">x</span>): <span class="comment"># f(x)导数</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">x, epsilon, iteration</span>):</span><br><span class="line">    iter_num = <span class="number">0</span></span><br><span class="line">    f_change = f_current = f(x)</span><br><span class="line">    <span class="keyword">while</span>(iter_num &lt; iteration <span class="keyword">and</span> f_change &gt; <span class="number">1e-10</span>):</span><br><span class="line">        iter_num += <span class="number">1</span></span><br><span class="line">        x = x - epsilon * df(x)</span><br><span class="line">        f_new = f(x)</span><br><span class="line">        f_change = <span class="built_in">abs</span>(f_current - f_new)</span><br><span class="line">        f_current = f_new</span><br><span class="line">        GD_X.append(x)</span><br><span class="line">        GD_Y.append(f_new)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;第%d次迭代：x=%f,f(x)=%f,df(x)=%f&quot;</span> %(iter_num,x,f_new,df(x)))</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">x = <span class="number">2</span> <span class="comment"># 初始点</span></span><br><span class="line">epsilon = <span class="number">0.1</span> <span class="comment"># 学习率</span></span><br><span class="line">GD_X = []</span><br><span class="line">GD_Y = []</span><br><span class="line">x_g = gradient_descent(x, epsilon, <span class="number">100</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最终优化参数:%.10f&quot;</span> %x_g)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">第<span class="number">1</span>次迭代：x=<span class="number">1.800000</span>,f(x)=<span class="number">1.620000</span>,df(x)=<span class="number">1.800000</span></span><br><span class="line">第<span class="number">2</span>次迭代：x=<span class="number">1.620000</span>,f(x)=<span class="number">1.312200</span>,df(x)=<span class="number">1.620000</span></span><br><span class="line">第<span class="number">3</span>次迭代：x=<span class="number">1.458000</span>,f(x)=<span class="number">1.062882</span>,df(x)=<span class="number">1.458000</span></span><br><span class="line">第<span class="number">4</span>次迭代：x=<span class="number">1.312200</span>,f(x)=<span class="number">0.860934</span>,df(x)=<span class="number">1.312200</span></span><br><span class="line">第<span class="number">5</span>次迭代：x=<span class="number">1.180980</span>,f(x)=<span class="number">0.697357</span>,df(x)=<span class="number">1.180980</span></span><br><span class="line">第<span class="number">6</span>次迭代：x=<span class="number">1.062882</span>,f(x)=<span class="number">0.564859</span>,df(x)=<span class="number">1.062882</span></span><br><span class="line">第<span class="number">7</span>次迭代：x=<span class="number">0.956594</span>,f(x)=<span class="number">0.457536</span>,df(x)=<span class="number">0.956594</span></span><br><span class="line">第<span class="number">8</span>次迭代：x=<span class="number">0.860934</span>,f(x)=<span class="number">0.370604</span>,df(x)=<span class="number">0.860934</span></span><br><span class="line">第<span class="number">9</span>次迭代：x=<span class="number">0.774841</span>,f(x)=<span class="number">0.300189</span>,df(x)=<span class="number">0.774841</span></span><br><span class="line">第<span class="number">10</span>次迭代：x=<span class="number">0.697357</span>,f(x)=<span class="number">0.243153</span>,df(x)=<span class="number">0.697357</span></span><br><span class="line">第<span class="number">11</span>次迭代：x=<span class="number">0.627621</span>,f(x)=<span class="number">0.196954</span>,df(x)=<span class="number">0.627621</span></span><br><span class="line">第<span class="number">12</span>次迭代：x=<span class="number">0.564859</span>,f(x)=<span class="number">0.159533</span>,df(x)=<span class="number">0.564859</span></span><br><span class="line">第<span class="number">13</span>次迭代：x=<span class="number">0.508373</span>,f(x)=<span class="number">0.129222</span>,df(x)=<span class="number">0.508373</span></span><br><span class="line">第<span class="number">14</span>次迭代：x=<span class="number">0.457536</span>,f(x)=<span class="number">0.104670</span>,df(x)=<span class="number">0.457536</span></span><br><span class="line">第<span class="number">15</span>次迭代：x=<span class="number">0.411782</span>,f(x)=<span class="number">0.084782</span>,df(x)=<span class="number">0.411782</span></span><br><span class="line">第<span class="number">16</span>次迭代：x=<span class="number">0.370604</span>,f(x)=<span class="number">0.068674</span>,df(x)=<span class="number">0.370604</span></span><br><span class="line">第<span class="number">17</span>次迭代：x=<span class="number">0.333544</span>,f(x)=<span class="number">0.055626</span>,df(x)=<span class="number">0.333544</span></span><br><span class="line">第<span class="number">18</span>次迭代：x=<span class="number">0.300189</span>,f(x)=<span class="number">0.045057</span>,df(x)=<span class="number">0.300189</span></span><br><span class="line">第<span class="number">19</span>次迭代：x=<span class="number">0.270170</span>,f(x)=<span class="number">0.036496</span>,df(x)=<span class="number">0.270170</span></span><br><span class="line">第<span class="number">20</span>次迭代：x=<span class="number">0.243153</span>,f(x)=<span class="number">0.029562</span>,df(x)=<span class="number">0.243153</span></span><br><span class="line">第<span class="number">21</span>次迭代：x=<span class="number">0.218838</span>,f(x)=<span class="number">0.023945</span>,df(x)=<span class="number">0.218838</span></span><br><span class="line">第<span class="number">22</span>次迭代：x=<span class="number">0.196954</span>,f(x)=<span class="number">0.019395</span>,df(x)=<span class="number">0.196954</span></span><br><span class="line">第<span class="number">23</span>次迭代：x=<span class="number">0.177259</span>,f(x)=<span class="number">0.015710</span>,df(x)=<span class="number">0.177259</span></span><br><span class="line">第<span class="number">24</span>次迭代：x=<span class="number">0.159533</span>,f(x)=<span class="number">0.012725</span>,df(x)=<span class="number">0.159533</span></span><br><span class="line">第<span class="number">25</span>次迭代：x=<span class="number">0.143580</span>,f(x)=<span class="number">0.010308</span>,df(x)=<span class="number">0.143580</span></span><br><span class="line">第<span class="number">26</span>次迭代：x=<span class="number">0.129222</span>,f(x)=<span class="number">0.008349</span>,df(x)=<span class="number">0.129222</span></span><br><span class="line">第<span class="number">27</span>次迭代：x=<span class="number">0.116299</span>,f(x)=<span class="number">0.006763</span>,df(x)=<span class="number">0.116299</span></span><br><span class="line">第<span class="number">28</span>次迭代：x=<span class="number">0.104670</span>,f(x)=<span class="number">0.005478</span>,df(x)=<span class="number">0.104670</span></span><br><span class="line">第<span class="number">29</span>次迭代：x=<span class="number">0.094203</span>,f(x)=<span class="number">0.004437</span>,df(x)=<span class="number">0.094203</span></span><br><span class="line">第<span class="number">30</span>次迭代：x=<span class="number">0.084782</span>,f(x)=<span class="number">0.003594</span>,df(x)=<span class="number">0.084782</span></span><br><span class="line">第<span class="number">31</span>次迭代：x=<span class="number">0.076304</span>,f(x)=<span class="number">0.002911</span>,df(x)=<span class="number">0.076304</span></span><br><span class="line">第<span class="number">32</span>次迭代：x=<span class="number">0.068674</span>,f(x)=<span class="number">0.002358</span>,df(x)=<span class="number">0.068674</span></span><br><span class="line">第<span class="number">33</span>次迭代：x=<span class="number">0.061806</span>,f(x)=<span class="number">0.001910</span>,df(x)=<span class="number">0.061806</span></span><br><span class="line">第<span class="number">34</span>次迭代：x=<span class="number">0.055626</span>,f(x)=<span class="number">0.001547</span>,df(x)=<span class="number">0.055626</span></span><br><span class="line">第<span class="number">35</span>次迭代：x=<span class="number">0.050063</span>,f(x)=<span class="number">0.001253</span>,df(x)=<span class="number">0.050063</span></span><br><span class="line">第<span class="number">36</span>次迭代：x=<span class="number">0.045057</span>,f(x)=<span class="number">0.001015</span>,df(x)=<span class="number">0.045057</span></span><br><span class="line">第<span class="number">37</span>次迭代：x=<span class="number">0.040551</span>,f(x)=<span class="number">0.000822</span>,df(x)=<span class="number">0.040551</span></span><br><span class="line">第<span class="number">38</span>次迭代：x=<span class="number">0.036496</span>,f(x)=<span class="number">0.000666</span>,df(x)=<span class="number">0.036496</span></span><br><span class="line">第<span class="number">39</span>次迭代：x=<span class="number">0.032846</span>,f(x)=<span class="number">0.000539</span>,df(x)=<span class="number">0.032846</span></span><br><span class="line">第<span class="number">40</span>次迭代：x=<span class="number">0.029562</span>,f(x)=<span class="number">0.000437</span>,df(x)=<span class="number">0.029562</span></span><br><span class="line">第<span class="number">41</span>次迭代：x=<span class="number">0.026606</span>,f(x)=<span class="number">0.000354</span>,df(x)=<span class="number">0.026606</span></span><br><span class="line">第<span class="number">42</span>次迭代：x=<span class="number">0.023945</span>,f(x)=<span class="number">0.000287</span>,df(x)=<span class="number">0.023945</span></span><br><span class="line">第<span class="number">43</span>次迭代：x=<span class="number">0.021551</span>,f(x)=<span class="number">0.000232</span>,df(x)=<span class="number">0.021551</span></span><br><span class="line">第<span class="number">44</span>次迭代：x=<span class="number">0.019395</span>,f(x)=<span class="number">0.000188</span>,df(x)=<span class="number">0.019395</span></span><br><span class="line">第<span class="number">45</span>次迭代：x=<span class="number">0.017456</span>,f(x)=<span class="number">0.000152</span>,df(x)=<span class="number">0.017456</span></span><br><span class="line">第<span class="number">46</span>次迭代：x=<span class="number">0.015710</span>,f(x)=<span class="number">0.000123</span>,df(x)=<span class="number">0.015710</span></span><br><span class="line">第<span class="number">47</span>次迭代：x=<span class="number">0.014139</span>,f(x)=<span class="number">0.000100</span>,df(x)=<span class="number">0.014139</span></span><br><span class="line">第<span class="number">48</span>次迭代：x=<span class="number">0.012725</span>,f(x)=<span class="number">0.000081</span>,df(x)=<span class="number">0.012725</span></span><br><span class="line">第<span class="number">49</span>次迭代：x=<span class="number">0.011453</span>,f(x)=<span class="number">0.000066</span>,df(x)=<span class="number">0.011453</span></span><br><span class="line">第<span class="number">50</span>次迭代：x=<span class="number">0.010308</span>,f(x)=<span class="number">0.000053</span>,df(x)=<span class="number">0.010308</span></span><br><span class="line">第<span class="number">51</span>次迭代：x=<span class="number">0.009277</span>,f(x)=<span class="number">0.000043</span>,df(x)=<span class="number">0.009277</span></span><br><span class="line">第<span class="number">52</span>次迭代：x=<span class="number">0.008349</span>,f(x)=<span class="number">0.000035</span>,df(x)=<span class="number">0.008349</span></span><br><span class="line">第<span class="number">53</span>次迭代：x=<span class="number">0.007514</span>,f(x)=<span class="number">0.000028</span>,df(x)=<span class="number">0.007514</span></span><br><span class="line">第<span class="number">54</span>次迭代：x=<span class="number">0.006763</span>,f(x)=<span class="number">0.000023</span>,df(x)=<span class="number">0.006763</span></span><br><span class="line">第<span class="number">55</span>次迭代：x=<span class="number">0.006087</span>,f(x)=<span class="number">0.000019</span>,df(x)=<span class="number">0.006087</span></span><br><span class="line">第<span class="number">56</span>次迭代：x=<span class="number">0.005478</span>,f(x)=<span class="number">0.000015</span>,df(x)=<span class="number">0.005478</span></span><br><span class="line">第<span class="number">57</span>次迭代：x=<span class="number">0.004930</span>,f(x)=<span class="number">0.000012</span>,df(x)=<span class="number">0.004930</span></span><br><span class="line">第<span class="number">58</span>次迭代：x=<span class="number">0.004437</span>,f(x)=<span class="number">0.000010</span>,df(x)=<span class="number">0.004437</span></span><br><span class="line">第<span class="number">59</span>次迭代：x=<span class="number">0.003993</span>,f(x)=<span class="number">0.000008</span>,df(x)=<span class="number">0.003993</span></span><br><span class="line">第<span class="number">60</span>次迭代：x=<span class="number">0.003594</span>,f(x)=<span class="number">0.000006</span>,df(x)=<span class="number">0.003594</span></span><br><span class="line">第<span class="number">61</span>次迭代：x=<span class="number">0.003235</span>,f(x)=<span class="number">0.000005</span>,df(x)=<span class="number">0.003235</span></span><br><span class="line">第<span class="number">62</span>次迭代：x=<span class="number">0.002911</span>,f(x)=<span class="number">0.000004</span>,df(x)=<span class="number">0.002911</span></span><br><span class="line">第<span class="number">63</span>次迭代：x=<span class="number">0.002620</span>,f(x)=<span class="number">0.000003</span>,df(x)=<span class="number">0.002620</span></span><br><span class="line">第<span class="number">64</span>次迭代：x=<span class="number">0.002358</span>,f(x)=<span class="number">0.000003</span>,df(x)=<span class="number">0.002358</span></span><br><span class="line">第<span class="number">65</span>次迭代：x=<span class="number">0.002122</span>,f(x)=<span class="number">0.000002</span>,df(x)=<span class="number">0.002122</span></span><br><span class="line">第<span class="number">66</span>次迭代：x=<span class="number">0.001910</span>,f(x)=<span class="number">0.000002</span>,df(x)=<span class="number">0.001910</span></span><br><span class="line">第<span class="number">67</span>次迭代：x=<span class="number">0.001719</span>,f(x)=<span class="number">0.000001</span>,df(x)=<span class="number">0.001719</span></span><br><span class="line">第<span class="number">68</span>次迭代：x=<span class="number">0.001547</span>,f(x)=<span class="number">0.000001</span>,df(x)=<span class="number">0.001547</span></span><br><span class="line">第<span class="number">69</span>次迭代：x=<span class="number">0.001392</span>,f(x)=<span class="number">0.000001</span>,df(x)=<span class="number">0.001392</span></span><br><span class="line">第<span class="number">70</span>次迭代：x=<span class="number">0.001253</span>,f(x)=<span class="number">0.000001</span>,df(x)=<span class="number">0.001253</span></span><br><span class="line">第<span class="number">71</span>次迭代：x=<span class="number">0.001128</span>,f(x)=<span class="number">0.000001</span>,df(x)=<span class="number">0.001128</span></span><br><span class="line">第<span class="number">72</span>次迭代：x=<span class="number">0.001015</span>,f(x)=<span class="number">0.000001</span>,df(x)=<span class="number">0.001015</span></span><br><span class="line">第<span class="number">73</span>次迭代：x=<span class="number">0.000914</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000914</span></span><br><span class="line">第<span class="number">74</span>次迭代：x=<span class="number">0.000822</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000822</span></span><br><span class="line">第<span class="number">75</span>次迭代：x=<span class="number">0.000740</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000740</span></span><br><span class="line">第<span class="number">76</span>次迭代：x=<span class="number">0.000666</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000666</span></span><br><span class="line">第<span class="number">77</span>次迭代：x=<span class="number">0.000599</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000599</span></span><br><span class="line">第<span class="number">78</span>次迭代：x=<span class="number">0.000539</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000539</span></span><br><span class="line">第<span class="number">79</span>次迭代：x=<span class="number">0.000485</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000485</span></span><br><span class="line">第<span class="number">80</span>次迭代：x=<span class="number">0.000437</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000437</span></span><br><span class="line">第<span class="number">81</span>次迭代：x=<span class="number">0.000393</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000393</span></span><br><span class="line">第<span class="number">82</span>次迭代：x=<span class="number">0.000354</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000354</span></span><br><span class="line">第<span class="number">83</span>次迭代：x=<span class="number">0.000319</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000319</span></span><br><span class="line">第<span class="number">84</span>次迭代：x=<span class="number">0.000287</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000287</span></span><br><span class="line">第<span class="number">85</span>次迭代：x=<span class="number">0.000258</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000258</span></span><br><span class="line">第<span class="number">86</span>次迭代：x=<span class="number">0.000232</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000232</span></span><br><span class="line">第<span class="number">87</span>次迭代：x=<span class="number">0.000209</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000209</span></span><br><span class="line">第<span class="number">88</span>次迭代：x=<span class="number">0.000188</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000188</span></span><br><span class="line">第<span class="number">89</span>次迭代：x=<span class="number">0.000169</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000169</span></span><br><span class="line">第<span class="number">90</span>次迭代：x=<span class="number">0.000152</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000152</span></span><br><span class="line">第<span class="number">91</span>次迭代：x=<span class="number">0.000137</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000137</span></span><br><span class="line">第<span class="number">92</span>次迭代：x=<span class="number">0.000123</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000123</span></span><br><span class="line">第<span class="number">93</span>次迭代：x=<span class="number">0.000111</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000111</span></span><br><span class="line">第<span class="number">94</span>次迭代：x=<span class="number">0.000100</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000100</span></span><br><span class="line">第<span class="number">95</span>次迭代：x=<span class="number">0.000090</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000090</span></span><br><span class="line">第<span class="number">96</span>次迭代：x=<span class="number">0.000081</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000081</span></span><br><span class="line">第<span class="number">97</span>次迭代：x=<span class="number">0.000073</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000073</span></span><br><span class="line">第<span class="number">98</span>次迭代：x=<span class="number">0.000066</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000066</span></span><br><span class="line">第<span class="number">99</span>次迭代：x=<span class="number">0.000059</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000059</span></span><br><span class="line">第<span class="number">100</span>次迭代：x=<span class="number">0.000053</span>,f(x)=<span class="number">0.000000</span>,df(x)=<span class="number">0.000053</span></span><br><span class="line">最终优化参数:<span class="number">0.0000531228</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最终优化参数x并非真实的0,而是无限逼近0，这是梯度下降缺点之一，没法非常精确，但在可接受范围内。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化梯度下降过程</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">X = np.arange(-<span class="number">2</span>, <span class="number">2</span>, <span class="number">0.05</span>)</span><br><span class="line">Y = f(X)</span><br><span class="line">Y = np.array(Y)</span><br><span class="line">plt.plot(X,Y)</span><br><span class="line">plt.scatter(GD_X, GD_Y)</span><br><span class="line">plt.title(<span class="string">&quot;$f(x) = 0.5x^2$&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/gradient_descent1.png"></img></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调整学习率为1.5</span></span><br><span class="line">plt.plot(GD_X, GD_Y)</span><br><span class="line">plt.title(<span class="string">&quot;$f(x) = 0.5x^2$&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/gradient_descent2.png"></img></p>
<h4 id="手动推算-1"><a href="#手动推算-1" class="headerlink" title="手动推算"></a>手动推算</h4><p><img src="/images/gradient_descent3.png" width="800px"></img></p>
<h4 id="Jacobian矩阵和Hessian矩阵"><a href="#Jacobian矩阵和Hessian矩阵" class="headerlink" title="Jacobian矩阵和Hessian矩阵"></a>Jacobian矩阵和Hessian矩阵</h4><p>&emsp;&emsp;<strong>Jacobian矩阵</strong> ：有时我们需要计算输入和输出都为向量的函数的所有偏导数，包含所有这样的偏导数的矩阵称为Jacobian矩阵。定义：有一个函数$f:\mathbb{R}^{m}\rightarrow \mathbb{R}^{n}$，$f$的Jacobian矩阵$J\in \mathbb{R}^{n\times m}$，定义为$J_{i,j}&#x3D;\frac{\partial }{\partial x_{j}}f(x)_{i}$。</p>
<p>&emsp;&emsp;<strong>Hessian矩阵</strong>：当函数具有多维输入时，二阶导数有很多。将这些导数合并为一个矩阵，称为Hessian矩阵，定义为：</p>
<center>$H(f)(x)_{i,j}=\frac{\partial ^{2}}{\partial x_{i}\partial x_{j}}f(x)$</center>

<p>&emsp;&emsp;Hessian矩阵等价于梯度的Jacobian矩阵。</p>
<h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><p>&emsp;&emsp;<strong>牛顿法（Newton’s Method）</strong>：基于宇哥二阶泰勒展开来近似$x^{(0)}$附近的$f(x)$：</p>
<center>$f(x)\approx f(x^{(0)})+(x-x^{(0)})^{\top}\bigtriangledown _{x}f(x^{(0)})+\frac{1}{2}(x-x^{(0)})^{\top }H(f)(x^{(0)})(x-x^{(0)})$</center>

<p>&emsp;&emsp;接着通过计算，可以得到这个函数的临界点：</p>
<center>$x^{*}=x^{(0)}-H(f)(x^{(0)})^{-1}\bigtriangledown _{x}f(x^{(0)})$</center>

<p>&emsp;&emsp;牛顿法迭代更新近似函数和跳到近似函数的最小点可以比梯度下降法更快地到达临界点。这在接近全局极小时是一个特别有用的性质，但在鞍点附近是有害的。<br>&emsp;&emsp;针对上述实例，计算得到：$H&#x3D;A^{\top}A$，进一步计算得到最优解：</p>
<center>$x^{*}=x^{(0)}-(A^{\top}A)^{-1}(A^{\top}Ax^{(0)}-A^{\top}b)=(A^{\top}A)^{-1}A^{\top}b$</center>

<h4 id="手动推算-2"><a href="#手动推算-2" class="headerlink" title="手动推算"></a>手动推算</h4><h3 id="约束优化"><a href="#约束优化" class="headerlink" title="约束优化"></a>约束优化</h3><p>&emsp;&emsp;<strong>约束优化（Constrained Optimization）</strong> : $x$在某些集合$S$中找$f(x)$的最大值和最小值，而非在所有值下的最大和最小值，这称为约束优化。<br>&emsp;&emsp;通过$m$个函数$g^{(i)}$和$n$个函数$h^{(j)}$描述$S$，那么$S$可以表示为$S&#x3D;\left\{x\mid \forall i,g^{(i)}(x)&#x3D;0 and \forall j,h^{(j)}(x)\leqslant 0 \right\}$。其中涉及$g^{(i)}$的等式称为等式约束，涉及$h^{(j)}$的不等式称为不等式约束。</p>
<p>&emsp;&emsp;为每个约束引入新的变量$\lambda _{i}$和$\alpha _{j}$，这些新变量被称为KKT乘子。广义拉格朗日式定义：</p>
<center>$L(x,\lambda ,\alpha )=f(x)+\sum_{i}\lambda _{i}g^{(i)}(x)+\sum_{j}\lambda _{j}h^{(j)}(x)$</center>

<p>&emsp;&emsp;可以通过优化无约束的广义拉格朗日式解决约束最小化问题：</p>
<center>$\underset{x}{min}\ \underset{\lambda}{max}\ \underset{\alpha ,\alpha \geqslant 0}{max}L(x,\lambda,\alpha)$</center>

<p>&emsp;&emsp;优化该式与下式等价：</p>
<center>$\underset{m\in S}{min}\ f(x)$</center>

<p>&emsp;&emsp;针对上述实例，约束优化：$x^{\top}x\leqslant 1$</p>
<p>&emsp;&emsp;引入广义拉格朗日式：</p>
<center>$L(x,\lambda)=f(x)+\lambda (x^{\top}x-1)$</center>

<p>&emsp;&emsp;解决以下问题：</p>
<center>$\underset{x}{min}\ \underset{\lambda,\lambda \geqslant 0}{max}L(x,\lambda)$</center>

<p>&emsp;&emsp;关于$x$对于Lagrangian微分，我们得到方程：</p>
<center>$A^{\top}Ax-A^{\top}b+2\lambda x=0$</center>

<p>&emsp;&emsp;得到解的形式：</p>
<center>$x=(A^{\top}A+2\lambda I)^{-1}A^{\top}b$</center>

<p>&emsp;&emsp;$\lambda$的选择必须使结果服从约束，可以对$\lambda$梯度上升找到这个值：</p>
<center>$\frac{\partial }{\partial \lambda}L(x,\lambda)=x^{\top}x-1$</center>

<h4 id="手动推算-3"><a href="#手动推算-3" class="headerlink" title="手动推算"></a>手动推算</h4><h3 id="线性最小二乘"><a href="#线性最小二乘" class="headerlink" title="线性最小二乘"></a>线性最小二乘</h3><p>&emsp;&emsp;<strong>最小二乘法</strong>：<font color="#ff0000"><sup>[<a target="_blank" rel="noopener" href="https://www.pianshen.com/article/2053395762">4</a>]</sup></font><font color="#ff0000"><sup>[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/140377384">5</a>]</sup></font><font color="#ff0000"><sup>[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38128785">6</a>]</sup></font>用来做函数拟合或者求函数极值的方法，在机器学习中，在回归模型中较为常见。</p>
<p>&emsp;&emsp;例如：引入实例：</p>
<center>$f(x)=\frac{1}{2}\left\| Ax-b\right\|^{2}_{2}$</center>

<p>&emsp;&emsp;假设我们希望找到最⼩化该式的$x$值，计算梯度得到：</p>
<center>$\bigtriangledown _{x}f(x)=A^{\top}(Ax-b)=A^{\top}Ax-A^{\top}b$</center>

<h4 id="手动推算-4"><a href="#手动推算-4" class="headerlink" title="手动推算"></a>手动推算</h4><h4 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numpy.linalg <span class="keyword">as</span> la</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matmul_chain</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(args) == <span class="number">0</span>: <span class="keyword">return</span> np.nan</span><br><span class="line">    result = args[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> args[<span class="number">1</span>:]:</span><br><span class="line">        result = result@x</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">牛顿法</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">newton</span>(<span class="params">x, A, b, delta</span>):</span><br><span class="line">    x = matmul_chain(np.linalg.inv(matmul_chain(A.T, A)), A.T, b)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">梯度下降法</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_decent</span>(<span class="params">x, A, b, epsilon, delta</span>):</span><br><span class="line">    <span class="keyword">while</span> la.norm(matmul_chain(A.T, A, x) - matmul_chain(A.T, b)) &gt; delta:</span><br><span class="line">        x -= epsilon*(matmul_chain(A.T, A, x) - matmul_chain(A.T, b))</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">约束优化</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">constrain_opti</span>(<span class="params">x, A, b, delta</span>):</span><br><span class="line">    k = <span class="built_in">len</span>(x)</span><br><span class="line">    lamb = <span class="number">0</span></span><br><span class="line">    <span class="comment"># delta 设为 5e-2，最优设为 0 x = matmul_chain(np.linalg.inv(matmul_chain(A.T, A)+2*lamb*np.identity(k)), A.T, b)</span></span><br><span class="line">    <span class="keyword">while</span> np.<span class="built_in">abs</span>(np.dot(x.T, x) - <span class="number">1</span>) &gt; <span class="number">5e-2</span>: </span><br><span class="line">        lamb += np.dot(x.T, x) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x0 = np.array([<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>])</span><br><span class="line">A = np.array([[<span class="number">1.0</span>, -<span class="number">2.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">2.0</span>, -<span class="number">8.0</span>], [-<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">9.0</span>]])</span><br><span class="line">b = np.array([<span class="number">0.0</span>, <span class="number">8.0</span>, -<span class="number">9.0</span>])</span><br><span class="line">epsilon = <span class="number">0.01</span></span><br><span class="line">delta = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;牛顿法：&quot;</span>, newton(x0, A, b, delta))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;梯度下降法：&quot;</span>, gradient_decent(x0, A, b, epsilon, delta))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;约束优化：&quot;</span>, constrain_opti(x0, A, b, delta))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">牛顿法： [<span class="number">29.</span> <span class="number">16.</span>  <span class="number">3.</span>]</span><br><span class="line">梯度下降法： [<span class="number">27.82277014</span>, <span class="number">15.34731055</span>, <span class="number">2.83848939</span>]</span><br><span class="line">约束优化： [<span class="number">0.23637902</span>, <span class="number">0.05135858</span>, -<span class="number">0.94463626</span>]</span><br></pre></td></tr></table></figure>


</div><script type="text/javascript" src="../../../../js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://deeplearner.top/2022/01/04/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97/" data-id="clkor69p1003wc6s6ecckfqd5" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAEmCAAAAADqr2IGAAAEjElEQVR42u3ay26jQBAF0Pn/n85IsxopsX1vFZbc5LCKiIHmsOh6/fkTH1//jkfn///v978f/SY5//25j571fD3Pr7rswIQJEyZMH8n09fTIXyxnaj9Gcufnq2rv88O1mDBhwoTpcKb8dsl/Z5txHgq0lHlI8fA8JkyYMGH6BUzPE84cN0Gf3XlzFSZMmDBhwtQWfJMEe1NcTs4kSTImTJgwYfoNTPnt2ofNXmnWnkyS8LfXwjFhwoQJ04cxtYM7d/r7jfNNmDBhwoTpA5i+yiNvQM5S3Hw7n7VIh2+NCRMmTJiOZcpbjJsAIk9Q28JxPiR0wRMxYcKECdOBTLOSbpIY5wOmeSiwYdqk8ZgwYcKE6R5M+fBN0pLMm4ttETZfQ7uehyvEhAkTJkzHMl07TDMr1256iDnxLODAhAkTJkz3YJqli/sFbRLv9szsuUUkggkTJkyYPp6pvThPMmdDP/vNftayLSrimDBhwoTp45na0c9Z4npVSNE2JvPBo+EkLyZMmDBh+nimq7b8diw1DxGKuCZ+4VnrFBMmTJgwnciUb8ntFp5v1e3wzSYEGRaXMWHChAnTLZhmAzrRTFA7BnppIr0ZvcWECRMmTHdi2mzke8pNYrxJuSM+TJgwYcJ0LFN762tbhpvNuyWbjbRiwoQJE6Z7MO232BnZrEk5C0c2HxgTJkyYMN2PaVaKbY+rirztmbZNiwkTJkyYTmfacOSBwmYcZ/b0qz4kJkyYMGE6l6ktp+6Bko25HWlN+JL3ejGiigkTJkyYDmTKx2tmbcI8nU6W26apw4HUNjbBhAkTJkwfzzQb35klsTnH7Np9khzBYcKECROmQ5g2xdDNtr0vKOdJ7P6NMGHChAnTPZiSRbQLbZPM2SBsHgrMgh5MmDBhwnQuU9JcTFLZ/Jft+Zwmz/XzZmqUu2PChAkTpg9masupbTqa4+5DgTZQKD4/JkyYMGE6nClPUGdty9n2v396MiYbhUSYMGHChOlYprZNmKSUmw17M3yzX8mLMAgTJkyYMB3ItB/0zLfhTTCxT4nbpuaLri8mTJgwYTqEqd3mrx2amY34zIaN8pVHvV9MmDBhwnQIU7ugdyyxDSY26fEwGcaECRMmTMcy5e3G2VjPO8Z9kpfMR12LdiYmTJgwYTqQKS/CtonxbEQ1+aqzzH5VgMaECRMmTLdjakdk8vP7p7fDqReEJpgwYcKE6XCmvCSaF17zu80GWDel4TzseDHfhAkTJkyYjmJqU+KocrwIDtrtf9PUxIQJEyZMd2W6tgg7a21u2p9J2Tdvuxbrx4QJEyZMhzB9lcemwZn8cpYqtx8p/+QPB3cwYcKECdMhTO/bkt8xDJSn3wlr/tkwYcKECdPpTHk78LJRmNEmPcvm2zQeEyZMmDDdlandsPMkeV94zRurw9p23sLEhAkTJky3ZmpHc/bl13YQ9qpP/sMTMWHChAnTr2fKt9j2fBuUtGXr6ANjwoQJE6bDmWZF3nb8JR8G2rdR25W8eDomTJgwYTqW6artM99c25S1LQfniLNnYcKECROmQ5j+AoY+OboSPSkMAAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="../../../../tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><i class="fa fa-tag"></i>深度学习笔记</a><a href="../../../../tags/%E8%8A%B1%E4%B9%A6/"><i class="fa fa-tag"></i>花书</a></div><div class="post-nav"><a class="pre" href="../../10/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-5-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">DeepLearning-5.机器学习基础</a><a class="next" href="../../../../2021/12/22/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-%E6%A6%82%E7%8E%87%E4%B8%8E%E4%BF%A1%E6%81%AF%E8%AE%BA/">DeepLearning-3.概率与信息论</a></div><div class="nofancybox" id="waline"></div><script src="//unpkg.com/@waline/client@v2/dist/waline.js"></script><link rel="stylesheet" type="text/css" href="//unpkg.com/@waline/client@v2/dist/waline.css"><script>let metaInfo = ['nick', 'mail', 'link']
let requiredMeta = 'nick'.split(',').filter(item => {
  return metaInfo.indexOf(item) > -1
})
Waline.init({
  el: '#waline',
  comment: true,
  serverURL: 'https://waline-deeplearner.vercel.app',
  pageSize: '10',
  wordLimit: '500',
  requiredMeta,
})
</script></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/images/blog/avatar_new.png"/></a><p>心外无物，知行合一</p><a class="info-icon" href="/resume/" title="个人简历" target="_blank" style="margin-inline:5px"> <i class="fa fa-share-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:conghaoyuan@gmail.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/conghaoyuan" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/AIGC/">AIGC</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Algorithm/">Algorithm</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/CV/">CV</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/GUI/">GUI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/NLP/">NLP</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Python/">Python</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%97%E4%BA%BA/">数字人</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%A6/">数学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/">深度学习模型</a><span class="category-list-count">11</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/10/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8B%E9%80%9A%E4%B9%89Prompt%E5%9B%9B%E5%8D%81%E5%BC%8F/">AIGC-LLM-辟邪剑谱之通义Prompt四十式</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/06/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8BPrompt%20Engineering/">AIGC-LLM-辟邪剑谱之Prompt Engineering</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/11/18/AIGC-Agent-%E4%BD%9C%E6%96%87%E6%89%B9%E6%94%B9%E6%A0%87%E6%B3%A8/">AIGC-Agent-作文批改标注</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/09/27/LLama32%E8%B0%83%E7%A0%94/">LLama3.2调研报告</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/04/09/AIGC-LLM-Prompt%E6%8A%80%E8%83%BD/">AIGC-LLM-Prompt工程技能综述</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/03/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-BERT/">Deeeplearning模型-NLP-BERT</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/01/16/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E8%AF%AD%E6%96%99%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/">Deeeplearning模型-NLP-大语言模型微调语料生成工具</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-Transformer%E6%A8%A1%E5%9E%8B/">Deeeplearning模型-NLP-Transformer模型</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/01/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-NLP%E5%9F%BA%E7%A1%80/">Deeeplearning模型-NLP-NLP基础</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/10/26/%E6%95%B0%E5%AD%97%E4%BA%BA-%E6%95%B0%E5%AD%97%E4%BA%BA%E6%8A%80%E6%9C%AF%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94/">数字人-数字人技术效果对比</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://conghaoyuan.github.io" title="Fern[个人早期博客]" target="_blank">Fern[个人早期博客]</a></div><div class="widget"><div class="widget-title"><i class="fa fa-star"> 天文学</i></div><ul></ul><a href="/tools/tianwenli.html" title="天文历" target="_self">天文历</a></div><div class="widget"><div class="widget-title"><i class="fa fa-plus"> 数学小工具</i></div><ul></ul><a href="/tools/buy_house.html" title="丐版买房计算器" target="_self">丐版买房计算器</a><ul></ul><a href="/tools/equation_12.html" title="一元二次方程" target="_self">一元二次方程</a><ul></ul><a href="/tools/equation_line.html" title="直线方程" target="_self">直线方程</a><ul></ul><a href="/tools/equation_polynomial.html" title="多项式方程" target="_self">多项式方程</a><ul></ul><a href="/tools/regression_ridge.html" title="岭回归" target="_self">岭回归</a><ul></ul><a href="/tools/fourier_transform.html" title="傅里叶变换" target="_self">傅里叶变换</a><ul></ul><a href="/tools/gradient_descent.html" title="梯度下降" target="_self">梯度下降</a><ul></ul><a href="/tools/function_e.html" title="指数函数" target="_self">指数函数</a><ul></ul><a href="/tools/function_inverse_scale.html" title="反比例函数" target="_self">反比例函数</a><ul></ul><a href="/tools/function_log.html" title="对数函数" target="_self">对数函数</a><ul></ul><a href="/tools/function_sin.html" title="正弦函数" target="_self">正弦函数</a><ul></ul><a href="/tools/coord_grid.html" title="网格化坐标" target="_self">网格化坐标</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="../../../../." rel="nofollow">DeepLearner.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="../../../../js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="../../../../js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="../../../../css/search.css?v=1.0.0"><script type="text/javascript" src="../../../../js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="/static/js/MathJax.js?config=TeX-MML-AM_CHTML" async></script><div id="script" type="text/javascript" src="../../../../js/mathjaxs.js" async></div><script type="text/javascript" src="../../../../js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="../../../../js/smartresize.js?v=1.0.0"></script></div><!-- hexo injector body_end start -->
<script src="assets/prism-bundle.js"></script>
<script src="assets/prism-plus.js" data-pjax=""></script>
<!-- hexo injector body_end end --></body></html>