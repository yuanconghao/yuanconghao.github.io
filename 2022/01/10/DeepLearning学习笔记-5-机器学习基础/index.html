<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Just Do It"><title>DeepLearning-5.机器学习基础 | DeepLearner</title><link rel="stylesheet" type="text/css" href="../../../../css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/static/css/normalize.css"><link rel="stylesheet" type="text/css" href="/static/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/static/css/grids-responsive-min.css"><link rel="stylesheet" href="/static/css/font-awesome.min.css"><script type="text/javascript" src="/static/js/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="../../../../favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="../../../../favicon.ico"><link rel="apple-touch-icon" href="../../../../apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="../../../../apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="../../../../atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-83251621-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + 'b207d9febf3295d375e6e9b42006f9eb';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/static/js/clipboard.min.js"></script><script type="text/javascript" src="/static/js/toastr.min.js"></script><link rel="stylesheet" href="/static/css/toastr.min.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">DeepLearning-5.机器学习基础</h1><a id="logo" href="../../../../.">DeepLearner</a><p class="description">Deep Learning Notes</p></div><div id="nav-menu"><a class="current" href="../../../../."><i class="fa fa-home"> 首页</i></a><a href="../../../../archives/"><i class="fa fa-archive"> 归档</i></a><a href="../../../../tagcloud/"><i class="fa fa-tags"> 标签</i></a><a href="../../../../resource/"><i class="fa fa-cloud"> 资源</i></a><a href="../../../../guestbook/"><i class="fa fa-wechat"> 留言</i></a><a href="../../../../codeline/"><i class="fa fa-circle"> 码迹</i></a><a href="../../../../resume/"><i class="fa fa-user"> 个人简历</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">DeepLearning-5.机器学习基础</h1><div class="post-meta">created:2022-01-10</div><div class="post-meta">updated:2023-07-30<span> | </span><span class="category"><a href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><a class="disqus-comment-count" href="#waline"><span class="waline-comment-count" id=""></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1T"><span class="toc-number">1.1.</span> <span class="toc-text">任务T</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8FP"><span class="toc-number">1.2.</span> <span class="toc-text">性能度量P</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8CE"><span class="toc-number">1.3.</span> <span class="toc-text">经验E</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.4.</span> <span class="toc-text">示例：线性回归</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%B9%E9%87%8F%E3%80%81%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-number">2.</span> <span class="toc-text">容量、过拟合和欠拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E5%8D%88%E9%A4%90%E5%AE%9A%E7%90%86"><span class="toc-number">2.1.</span> <span class="toc-text">没有免费午餐定理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">2.2.</span> <span class="toc-text">正则化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E5%92%8C%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="toc-number">3.</span> <span class="toc-text">超参数和验证集</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">3.1.</span> <span class="toc-text">交叉验证</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#k-%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E7%AE%97%E6%B3%95Define-KFlodXV-mathbb-D-A-L-k-Require-mathbb-D-%E4%B8%BA%E7%BB%99%E5%AE%9A%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E5%85%B6%E4%B8%AD%E5%85%83%E7%B4%A0%E4%B8%BA-z-i-Require-A-%E4%B8%BA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%8C%E5%8F%AF%E8%A7%86%E4%B8%BA%E4%B8%80%E4%B8%AA%E5%87%BD%E6%95%B0%EF%BC%88%E4%BD%BF%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%9C%E4%B8%BA%E8%BE%93%E5%85%A5%EF%BC%8C%E8%BE%93%E5%87%BA%E4%B8%80%E4%B8%AA%E5%AD%A6%E5%A5%BD%E7%9A%84%E5%87%BD%E6%95%B0%EF%BC%89Require-L-%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E5%8F%AF%E8%A7%86%E4%B8%BA%E6%9D%A5%E8%87%AA%E5%AD%A6%E5%A5%BD%E7%9A%84%E5%87%BD%E6%95%B0-f-%EF%BC%8C%E5%B0%86%E6%A0%B7%E6%9C%AC-z-i-in-mathbb-D-%E6%98%A0%E5%B0%84%E5%88%B0-mathbb-R-%E4%B8%AD%E6%A0%87%E9%87%8F%E7%9A%84%E5%87%BD%E6%95%B0Require-k-%E4%B8%BA%E6%8A%98%E6%95%B0-emsp-emsp-%E5%B0%86-mathbb-D-%E5%88%86%E4%B8%BA-k-%E4%B8%AA%E4%BA%92%E6%96%A5%E5%AD%90%E9%9B%86-mathbb-D-i-%EF%BC%8C%E5%AE%83%E4%BB%AC%E7%9A%84%E5%B9%B6%E9%9B%86%E4%B8%BA-mathbb-D-emsp-emsp-for-i-from-1-to-k-do-emsp-emsp-emsp-emsp-f-i-A-mathbb-D-setminus-mathbb-D-i-emsp-emsp-emsp-emsp-for-z-j-in-mathbb-D-i-do-emsp-emsp-emsp-emsp-emsp-emsp-e-j-L-f-i-z-j-emsp-emsp-emsp-emsp-end-for-emsp-emsp-end-for-emsp-emsp-Return-e"><span class="toc-number"></span> <span class="toc-text">k-折交叉验证算法Define KFlodXV($\mathbb{D},A,L,k$):Require: $\mathbb{D}$为给定的数据集，其中元素为$z^{(i)}$Require: $A$为学习算法，可视为一个函数（使用数据集作为输入，输出一个学好的函数）Require: $L$为损失函数，可视为来自学好的函数$f$，将样本$z^{(i)}\in \mathbb{D}$映射到$\mathbb{R}$中标量的函数Require: $k$为折数  将$\mathbb{D}$分为$k$个互斥子集$\mathbb{D}_{i}$，它们的并集为$\mathbb{D}$  for $i$ from 1 to $k$ do    $f_{i}&#x3D;A(\mathbb{D}\setminus \mathbb{D}_{i})$    for $z^{(j)}$ in $\mathbb{D}_{i}$ do      $e_{j}&#x3D;L(f_{i},z^{(j)})$    end for  end for  Return $e$</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">0.0.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%B0%E8%AE%A1%E3%80%81%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="toc-number">1.</span> <span class="toc-text">估计、偏差和方差</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%82%B9%E4%BC%B0%E8%AE%A1"><span class="toc-number">1.1.</span> <span class="toc-text">点估计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE"><span class="toc-number">1.2.</span> <span class="toc-text">偏差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E5%B7%AE%E5%92%8C%E6%A0%87%E5%87%86%E5%B7%AE"><span class="toc-number">1.3.</span> <span class="toc-text">方差和标准差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E4%B8%8E%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">1.4.</span> <span class="toc-text">误差与偏差和方差的关系</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="toc-number">2.</span> <span class="toc-text">最大似然估计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8E%A8%E7%AE%97"><span class="toc-number">2.1.</span> <span class="toc-text">手动推算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">2.2.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E7%9A%84%E6%80%A7%E8%B4%A8"><span class="toc-number">2.3.</span> <span class="toc-text">最大似然的性质</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1"><span class="toc-number">3.</span> <span class="toc-text">贝叶斯统计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8E%A8%E7%AE%97-1"><span class="toc-number">3.1.</span> <span class="toc-text">手动推算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">3.2.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1"><span class="toc-number">3.3.</span> <span class="toc-text">最大后验估计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-3"><span class="toc-number">3.4.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">监督学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">4.1.</span> <span class="toc-text">支持向量机</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E7%AE%80%E5%8D%95%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text">其他简单监督学习算法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">无监督学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90"><span class="toc-number">5.1.</span> <span class="toc-text">主成分分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#k-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB"><span class="toc-number">5.2.</span> <span class="toc-text">k-均值聚类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">6.</span> <span class="toc-text">随机梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">7.</span> <span class="toc-text">构建机器学习算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%91%E5%B1%95%E7%9A%84%E6%8C%91%E6%88%98"><span class="toc-number">8.</span> <span class="toc-text">深度学习发展的挑战</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE"><span class="toc-number">8.0.1.</span> <span class="toc-text">维数灾难</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E4%B8%8D%E5%8F%98%E5%BD%A2%E5%92%8C%E5%B9%B3%E6%BB%91%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">8.1.</span> <span class="toc-text">局部不变形和平滑正则化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%81%E8%A1%8C%E5%AD%A6%E4%B9%A0"><span class="toc-number">8.2.</span> <span class="toc-text">流行学习</span></a></li></ol></li></ol></div></div><div class="post-content"><p>深度学习是机器学习的一个特定分支。</p>
<span id="more"></span>

<h3 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h3><p>&emsp;&emsp;机器学习算法描述一种能够<strong>从数据中学习的算法</strong>。<strong>学习</strong>指对于某类<strong>任务T</strong>和<strong>性能度量P</strong>，一个计算机程序被认为可以从<strong>经验E</strong>中学习，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。</p>
<h4 id="任务T"><a href="#任务T" class="headerlink" title="任务T"></a>任务T</h4><p>&emsp;&emsp;机器学习任务定义为机器学习系统应该如何处理样本（Example）。样本是指从某些机器学习系统处理的对象或事件中收集到的已经量化的特征（Feature）的集合，用向量$x \in R^{n}$表示，其中向量的每个元素$x_{i}$是一个特征。常见的机器学习任务如分类、输入缺失分类、回归、转录、机器翻译、结构化输出、异常检测、合成和采样、缺失值填补、去噪、密度估计或概率质量函数估计。</p>
<h4 id="性能度量P"><a href="#性能度量P" class="headerlink" title="性能度量P"></a>性能度量P</h4><p>&emsp;&emsp;为了评估机器学习算法的优劣，需要对算法的输出结果进行定量的衡量分析，需要合适的性能度量指标：</p>
<p>&emsp;&emsp;针对分类任务：</p>
<ul>
<li>准确率：<br>$Accuracy&#x3D;\frac{TP+TN}{TP+TN+FP+FN}$</li>
<li>错误率：<br>$Errorrate&#x3D;1-Accuracy$</li>
<li>精确率：<br>$Precision&#x3D;\frac{TP}{TP+FP}$</li>
<li>召回率：<br>$Recall&#x3D;\frac{TP}{TP+FN}$</li>
<li>F1值：<br>$F1&#x3D;\frac{2\cdot Precision\cdot Recall}{Precision+Recall}$</li>
</ul>
<table>
<thead>
<tr>
<th>指标</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>TP</td>
<td>True Positive，正样本预测为正例数目</td>
</tr>
<tr>
<td>TN</td>
<td>True Negative，正样本预测为负例数目</td>
</tr>
<tr>
<td>FP</td>
<td>False Positive，负样本预测为正例数目</td>
</tr>
<tr>
<td>FN</td>
<td>False Negative，负样本预测为负例数目</td>
</tr>
</tbody></table>
<p>&emsp;&emsp;针对回归任务：距离误差</p>
<h4 id="经验E"><a href="#经验E" class="headerlink" title="经验E"></a>经验E</h4><p>&emsp;&emsp;根据经验E的不同，机器学习算法可以分为：无监督学习算法（Unsupervised Learning）和监督学习算法（Supervised Learning）。</p>
<ul>
<li>无监督学习：训练含有很多<strong>样本特征</strong>的数据集，算法需要从中学习出特征中隐藏的结构性质。例如：密度估计、聚类。</li>
<li>监督学习：训练含有很多特征（样本特征和标签值）的数据集。例如：分类、回归。</li>
</ul>
<h4 id="示例：线性回归"><a href="#示例：线性回归" class="headerlink" title="示例：线性回归"></a>示例：线性回归</h4><p>&emsp;&emsp;<strong>线性回归（Linear Regression）</strong>的目标：建立一个系统，将向量$x \in R^{n}$作为输入，预测标量$y \in R$作为输出。线性回归的输出是其输入的线性函数。令$\hat{y}$表示模型预测$y$应该取得值：</p>
<center>$\hat{y}=w^{\top}x$</center>
&emsp;&emsp;其中$w \in R^{n}$是参数向量。

<p>&emsp;&emsp;性能度量P的定义：假设测试集的特征和标签分别用$X^{(test)}$和$y^{(test)}$表示。<br>&emsp;&emsp;性能度量的方式：均方误差（Mean Squared Error），如果$\hat{y}^{(test)}$表示模型在测试集上的预测值，那么均方误差公式：</p>
<center>$MSE_{test}=\frac{1}{m}\sum_{i}(\hat{y}^{(test)}-y^{(test)})^{2}_{i}=\frac{1}{m}\left\|\hat{y}^{(test)}-y^{(test)} \right\|^{2}_{2}$</center>

<p>&emsp;&emsp;当预测值和目标值之间的欧氏距离增加时，误差也会增加。为了构建一个机器学习算法，设计的算法通过观察训练集获取经验，减少$MSE_{test}$来改进权重$w$。一种直观方式是最小化训练集上的均方误差，即$MSE_{train}$。最小化$MSE_{train}$可以对导数为0进行求解：</p>
<center>$\triangledown _{w}MSE_{train}=0$</center>
<center>$\Rightarrow \triangledown _{w}\frac{1}{m}\left\|\hat{y}^{(train)}-y^{(train)}\right\|^{2}_{2}=0 $</center>
<center>$\Rightarrow \frac{1}{m} \triangledown _{w}\left\|X^{(train)}w-y^{(train)}\right\|^{2}_{2}=0$</center>
<center>$\Rightarrow \triangledown _{w}(X^{(train)}w-y^{(train)})^{\top}(X^{(train)}w-y^{(train)})=0$</center>
<center>$\Rightarrow \triangledown _{w}(w^{\top}X^{(train)\top}X^{(train)}w-2w^{\top}X^{(train)\top}y^{(train)}+y^{(train)\top}y^{(train)})=0$</center>
<center>$\Rightarrow 2X^{(train)\top}X^{(train)}w-2X^{(train)\top}y^{(train)}=0$</center>
<center>$\Rightarrow w=(X^{(train)\top}X^{(train)})^{-1}X^{(train)\top}y^{(train)}$</center>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># y = wx + b</span></span><br><span class="line">X = np.hstack((np.array([[-<span class="number">0.5</span>,-<span class="number">0.45</span>,-<span class="number">0.35</span>,-<span class="number">0.35</span>,-<span class="number">0.1</span>,<span class="number">0</span>,<span class="number">0.2</span>,<span class="number">0.25</span>,<span class="number">0.3</span>,<span class="number">0.5</span>]]).reshape(-<span class="number">1</span>, <span class="number">1</span>), np.ones((<span class="number">10</span>,<span class="number">1</span>))*<span class="number">1</span>))</span><br><span class="line">y = np.array([-<span class="number">0.2</span>,<span class="number">0.1</span>,-<span class="number">1.25</span>,-<span class="number">1.2</span>,<span class="number">0</span>,<span class="number">0.5</span>,-<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.5</span>,<span class="number">1.2</span>]).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小化MSE,w详见公式</span></span><br><span class="line">w = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weight:&quot;</span>, <span class="built_in">list</span>(w))</span><br><span class="line"></span><br><span class="line">hat_y = X.dot(w)</span><br><span class="line">x = np.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">50</span>)</span><br><span class="line">hat_y = x * w[<span class="number">0</span>] + w[<span class="number">1</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line">plt.xlim(-<span class="number">1.0</span>, <span class="number">1.0</span>)</span><br><span class="line">plt.xticks(np.linspace(-<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">5</span>))</span><br><span class="line">plt.ylim(-<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">plt.plot(x, hat_y, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], y[:,<span class="number">0</span>], color=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;$x_1$&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;$y$&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;$Linear Regression$&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">Weight: [array([<span class="number">1.49333333</span>]), array([<span class="number">0.04966667</span>])]</span><br></pre></td></tr></table></figure>
<p><img src="/images/linearregression.png"></img></p>
<h3 id="容量、过拟合和欠拟合"><a href="#容量、过拟合和欠拟合" class="headerlink" title="容量、过拟合和欠拟合"></a>容量、过拟合和欠拟合</h3><p>&emsp;&emsp;<strong>泛化（Generalization）</strong> ：不只在训练集上表现良好，能够在先前未观测到的新输入上表现良好的能力。<br>&emsp;&emsp;<strong>训练误差（Training Error）</strong> ：量化模型在训练集上的表现。<br>&emsp;&emsp;<strong>测试误差（Test Error）</strong> ：量化模型在测试集上的表现，或称为<strong>泛化误差（Generalization Error）</strong> 。理想的模型是在最小训练误差的同时，最小化泛化误差。</p>
<p>&emsp;&emsp;在实际应用过程中，会采样两个数据集，减小训练误差得到参数后，再在测试集中验证。这个过程中，会发生测试误差的期望大于训练误差的期望的情况。决定机器学习算法是否好的因素：</p>
<ul>
<li>降低训练误差。</li>
<li>缩小训练误差与测试误差之间的差距。</li>
</ul>
<p>&emsp;&emsp;两个因素分别对应机器学习的两大挑战：欠拟合（Underfitting）和过拟合（Overfitting）。<br>&emsp;&emsp;<strong>欠拟合</strong>是指模型在训练集上的误差较大，通常由于<strong>训练不充分</strong>或<strong>模型不合适</strong>导致。<br>&emsp;&emsp;<strong>过拟合</strong>是指模型在训练集和测试集上的误差差距过大，通常由于<strong>模型过分拟合了训练集中的随机噪音，导致泛化能力较差</strong>。可采用<strong>正则化</strong>，降低泛化误差。</p>
<p>&emsp;&emsp;<strong>容量（Capacity）</strong>：是描述了整个模型拟合各种函数的能力，通过调节机器学习模型的容量，可以控制模型是否偏于过拟合还是欠拟合。如果容量不足，模型将不能够很好地表示数据，表现为欠拟合；如果容量过大，模型就很容易过分拟合数据，因为其记住了不适合测试集的训练集特性，表现为过拟合。容量的控制方法有：</p>
<ul>
<li>选择控制模型的假设空间（Hypothesis Space），即学习算法可以选择为解决方案的函数集。</li>
<li>添加正则项对模型进行偏好排除。</li>
</ul>
<p>&emsp;&emsp;当机器学习算法的容量适合于所执行任务的复杂度和所提供训练数据的数量时，算法效果通常最佳。</p>
<p>&emsp;&emsp;通常，当模型容量上升时，训练误差会下降，直到其渐近最小可能误差（假设误差度量有最小值）。通常，泛化误差是一个关于模型容量的U形曲线函数。如图所示，容量和误差之间的关系，左侧训练误差和泛化误差都非常高，为欠拟合机制。当容量增加时，训练误差减小，但训练误差和泛化误差之间的间距扩大，当间距的大小超过了训练误差的下降，则进入到了过拟合机制，其中容量过大，超过了最佳容量。</p>
<p><img src="/images/capacity.png" width="600px"></img></p>
<h4 id="没有免费午餐定理"><a href="#没有免费午餐定理" class="headerlink" title="没有免费午餐定理"></a>没有免费午餐定理</h4><p>&emsp;&emsp;<strong>没有免费午餐定理（No Free Lunch Theorem）</strong> ，通俗来讲，“没有最优的学习算法”。在所有的数据生成分布上平均后，最先进的算法和简单的算法在性能上相差无异。机器学习没有通用的学习算法来处理所有的概率分布问题，而是需要根据什么样的分布，采用什么样的机器学习算法在该数据分布上效果最好。</p>
<h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>&emsp;&emsp;正则化（Regularization）是指修改学习算法，使其降低泛化误差而非训练误差。</p>
<h3 id="超参数和验证集"><a href="#超参数和验证集" class="headerlink" title="超参数和验证集"></a>超参数和验证集</h3><p>&emsp;&emsp;<strong>超参数</strong>:用来控制学习算法的参数而非学习算法本身学出来的参数。例如，进行曲线的回归拟合时，曲线的次数就是一个超参数；在构建模型对一些参数的分布假设也是超参数。</p>
<p>&emsp;&emsp;<strong>验证集</strong>：通常在需要选取超参数时，将训练集再划分为训练和验证集两部分，使用新的训练集训练模型，验证集用来进行测试和调整超参。通常，80%的训练数据用于训练学习参数，20%用于验证。</p>
<h4 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h4><p>&emsp;&emsp;<strong>k折交叉验证</strong>：将数据集均分为不相交的k份，每次选取其中的一份作为测试集，其他的为训练集，训练误差为k次的平均误差。</p>
<hr>
<h2 id="k-折交叉验证算法Define-KFlodXV-mathbb-D-A-L-k-Require-mathbb-D-为给定的数据集，其中元素为-z-i-Require-A-为学习算法，可视为一个函数（使用数据集作为输入，输出一个学好的函数）Require-L-为损失函数，可视为来自学好的函数-f-，将样本-z-i-in-mathbb-D-映射到-mathbb-R-中标量的函数Require-k-为折数-emsp-emsp-将-mathbb-D-分为-k-个互斥子集-mathbb-D-i-，它们的并集为-mathbb-D-emsp-emsp-for-i-from-1-to-k-do-emsp-emsp-emsp-emsp-f-i-A-mathbb-D-setminus-mathbb-D-i-emsp-emsp-emsp-emsp-for-z-j-in-mathbb-D-i-do-emsp-emsp-emsp-emsp-emsp-emsp-e-j-L-f-i-z-j-emsp-emsp-emsp-emsp-end-for-emsp-emsp-end-for-emsp-emsp-Return-e"><a href="#k-折交叉验证算法Define-KFlodXV-mathbb-D-A-L-k-Require-mathbb-D-为给定的数据集，其中元素为-z-i-Require-A-为学习算法，可视为一个函数（使用数据集作为输入，输出一个学好的函数）Require-L-为损失函数，可视为来自学好的函数-f-，将样本-z-i-in-mathbb-D-映射到-mathbb-R-中标量的函数Require-k-为折数-emsp-emsp-将-mathbb-D-分为-k-个互斥子集-mathbb-D-i-，它们的并集为-mathbb-D-emsp-emsp-for-i-from-1-to-k-do-emsp-emsp-emsp-emsp-f-i-A-mathbb-D-setminus-mathbb-D-i-emsp-emsp-emsp-emsp-for-z-j-in-mathbb-D-i-do-emsp-emsp-emsp-emsp-emsp-emsp-e-j-L-f-i-z-j-emsp-emsp-emsp-emsp-end-for-emsp-emsp-end-for-emsp-emsp-Return-e" class="headerlink" title="k-折交叉验证算法Define KFlodXV($\mathbb{D},A,L,k$):Require: $\mathbb{D}$为给定的数据集，其中元素为$z^{(i)}$Require: $A$为学习算法，可视为一个函数（使用数据集作为输入，输出一个学好的函数）Require: $L$为损失函数，可视为来自学好的函数$f$，将样本$z^{(i)}\in \mathbb{D}$映射到$\mathbb{R}$中标量的函数Require: $k$为折数&emsp;&emsp;将$\mathbb{D}$分为$k$个互斥子集$\mathbb{D}_{i}$，它们的并集为$\mathbb{D}$&emsp;&emsp;for $i$ from 1 to $k$ do&emsp;&emsp;&emsp;&emsp;$f_{i}&#x3D;A(\mathbb{D}\setminus \mathbb{D}_{i})$&emsp;&emsp;&emsp;&emsp;for $z^{(j)}$ in $\mathbb{D}_{i}$ do&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$e_{j}&#x3D;L(f_{i},z^{(j)})$&emsp;&emsp;&emsp;&emsp;end for&emsp;&emsp;end for&emsp;&emsp;Return $e$"></a><strong>k-折交叉验证算法</strong><br><strong>Define</strong> KFlodXV($\mathbb{D},A,L,k$):<br><strong>Require:</strong> $\mathbb{D}$为给定的数据集，其中元素为$z^{(i)}$<br><strong>Require:</strong> $A$为学习算法，可视为一个函数（使用数据集作为输入，输出一个学好的函数）<br><strong>Require:</strong> $L$为损失函数，可视为来自学好的函数$f$，将样本$z^{(i)}\in \mathbb{D}$映射到$\mathbb{R}$中标量的函数<br><strong>Require:</strong> $k$为折数<br>&emsp;&emsp;将$\mathbb{D}$分为$k$个互斥子集$\mathbb{D}_{i}$，它们的并集为$\mathbb{D}$<br>&emsp;&emsp;for $i$ from 1 to $k$ do<br>&emsp;&emsp;&emsp;&emsp;$f_{i}&#x3D;A(\mathbb{D}\setminus \mathbb{D}_{i})$<br>&emsp;&emsp;&emsp;&emsp;for $z^{(j)}$ in $\mathbb{D}_{i}$ do<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$e_{j}&#x3D;L(f_{i},z^{(j)})$<br>&emsp;&emsp;&emsp;&emsp;end for<br>&emsp;&emsp;end for<br>&emsp;&emsp;Return $e$</h2><h5 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">KFoldCV</span>(<span class="params">D, A, k</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    k-fold 交叉验证</span></span><br><span class="line"><span class="string">    参数说明：</span></span><br><span class="line"><span class="string">    D：给定数据集</span></span><br><span class="line"><span class="string">    A：学习函数</span></span><br><span class="line"><span class="string">    k：折数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.shuffle(D)</span><br><span class="line">    dataset = np.split(D, k)</span><br><span class="line">    acc_rate = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        train_set = dataset.copy()</span><br><span class="line">        test_set = train_set.pop(i)</span><br><span class="line">        train_set = np.vstack(train_set)</span><br><span class="line">        A.train(train_set[:,:-<span class="number">1</span>], train_set[:,-<span class="number">1</span>]) <span class="comment"># 每次的训练集</span></span><br><span class="line">        labels = A.fit(test_set[:,:-<span class="number">1</span>]) <span class="comment"># 每次的测试集</span></span><br><span class="line">        acc_rate += np.mean(labels==test_set[:,-<span class="number">1</span>]) <span class="comment"># 计算平均误差</span></span><br><span class="line">    <span class="keyword">return</span> acc_rate/k</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="估计、偏差和方差"><a href="#估计、偏差和方差" class="headerlink" title="估计、偏差和方差"></a>估计、偏差和方差</h3><h4 id="点估计"><a href="#点估计" class="headerlink" title="点估计"></a>点估计</h4><p>&emsp;&emsp;点估计试图为一些感兴趣的量提供单个“最优”预测。</p>
<h4 id="偏差"><a href="#偏差" class="headerlink" title="偏差"></a>偏差</h4><p>&emsp;&emsp;估计的偏差被定义为</p>
<center>$bias(\hat{\theta}_{m})=\mathbb{E}(\hat{\theta}_{m})-\theta$</center>
&emsp;&emsp;期望作用在所有数据上，$\theta$用于定于数据生成分布的$\theta$的真实值。

<h4 id="方差和标准差"><a href="#方差和标准差" class="headerlink" title="方差和标准差"></a>方差和标准差</h4><p>&emsp;&emsp;估计的方差被定义为</p>
<center>$Var(\hat{\theta})$</center>
&emsp;&emsp;方差反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。

<p>&emsp;&emsp;标准差被记为</p>
<center>$SE(\hat{\mu _{m}})=\sqrt{Var\left [ \frac{1}{m}\sum_{i=1}^{m}x^{(i)} \right ]}=\frac{\sigma }{\sqrt{m}}$</center>
&emsp;&emsp;其中，$\sigma^{2}$是样本$x^{(i)}$的真实方差，标准差通常被记为$\sigma$。

<h4 id="误差与偏差和方差的关系"><a href="#误差与偏差和方差的关系" class="headerlink" title="误差与偏差和方差的关系"></a>误差与偏差和方差的关系</h4><p>&emsp;&emsp;泛化误差可分解为偏差、方差和噪音之和。需要在模型复杂度之间权衡，使偏差和方差得以均衡，这样模型的整体误差才会最小。<br><img src="/images/capacity3.png" width="600px"></img><br>&emsp;&emsp;当容量增大时，偏差随之减小；而方差随之增大，泛化误差为U型。</p>
<h3 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h3><p>&emsp;&emsp;<strong>最大似然估计（Maximum Likelihood Estimation，MLE）</strong>是一种最为常见的估计准则，其思想是在已知分布产生的一些样本而未知分布具体参数的情况下，根据样本值推断最有可能产生样本的参数值。将数据的真实分布记为$P_{data(x)}$，为了使⽤$MLE$，需要先假设样本服从某⼀簇有参数确定的分布$P_{model(x;\theta)}$，现在的⽬标就是使⽤估计的$P_{model}$来拟合真实的$P_{data}$(条件一:”模型已定，参数未知”)。<br>&emsp;&emsp;对于⼀组由$m$个样本组成的数据集$X&#x3D;{x^{(1)},…,x^{(m)}}$，假设数据独⽴且由未知的真实数据分布$P_{data(x)}$⽣成 (条件二：独立同分布采样的数据)，可以通过最⼤似然估计，获取真实分布的参数。</p>
<center>$\theta _{ML}=\underset{\theta}{arg\ max}P_{model}(X;\theta)=\underset{\theta}{arg\ max}\coprod_{i=1}^{m}P_{model}(x^{(i)};\theta)$</center>
&emsp;&emsp;通常为了计算⽅便，会对$MLE$加上$log$，将乘积转化为求和然后将求和变为期望：$\theta _{ML}=\underset{\theta}{arg\ max}\sum_{i=1}^{m}logP_{model}(x^{(i)};\theta)$ 。

<p>&emsp;&emsp;使⽤训练数据经验分布$\hat{P}_{data}$相关的期望进⾏计算：$\theta _{ML}&#x3D;\underset{\theta}{arg\ max}\mathbb{E}_{x\sim \hat{P}_{data}}logP_{model}(x;\theta)$。该式是许多监督学习算法的基础假设。</p>
<p>&emsp;&emsp;最⼤似然估计的⼀种解释是使$P_{model}$与$P_{data}$之间的差异性尽可能的⼩，形式化的描述为最⼩化两者的$KL$散度。</p>
<p>&emsp;&emsp;<font color="#ff0000">定义看了半天，看了个寂寞，直接举例推导：</font><br><img src="/images/mle_example.png" width="500px"></img><br>&emsp;&emsp;一枚硬币抛10次，得到$X$数据为{反，正，正，正，正，反，正，正，正，反}。得到似然函数$f(x_{0};\theta)&#x3D;(1-\theta)\times\theta\times\theta\times\theta\times\theta\times(1-\theta)\times\theta\times\theta\times\theta\times(1-\theta)&#x3D;(1-\theta)^{3}\times \theta ^{7}$<br>&emsp;&emsp;博客<font color="#ff0000"><sup>[<a target="_blank" rel="noopener" href="https://blog.csdn.net/u011508640/article/details/72815981">2</a>][<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46737512">3</a>]</sup></font>中已经推导解释的非常好，负责将代码实现。</p>
<h4 id="手动推算"><a href="#手动推算" class="headerlink" title="手动推算"></a>手动推算</h4><p><img src="/images/mle_2.png" width="500px"></img></p>
<h4 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">theta</span>): <span class="comment"># f(theta)</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span>-theta)**<span class="number">3</span> * theta**<span class="number">7</span></span><br><span class="line"></span><br><span class="line">X = np.arange(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.001</span>)</span><br><span class="line">Y = f(X)</span><br><span class="line">Y = np.array(Y)</span><br><span class="line">plt.plot(X,Y)</span><br><span class="line">plt.title(<span class="string">&quot;$f(theta) = (1-theta)^3theta^7$&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/mle1.png" width="500px"></img></p>
<p>&emsp;&emsp;可以看出，$\theta&#x3D;0.7$时，似然函数取得最大值。</p>
<p>&emsp;&emsp;<strong>通俗来讲，是利用已知的样本结果信息，反推最大概率导致这些样本结果出现的模型参考值。</strong>极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。</p>
<h4 id="最大似然的性质"><a href="#最大似然的性质" class="headerlink" title="最大似然的性质"></a>最大似然的性质</h4><ul>
<li>真实分布$p_{data}$必须在模型族$p_{model}(.;\theta)$中。否则，没有估计可以还原$p_{data}$。</li>
<li>真实分布$p_{data}$必须刚好对应一个$\theta$值。否则，最大似然估计恢复出真实分布$p_{data}$后，也不能决定数据生成过程使用哪个$\theta$。</li>
</ul>
<h3 id="贝叶斯统计"><a href="#贝叶斯统计" class="headerlink" title="贝叶斯统计"></a>贝叶斯统计</h3><p>&emsp;&emsp;通过贝叶斯准则来估计参数的后验分布情况，贝叶斯统计（Bayesian Statistics）认为训练数据是确定的，而参数是随机且不唯一的，每个参数都有相应的概率。在观察数据之前，将$\theta$的已知知识表示成先验概率分布$p(\theta)$。如有一组数据样本$\{x^{(1)},…,x^{(m)}\}$，通过贝叶斯规则结合数据似然$p(x^{(1)},…,x^{(m)}\mid \theta)$和先验，得到：</p>
<center>$p(\theta \mid x^{(1)},...,x^{(m)})=\frac{p(x^{(1)},...,x^{(m)}\mid \theta )p(\theta)}{p(x^{(1)},...,x^{(m)})}$</center>

<p>&emsp;&emsp;相对于最大似然估计，贝叶斯估计有两个重要区别：第一，不像最大似然方法预测时使用$\theta$的点估计，贝叶斯方法使用$\theta$的全分布。第二，贝叶斯为先验分布，先验通常表现为偏好更简单或更光滑的模型，当训练数据有限时，贝叶斯方法通常泛化得更好，当训练样本数目很大时，通常计算代价很大。</p>
<h4 id="手动推算-1"><a href="#手动推算-1" class="headerlink" title="手动推算"></a>手动推算</h4><h4 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h4><h4 id="最大后验估计"><a href="#最大后验估计" class="headerlink" title="最大后验估计"></a>最大后验估计</h4><p>&emsp;&emsp;完整的贝叶斯估计需要使用参数的完整分布进行预测，但计算繁重。最大后验估计（Maximum A Posterior，MAP）来选取一个计算可行的单点估计参数作为贝叶斯估计的近似解，公式：</p>
<center>$\theta _{MAP}=\underset{\theta}{arg \ max}\ p(\theta \mid x)=\underset{\theta}{arg \ max}\ log \ p(x\mid \theta)+log\ p(\theta)$</center>

<p>&emsp;&emsp;MAP的估计实际上就是对数似然加上参数的先验分布。实际上，在参数服从⾼斯分布的情况下，上式的右边就对应着L2正则项；在Laplace的情况下，对应着L1的正则项；在均匀分布的情况下则为0，等价于MLE。（太绕了）</p>
<p>&emsp;&emsp;最大似然估计（MLE）是求$\theta$使得似然函数$P(x_{0}\mid \theta)$最大。最大后验概率估计（MAP）是求$\theta$使得函数$P(x_{0}\mid \theta)P(\theta)$最大。$\theta$自己出现的先验概率也最大。</p>
<h4 id="代码实现-3"><a href="#代码实现-3" class="headerlink" title="代码实现"></a>代码实现</h4><h3 id="监督学习算法"><a href="#监督学习算法" class="headerlink" title="监督学习算法"></a>监督学习算法</h3><p>&emsp;&emsp;监督学习算法是给定一组输入$x$和输出$y$的训练集，学习如何关联输入和输出。在<a href="https://deeplearner.top/2021/11/29/AI%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E5%8F%8A%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/">人工智能是什么</a>一文中已经对监督学习和无监督学习进行了整理和划分。</p>
<p><img src="/images/jiandusuanfa.png" width="500px"></img></p>
<p>&emsp;&emsp;机器学习算法后续会专门整理、归纳、总结。</p>
<h4 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h4><h4 id="其他简单监督学习算法"><a href="#其他简单监督学习算法" class="headerlink" title="其他简单监督学习算法"></a>其他简单监督学习算法</h4><h3 id="无监督学习算法"><a href="#无监督学习算法" class="headerlink" title="无监督学习算法"></a>无监督学习算法</h3><h4 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h4><p>详见：<a href="https://deeplearner.top/2021/12/09/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90">DeepLearning学习笔记-2.线性代数</a> </p>
<h4 id="k-均值聚类"><a href="#k-均值聚类" class="headerlink" title="k-均值聚类"></a>k-均值聚类</h4><h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>详见：<a href="https://deeplearner.top/2022/01/04/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97/#%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95">DeepLearning学习笔记-4.数值计算</a> </p>
<h3 id="构建机器学习算法"><a href="#构建机器学习算法" class="headerlink" title="构建机器学习算法"></a>构建机器学习算法</h3><p>&emsp;&emsp;几乎所有的深度学习算法都是一样的流程：特定的数据集、代价函数、优化过程和模型。</p>
<h3 id="深度学习发展的挑战"><a href="#深度学习发展的挑战" class="headerlink" title="深度学习发展的挑战"></a>深度学习发展的挑战</h3><p>&emsp;&emsp;高维数据在新样本上泛化苦难，传统机器学习中实现泛化机制不适合学习高维空间中的复杂函数，涉及到巨大空间问题，计算代价很大，深度学习则旨在客服这些以及其他一些难题。</p>
<h5 id="维数灾难"><a href="#维数灾难" class="headerlink" title="维数灾难"></a>维数灾难</h5><p>&emsp;&emsp;当数据的维数很高时，很多机器学习问题变得相当困难，这种现象被称为维数灾难（Curse of Dimensionality）。维数灾难带来的一个挑战是统计挑战。</p>
<h4 id="局部不变形和平滑正则化"><a href="#局部不变形和平滑正则化" class="headerlink" title="局部不变形和平滑正则化"></a>局部不变形和平滑正则化</h4><p>&emsp;&emsp;为更好地泛化，机器学习算法需要由先验信念引导该学习什么类型的函数。先验信念还间接地体现在选择一些偏好某类函数的算法。其中使用最广泛的隐式先验是“平滑先验”和“局部不变性先验”，表明学习的函数不应在小区域内发生很大的变化。</p>
<h4 id="流行学习"><a href="#流行学习" class="headerlink" title="流行学习"></a>流行学习</h4><p>&emsp;&emsp;<strong>流行（Manifold）</strong>是指连接在一起的区域。<strong>流行学习（Manifold Learning）</strong>算法通过一个假设来克服很多机器学习问题无望的障碍。该假设认为$R^{n}$中大部分区域都是无效的输入，有意义的输入只分布在包含少量数据点的子集构成的一组流行中，而学习函数的输出中，有意义的变化都沿着流行的方向或仅发生在我们切换到另一流行时。（难懂，先看应用场景，后续再研究）</p>
<p>&emsp;&emsp;主要用于图像降维<sup>[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/40214106">7</a>]</sup>。</p>
</div><script type="text/javascript" src="../../../../js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://deeplearner.top/2022/01/10/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-5-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" data-id="clkor69p2003xc6s61xaddorm" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAEmCAAAAADqr2IGAAAElUlEQVR42u3ay2rjQBAFUP//T2dgVoFJlHurNOBWjlbB2K3WUaC6Hq9XfH38va4/+fz55+vf719/J18hv+/1b2+7MGHChAnTWzJ9XF6zTVx/kr+M/IXlHMmL+eK3mDBhwoTpcKZ8ueSIkLPmLywP5Ne/aikxYcKECdPvYcr5Zknv/rXld8eECRMmTJhaphaoDfCzT/IdYsKECROmZzPNlssPAZtC7ezA0a5/Wy0cEyZMmDC9GVMbkp/093+cb8KECRMmTG/A9FFe7eGgHdnJw/n+sFI8NSZMmDBhOpYpbwQmReH2wdp12m/OzkdFhQATJkyYMB3FNBttuQ7eedO0XW1WaL7e1Q9PjQkTJkyYDmTatCH3wzqzImyywyT9ztfBhAkTJkznMt2VTCaLtqlvUX6Njx3tzjFhwoQJ0zOYknCYF0zbwL9PWdujQ3tAwYQJEyZMz2BqE9GkVTkr4G52kh8v8vQeEyZMmDCdzrRJX/MScNI0nQ22bpqydZEXEyZMmDAdyJSH/Fn7sy3RzkL7bBSp4MOECRMmTAcyzUJ4e5t9qE6KvG0KXRSXMWHChAnTsUx5uTbZdMuRp7htorsZ0Ilq4ZgwYcKE6SimPEXMm4uz4Z78xezLyu36mDBhwoTpdKY8RdwXW/fB+97xoPqQgQkTJkyYjmLKC7htazMPtG2SvP87T7kxYcKECdO5TEnqmLceNyM4s1R51uCsXycmTJgwYXoc06yt2EbP9l53pdbtUQkTJkyYMJ3OlJdTN1vMR4I2I61tIh2lx5gwYcKE6XCm9mb5bWapbD6O06a+eSF4dSrBhAkTJkxvw3RvMrmhbIdv2qS9vVfx/4UJEyZMmN6YadOq3CTAs1LsLImdrf9t6osJEyZMmI5lSh4pH1rNw/OsFbpveRbFZUyYMGHCdCxTXh6dJaX7NXOaPNfPDxav2S0xYcKECdPbMOWP17YA24ffHwWGVe0kOceECRMmTMcyzdqBs7C6T57zluqmmfpD6osJEyZMmI5lmo3IbEJyDpE//OZAgwkTJkyYnseUp7v5d9okOXmkzW/zV/jtCpgwYcKE6XCmTRm3/WbbEM0Ha/btz6g2gAkTJkyYjmVqC7vt1vNgvB/W2Yz1fJv6YsKECROmRzAlm84hZmM99w4PXdNHLxgTJkyYMB3LlI/F5Ntqh3uSVzJLxdv7vpILEyZMmDA9gik5IswKuEnxty0Et+F/czTBhAkTJkxnMd31wG3QzR87b15u0vWoQoAJEyZMmA5nmo3abLa7Ced547N9FkyYMGHC9CSmNh1tHyMvIs8S1LzoPEuDX+1bwoQJEyZMb8Y0C+o59KaU3A7uzHYY/StgwoQJE6ZjmTYheVgwjclm40R5kTd/bZgwYcKE6XSmWQMyD+p52jm7b3tAmbU8MWHChAnT6UyzImmSKM6CfTswtKpt5y1MTJgwYcL0aKZ2Q23BtB36me0wSZi/WAcTJkyYMP0yps1xIW9btihtSlwcUzBhwoQJ0+FMeZG3/XwGsUl9818V+8GECRMmTMcybcLnXaXhffLctlprAkyYMGHCdB7THzgSbF0TjcnVAAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="../../../../tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><i class="fa fa-tag"></i>深度学习笔记</a><a href="../../../../tags/%E8%8A%B1%E4%B9%A6/"><i class="fa fa-tag"></i>花书</a></div><div class="post-nav"><a class="pre" href="../../../02/10/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-6-%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/">DeepLearning-6.深度前馈网络(一)</a><a class="next" href="../../04/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97/">DeepLearning-4.数值计算</a></div><div class="nofancybox" id="waline"></div><script src="//unpkg.com/@waline/client@v2/dist/waline.js"></script><link rel="stylesheet" type="text/css" href="//unpkg.com/@waline/client@v2/dist/waline.css"><script>let metaInfo = ['nick', 'mail', 'link']
let requiredMeta = 'nick'.split(',').filter(item => {
  return metaInfo.indexOf(item) > -1
})
Waline.init({
  el: '#waline',
  comment: true,
  serverURL: 'https://waline-deeplearner.vercel.app',
  pageSize: '10',
  wordLimit: '500',
  requiredMeta,
})
</script></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/images/blog/avatar_new.png"/></a><p>心外无物，知行合一</p><a class="info-icon" href="/resume/" title="个人简历" target="_blank" style="margin-inline:5px"> <i class="fa fa-share-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:conghaoyuan@gmail.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/conghaoyuan" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/AIGC/">AIGC</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Algorithm/">Algorithm</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/CV/">CV</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/GUI/">GUI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/NLP/">NLP</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Python/">Python</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%97%E4%BA%BA/">数字人</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%A6/">数学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/">深度学习模型</a><span class="category-list-count">11</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/10/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8B%E9%80%9A%E4%B9%89Prompt%E5%9B%9B%E5%8D%81%E5%BC%8F/">AIGC-LLM-辟邪剑谱之通义Prompt四十式</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/06/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8BPrompt%20Engineering/">AIGC-LLM-辟邪剑谱之Prompt Engineering</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/11/18/AIGC-Agent-%E4%BD%9C%E6%96%87%E6%89%B9%E6%94%B9%E6%A0%87%E6%B3%A8/">AIGC-Agent-作文批改标注</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/09/27/LLama32%E8%B0%83%E7%A0%94/">LLama3.2调研报告</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/04/09/AIGC-LLM-Prompt%E6%8A%80%E8%83%BD/">AIGC-LLM-Prompt工程技能综述</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/03/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-BERT/">Deeeplearning模型-NLP-BERT</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/01/16/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E8%AF%AD%E6%96%99%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/">Deeeplearning模型-NLP-大语言模型微调语料生成工具</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-Transformer%E6%A8%A1%E5%9E%8B/">Deeeplearning模型-NLP-Transformer模型</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/01/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-NLP%E5%9F%BA%E7%A1%80/">Deeeplearning模型-NLP-NLP基础</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/10/26/%E6%95%B0%E5%AD%97%E4%BA%BA-%E6%95%B0%E5%AD%97%E4%BA%BA%E6%8A%80%E6%9C%AF%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94/">数字人-数字人技术效果对比</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://conghaoyuan.github.io" title="Fern[个人早期博客]" target="_blank">Fern[个人早期博客]</a></div><div class="widget"><div class="widget-title"><i class="fa fa-star"> 天文学</i></div><ul></ul><a href="/tools/tianwenli.html" title="天文历" target="_self">天文历</a></div><div class="widget"><div class="widget-title"><i class="fa fa-plus"> 数学小工具</i></div><ul></ul><a href="/tools/buy_house.html" title="丐版买房计算器" target="_self">丐版买房计算器</a><ul></ul><a href="/tools/equation_12.html" title="一元二次方程" target="_self">一元二次方程</a><ul></ul><a href="/tools/equation_line.html" title="直线方程" target="_self">直线方程</a><ul></ul><a href="/tools/equation_polynomial.html" title="多项式方程" target="_self">多项式方程</a><ul></ul><a href="/tools/regression_ridge.html" title="岭回归" target="_self">岭回归</a><ul></ul><a href="/tools/fourier_transform.html" title="傅里叶变换" target="_self">傅里叶变换</a><ul></ul><a href="/tools/gradient_descent.html" title="梯度下降" target="_self">梯度下降</a><ul></ul><a href="/tools/function_e.html" title="指数函数" target="_self">指数函数</a><ul></ul><a href="/tools/function_inverse_scale.html" title="反比例函数" target="_self">反比例函数</a><ul></ul><a href="/tools/function_log.html" title="对数函数" target="_self">对数函数</a><ul></ul><a href="/tools/function_sin.html" title="正弦函数" target="_self">正弦函数</a><ul></ul><a href="/tools/coord_grid.html" title="网格化坐标" target="_self">网格化坐标</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="../../../../." rel="nofollow">DeepLearner.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="../../../../js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="../../../../js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="../../../../css/search.css?v=1.0.0"><script type="text/javascript" src="../../../../js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="/static/js/MathJax.js?config=TeX-MML-AM_CHTML" async></script><div id="script" type="text/javascript" src="../../../../js/mathjaxs.js" async></div><script type="text/javascript" src="../../../../js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="../../../../js/smartresize.js?v=1.0.0"></script></div><!-- hexo injector body_end start -->
<script src="assets/prism-bundle.js"></script>
<script src="assets/prism-plus.js" data-pjax=""></script>
<!-- hexo injector body_end end --></body></html>