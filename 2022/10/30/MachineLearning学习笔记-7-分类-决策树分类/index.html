<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Just Do It"><title>MachineLearning-7.（分类）决策树分类 | DeepLearner</title><link rel="stylesheet" type="text/css" href="../../../../css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/static/css/normalize.css"><link rel="stylesheet" type="text/css" href="/static/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/static/css/grids-responsive-min.css"><link rel="stylesheet" href="/static/css/font-awesome.min.css"><script type="text/javascript" src="/static/js/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="../../../../favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="../../../../favicon.ico"><link rel="apple-touch-icon" href="../../../../apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="../../../../apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="../../../../atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-83251621-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + 'b207d9febf3295d375e6e9b42006f9eb';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/static/js/clipboard.min.js"></script><script type="text/javascript" src="/static/js/toastr.min.js"></script><link rel="stylesheet" href="/static/css/toastr.min.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">MachineLearning-7.（分类）决策树分类</h1><a id="logo" href="../../../../.">DeepLearner</a><p class="description">Deep Learning Notes</p></div><div id="nav-menu"><a class="current" href="../../../../."><i class="fa fa-home"> 首页</i></a><a href="../../../../archives/"><i class="fa fa-archive"> 归档</i></a><a href="../../../../tagcloud/"><i class="fa fa-tags"> 标签</i></a><a href="../../../../resource/"><i class="fa fa-cloud"> 资源</i></a><a href="../../../../guestbook/"><i class="fa fa-wechat"> 留言</i></a><a href="../../../../codeline/"><i class="fa fa-circle"> 码迹</i></a><a href="../../../../resume/"><i class="fa fa-user"> 个人简历</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">MachineLearning-7.（分类）决策树分类</h1><div class="post-meta">created:2022-10-30</div><div class="post-meta">updated:2023-07-30<span> | </span><span class="category"><a href="../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><a class="disqus-comment-count" href="#waline"><span class="waline-comment-count" id=""></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">1.</span> <span class="toc-text">分类决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9A%E4%B9%89"><span class="toc-number">1.1.</span> <span class="toc-text">分类决策树定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%BB%93%E6%9E%84"><span class="toc-number">1.2.</span> <span class="toc-text">决策树结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E6%9E%84%E5%BB%BA"><span class="toc-number">1.3.</span> <span class="toc-text">决策树构建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.1.</span> <span class="toc-text">决策树算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-number">1.3.2.</span> <span class="toc-text">决策树特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A0%E4%BF%A1%E6%81%AF%E7%86%B5"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">①信息熵</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A1%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">②信息增益</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A2%E5%A2%9E%E7%9B%8A%E7%8E%87"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">③增益率</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%91%A3%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0"><span class="toc-number">1.3.2.4.</span> <span class="toc-text">④基尼系数</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%81%9C%E6%AD%A2%E5%88%86%E8%A3%82"><span class="toc-number">1.3.3.</span> <span class="toc-text">决策树停止分裂</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.4.</span> <span class="toc-text">决策树实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B"><span class="toc-number">1.4.1.</span> <span class="toc-text">波士顿房价预测</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%89%AA%E6%9E%9D"><span class="toc-number">1.5.</span> <span class="toc-text">决策树的剪枝</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">2.</span> <span class="toc-text">集成学习与随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="toc-number">2.1.</span> <span class="toc-text">集成学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Boosting"><span class="toc-number">2.2.</span> <span class="toc-text">Boosting</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Boosting%E5%AE%9A%E4%B9%89"><span class="toc-number">2.2.1.</span> <span class="toc-text">Boosting定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Boosting%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.2.2.</span> <span class="toc-text">Boosting实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">2.3.</span> <span class="toc-text">随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%AE%9A%E4%B9%89"><span class="toc-number">2.3.1.</span> <span class="toc-text">随机森林定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.3.2.</span> <span class="toc-text">随机森林实现</span></a></li></ol></li></ol></li></ol></div></div><div class="post-content"><p>决策树（Decision Tree）是一种基本的分类与回归方法。在分类问题中，表示基于特征对实例进行分类的过程，可认为是if-then规则的集合，是定义在特征空间与类空间上的条件概率分布。利用训练数据，根据损失函数最小化的原则建立决策树模型，预测时，对新的数据利用决策树模型进行分类，包括三个步骤：特征选择、决策树生成、决策树修剪。（李航《统计学习方法：决策树》）<font color="#ff0000"><sup>[<a target="_blank" rel="noopener" href="https://github.com/yuanconghao/deeplearning-datasets/blob/main/book/%E6%9D%8E%E8%88%AA-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B.pdf">1</a>]</sup></font></p>
<span id="more"></span>

<h2 id="分类决策树"><a href="#分类决策树" class="headerlink" title="分类决策树"></a>分类决策树</h2><h3 id="分类决策树定义"><a href="#分类决策树定义" class="headerlink" title="分类决策树定义"></a>分类决策树定义</h3><p>分类决策树模型是一种描述对实例进行分类的树形结构，决策树由节点（node）和有向边（directed edge）组成，结点有两种类型：内部结点（internal node）和叶结点（leaf node），内部结点表示一个特征或属性，叶结点表示一个类。<font color="#ff0000"><sup>[<a target="_blank" rel="noopener" href="https://github.com/yuanconghao/deeplearning-datasets/blob/main/book/%E6%9D%8E%E8%88%AA-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B.pdf">1</a>]</sup></font></p>
<p>如下图挑选西瓜的决策树模型（判断一个西瓜好坏）：<br><img src="/images/ml/classification/decision_tree4.png" alt="" width="300px" style="border:1px solid black;" /></p>
<h3 id="决策树结构"><a href="#决策树结构" class="headerlink" title="决策树结构"></a>决策树结构</h3><p>一棵决策树包含一个根节点、若干个内部节点和若干个叶子节点；叶子节点对应最终的决策结果，其它每个节点则对应与一个属性的测试。最终划分到同一个叶子节点上的样本，具有相同的决策属性，可以对这些样本的值求平均值来实现回归，对这些样本进行投票（选取样本数量最多的类别）实现分类。</p>
<img src="/images/ml/classification/decision_tree_structure.png" alt="" width="500px" style="border:1px solid black;" />

<h3 id="决策树构建"><a href="#决策树构建" class="headerlink" title="决策树构建"></a>决策树构建</h3><h4 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h4><p>决策树的构建，就是不断选取好的特征作为决策节点，构建一颗泛化能力较强的树结构，算法描述如下：</p>
<img src="/images/ml/classification/decision_tree5.png" alt="" width="600px"/>

<p><strong>决策树的构建是一个递归的过程</strong>，核心问题：</p>
<ol>
<li><font color="#ff0000">选取特征作为分裂点。决策树构建的每一步，选取最优的特征，使决策对数据集划分效果最好；</font></li>
<li><font color="#ff0000">何时停止分裂子节点；</font></li>
</ol>
<h4 id="决策树特征选择"><a href="#决策树特征选择" class="headerlink" title="决策树特征选择"></a>决策树特征选择</h4><h5 id="①信息熵"><a href="#①信息熵" class="headerlink" title="①信息熵"></a>①信息熵</h5><p>信息熵（information entropy）是度量样本集合纯度的常用指标，该值越大，表示该集合纯度越低（或越混乱），该值越小，表示该集合纯度越高（或越有序）. 信息熵定义：<br>$$<br>H &#x3D; -\sum_{i&#x3D;1}^{n}{P(x_i)log_2P(x_i)}<br>$$</p>
<p>其中，$P(x_i)$表示集合中第i类样本所占比例，当$P(x_i)$为1时（只有一个类别，比例为100%）, $log_2P(x_i)$的值为0，整个系统信息熵为0；当类别越多，则$P(x_i)$的值越接近于0，$log_2P(x_i)$趋近去负无穷大，整个系统信息熵就越大。以下代码，展示了类别数量从1…10的集合信息熵变化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 信息熵计算演示</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> mp</span><br><span class="line"></span><br><span class="line">class_num = <span class="number">10</span>  <span class="comment"># 类别最大数量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">entropy_calc</span>(<span class="params">n</span>):</span><br><span class="line">    p = <span class="number">1.0</span> / n  <span class="comment"># 计算每个类别的概率</span></span><br><span class="line">    entropy_value = <span class="number">0.0</span>  <span class="comment"># 信息熵</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        p_i = p * math.log(p)</span><br><span class="line">        entropy_value += p_i</span><br><span class="line">    <span class="keyword">return</span> -entropy_value  <span class="comment"># 返回熵值</span></span><br><span class="line"></span><br><span class="line">entropies = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, class_num + <span class="number">1</span>):</span><br><span class="line">    entropy = entropy_calc(i)  <span class="comment"># 计算类别为i的熵值</span></span><br><span class="line">    entropies.append(entropy)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(entropies)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化回归曲线</span></span><br><span class="line">mp.figure(<span class="string">&#x27;Entropy&#x27;</span>, facecolor=<span class="string">&#x27;lightgray&#x27;</span>)</span><br><span class="line">mp.title(<span class="string">&#x27;Entropy&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">mp.xlabel(<span class="string">&#x27;Class Num&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.ylabel(<span class="string">&#x27;Entropy&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.tick_params(labelsize=<span class="number">10</span>)</span><br><span class="line">mp.grid(linestyle=<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">mp.plot(x, entropies, c=<span class="string">&#x27;orangered&#x27;</span>, label=<span class="string">&#x27;entropy&#x27;</span>)</span><br><span class="line"></span><br><span class="line">mp.legend()</span><br><span class="line">mp.show()</span><br></pre></td></tr></table></figure>
<p>执行结果：<br><img src="/images/ml/classification/decision_tree6.png" alt="信息熵变化" width="400px"/></p>
<p>随着决策树的节点划分，纯度是在不断的提升，熵值不断的减小</p>
<h5 id="②信息增益"><a href="#②信息增益" class="headerlink" title="②信息增益"></a>②信息增益</h5><p>决策树根据属性进行判断，将具有相同属性的样本划分到相同节点下，此时，样本比划分之前更加有序（混乱程度降低），信息熵的值有所降低。用划分前的信息熵减去划分后的信息熵，就是决策树获得的信息增益。可以用以下表达式表示：<br>$$<br>Gain(D, a) &#x3D; Ent(D) - \sum_{v&#x3D;1}^{V} \frac{|D^v|}{|D|} Ent(D^v)<br>$$<br>其中，D表示样本集合，a表示属性，v表示属性可能的取值${v^1, v^2,…,v^n}$, $\frac{|D^v|}{|D|}$表示权重，样本越多的分支对分类结果影响更大，赋予更高的权重,  $Gain(D, a)$表示在样本集合D上使用属性a来划分子节点所获得的信息增益。</p>
<h5 id="③增益率"><a href="#③增益率" class="headerlink" title="③增益率"></a>③增益率</h5><p>增益率不直接采用信息增益，而采用信息增益与熵值的比率来作为衡量特征优劣的标准。C4.5算法就是使用增益率作为标准来划分属性。增益率定义为：<br>$$<br>Gain_ratio(D, a) &#x3D; \frac{Gain(D, a)}{IV(a)}<br>$$<br>其中<br>$$<br>IV(a) &#x3D; - \sum_{v&#x3D;1}^{V} \frac{|D^v|}{|D|} log_2 \frac{|D^v|}{|D|}<br>$$</p>
<h5 id="④基尼系数"><a href="#④基尼系数" class="headerlink" title="④基尼系数"></a>④基尼系数</h5><p>信息熵越大，数据越混乱，信息熵越小，数据越纯。</p>
<p>Gini系数越大，数据越混乱，Gini系数越小，数据越纯。</p>
<p>基尼系数定义为：<br>$$<br>Gini(p) &#x3D; \sum_{k&#x3D;1}^{k} p_k (1-p_k) &#x3D; 1 - \sum_{k&#x3D;1}^{k} p_k^2<br>$$</p>
<p>基尼系数反映了从数据集D中随机抽取两个样本，类别标记不一致的概率。因此，基尼系数越小，数据集的纯度越高。CART决策树（Classification And Regression Tree）使用基尼系数来选择划分属性，选择属性时，选择划分后基尼值最小的属性作为最优属性。采用和上式相同的符号表示，数据集D下属性a的基尼系数定义为：<br>$$<br>Gini_index(D, a) &#x3D;  \sum_{v&#x3D;1}^{V} \frac{|D^v|}{|D|} Gini(D^v)<br>$$</p>
<table>
<thead>
<tr>
<th>决策树算法</th>
<th>属性划分系数</th>
</tr>
</thead>
<tbody><tr>
<td>Cart</td>
<td>回归：均方误差mse</td>
</tr>
<tr>
<td>Cart</td>
<td>分类：Gini系数</td>
</tr>
<tr>
<td>ID3</td>
<td>信息增益</td>
</tr>
<tr>
<td>C4.5</td>
<td>增益率</td>
</tr>
</tbody></table>
<h4 id="决策树停止分裂"><a href="#决策树停止分裂" class="headerlink" title="决策树停止分裂"></a>决策树停止分裂</h4><p>以下几种情况会停止决策树子节点的构建：</p>
<ul>
<li>当前节点所有样本属于同一个类别，无需划分；</li>
<li>当前属性集为空，或者所有样本取值相同，无法划分；</li>
<li>当前节点包含的样本集合为空，不能划分；</li>
<li>当前节点样本数量少于指定数量；</li>
</ul>
<h3 id="决策树实现"><a href="#决策树实现" class="headerlink" title="决策树实现"></a>决策树实现</h3><p>scikit-learn中决策树相关API：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型</span></span><br><span class="line">model = st.DecisionTreeRegressor(max_depth=<span class="number">4</span>)  <span class="comment"># 决策树回归器</span></span><br><span class="line">model = st.DecisionTreeClassifier(max_depth=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">model.fit(train_x, train_y)</span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">pre_test_y = model.predict(test_x)</span><br></pre></td></tr></table></figure>

<h4 id="波士顿房价预测"><a href="#波士顿房价预测" class="headerlink" title="波士顿房价预测"></a>波士顿房价预测</h4><p>数据集介绍：该数据集为一个开放房价数据集，包含506个样本，每个样本包含13个特征和1个标签，具体如下所示：<br><img src="/images/ml/classification/boston_housing_features.png" alt="" width="600px"/></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 决策树回归示例</span></span><br><span class="line"><span class="comment"># 使用决策树预测波士顿房价</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sklearn.datasets <span class="keyword">as</span> sd</span><br><span class="line"><span class="keyword">import</span> sklearn.utils <span class="keyword">as</span> su</span><br><span class="line"><span class="keyword">import</span> sklearn.tree <span class="keyword">as</span> st</span><br><span class="line"><span class="keyword">import</span> sklearn.ensemble <span class="keyword">as</span> se</span><br><span class="line"><span class="keyword">import</span> sklearn.metrics <span class="keyword">as</span> sm</span><br><span class="line"></span><br><span class="line">boston = sd.load_boston()  <span class="comment"># 加载boston地区房价数据</span></span><br><span class="line"><span class="built_in">print</span>(boston.feature_names)</span><br><span class="line"><span class="built_in">print</span>(boston.data.shape)</span><br><span class="line"><span class="built_in">print</span>(boston.target.shape)</span><br><span class="line"></span><br><span class="line">random_seed = <span class="number">7</span>  <span class="comment"># 随机种子，计算随机值，相同的随机种子得到的随机值一样</span></span><br><span class="line">x, y = su.shuffle(boston.data, boston.target, random_state = random_seed)</span><br><span class="line"><span class="comment"># 计算训练数据的数量</span></span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="built_in">len</span>(x) * <span class="number">0.8</span>) <span class="comment"># 以boston.data中80%的数据作为训练数据</span></span><br><span class="line"><span class="comment"># 构建训练数据、测试数据</span></span><br><span class="line">train_x = x[:train_size]  <span class="comment"># 训练输入, x前面80%的数据</span></span><br><span class="line">test_x = x[train_size:]   <span class="comment"># 测试输入, x后面20%的数据</span></span><br><span class="line">train_y = y[:train_size]  <span class="comment"># 训练输出</span></span><br><span class="line">test_y = y[train_size:]   <span class="comment"># 测试输出</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######## 单棵树进行预测 ########</span></span><br><span class="line"><span class="comment"># 模型</span></span><br><span class="line">model = st.DecisionTreeRegressor(max_depth=<span class="number">4</span>)  <span class="comment"># 决策回归器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">model.fit(train_x, train_y)</span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">pre_test_y = model.predict(test_x)</span><br><span class="line"><span class="comment"># 打印预测输出和实际输出的R2值</span></span><br><span class="line"><span class="built_in">print</span>(sm.r2_score(test_y, pre_test_y))</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[<span class="symbol">&#x27;CRIM</span>&#x27; <span class="symbol">&#x27;ZN</span>&#x27; <span class="symbol">&#x27;INDUS</span>&#x27; <span class="symbol">&#x27;CHAS</span>&#x27; <span class="symbol">&#x27;NOX</span>&#x27; <span class="symbol">&#x27;RM</span>&#x27; <span class="symbol">&#x27;AGE</span>&#x27; <span class="symbol">&#x27;DIS</span>&#x27; <span class="symbol">&#x27;RAD</span>&#x27; <span class="symbol">&#x27;TAX</span>&#x27; <span class="symbol">&#x27;PTRATIO</span>&#x27;</span><br><span class="line"> <span class="symbol">&#x27;B</span>&#x27; <span class="symbol">&#x27;LSTAT</span>&#x27;]</span><br><span class="line">(<span class="name">506</span>, <span class="number">13</span>)</span><br><span class="line">(<span class="name">506</span>,)</span><br><span class="line"><span class="number">0.8202560889408634</span></span><br></pre></td></tr></table></figure>

<p>特征重要性：</p>
<p>作为决策树模型训练过程中的副产品，根据每个特征划分子表前后信息熵减少量就标志了该特征的重要程度，此即为该特征重要性的指标。训练后得到的模型对象提供了属性feature_importances_来存储每个特征的重要性。在工程应用上，可以对决策树做一些优化，不必让每一个特征都参与子表划分，而只选择其中较重要的（或者说影响因素较大的）的特征作为子表划分依据。特征重要性的评价指标，就是根据该特征划分子表后所带来的信息熵减少量，熵减越大的就越重要，也就越优先参与子表的划分。</p>
<p>在上述示例中加入如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">fi = model.feature_importances_  <span class="comment"># 获取特征重要性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;fi:&quot;</span>, fi)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征重要性可视化</span></span><br><span class="line">mp.figure(<span class="string">&quot;Feature importances&quot;</span>, facecolor=<span class="string">&quot;lightgray&quot;</span>)</span><br><span class="line">mp.plot()</span><br><span class="line">mp.title(<span class="string">&quot;DT Feature&quot;</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">mp.ylabel(<span class="string">&quot;Feature importances&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.grid(linestyle=<span class="string">&quot;:&quot;</span>, axis=<span class="number">1</span>)</span><br><span class="line">x = np.arange(fi.size)</span><br><span class="line">sorted_idx = fi.argsort()[::-<span class="number">1</span>]  <span class="comment"># 重要性排序(倒序)</span></span><br><span class="line">fi = fi[sorted_idx]  <span class="comment"># 根据排序索引重新排特征值</span></span><br><span class="line">mp.xticks(x, boston.feature_names[sorted_idx])</span><br><span class="line">mp.bar(x, fi, <span class="number">0.4</span>, color=<span class="string">&quot;dodgerblue&quot;</span>, label=<span class="string">&quot;DT Feature importances&quot;</span>)</span><br><span class="line"></span><br><span class="line">mp.legend()</span><br><span class="line">mp.tight_layout()</span><br><span class="line">mp.show()</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<img src="/images/ml/classification/decision_tree8.png" alt="" width="600px"/>

<h3 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h3><p>剪枝（pruning）是决策树学习算法对付“过拟合”的主要手段. 在决策树学习中，为了尽可能正确分类训练样本，节点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学的“太好了”，以至于把训练集本身的一些特点当做数据所具有的一般性质而导致过拟合. 因此，可通过主动去掉一些分支来降低过拟合风险. </p>
<p>（1）预剪枝. 决策树生成过程中，对每个节点在划分前进行评估，若当前节点不能带来决策树泛化性能的提升，则停止划分并将当前节点标记为叶子节点.</p>
<p>（2）后剪枝. 先训练为一颗完整的决策树，然后自低向上对非叶子节点进行考察，若将该节点对应的子树替换为叶节点能带来决策树泛化能力提升，则将该子树替换为叶节点.</p>
<h2 id="集成学习与随机森林"><a href="#集成学习与随机森林" class="headerlink" title="集成学习与随机森林"></a>集成学习与随机森林</h2><h3 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h3><p>集成学习（ensemble learning）通过构建并合并多个模型来完成学习任务，从而获得比单一学习模型更显著优越的泛化性能，简言之，集成学习就是利用模型的“集体智慧”，提升预测的准确率.  根据单个模型方式，集成学习可以分为两大类：</p>
<ul>
<li>个体间存在强依赖关系，必须串行生成的序列化方法，其代表为Boosting算法；</li>
<li>个体之间不存在强依赖关系，可同时生成的并行化方法，代表是Bagging和随机森林算法.</li>
</ul>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><h4 id="Boosting定义"><a href="#Boosting定义" class="headerlink" title="Boosting定义"></a>Boosting定义</h4><p>Boosting（直译为推进、提升）是一族可以将弱学习器提升为强学习器的算法，其工作原理是：</p>
<ul>
<li>先训练出一个初始模型；</li>
<li>根据模型的表现进行调整，使得模型预测错误的数据获得更多的关注，再重新训练下一个模型；</li>
<li>不断重复第二步，直到模型数量达到预先设定的数目T，最终将这T个模型加权结合.</li>
</ul>
<p>AdaBoosting是Boosting算法族中最著名的算法，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。</p>
<h4 id="Boosting实现"><a href="#Boosting实现" class="headerlink" title="Boosting实现"></a>Boosting实现</h4><p>sklearn中，AdaBoosting相关API：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn.tree <span class="keyword">as</span> st</span><br><span class="line"><span class="keyword">import</span> sklearn.ensemble <span class="keyword">as</span> se</span><br><span class="line"></span><br><span class="line"><span class="comment"># model: 决策树模型（单个模型，基学习器）</span></span><br><span class="line">model = st.DecisionTreeRegressor(max_depth=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># n_estimators：构建400棵不同权重的决策树，训练模型</span></span><br><span class="line">model = se.AdaBoostRegressor(model, <span class="comment"># 单模型</span></span><br><span class="line">                             n_estimators=<span class="number">400</span>, <span class="comment"># 决策树数量</span></span><br><span class="line">                             random_state=<span class="number">7</span>)<span class="comment"># 随机种子</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model.fit(train_x, train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line">pred_test_y = model.predict(test_x)</span><br></pre></td></tr></table></figure>

<p>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># AdaBoosting示例</span></span><br><span class="line"><span class="comment"># 使用AdaBoosting预测波士顿房价</span></span><br><span class="line"><span class="keyword">import</span> sklearn.datasets <span class="keyword">as</span> sd</span><br><span class="line"><span class="keyword">import</span> sklearn.utils <span class="keyword">as</span> su</span><br><span class="line"><span class="keyword">import</span> sklearn.tree <span class="keyword">as</span> st</span><br><span class="line"><span class="keyword">import</span> sklearn.ensemble <span class="keyword">as</span> se</span><br><span class="line"><span class="keyword">import</span> sklearn.metrics <span class="keyword">as</span> sm</span><br><span class="line"></span><br><span class="line">boston = sd.load_boston()  <span class="comment"># 加载boston地区房价数据</span></span><br><span class="line"><span class="built_in">print</span>(boston.feature_names)</span><br><span class="line"><span class="built_in">print</span>(boston.data.shape)</span><br><span class="line"><span class="built_in">print</span>(boston.target.shape)</span><br><span class="line"></span><br><span class="line">random_seed = <span class="number">7</span>  <span class="comment"># 随机种子，计算随机值，相同的随机种子得到的随机值一样</span></span><br><span class="line">x, y = su.shuffle(boston.data, boston.target, random_state = random_seed)</span><br><span class="line"><span class="comment"># 计算训练数据的数量</span></span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="built_in">len</span>(x) * <span class="number">0.8</span>) <span class="comment"># 以boston.data中80%的数据作为训练数据</span></span><br><span class="line"><span class="comment"># 构建训练数据、测试数据</span></span><br><span class="line">train_x = x[:train_size]  <span class="comment"># 训练输入, x前面80%的数据</span></span><br><span class="line">test_x = x[train_size:]   <span class="comment"># 测试输入, x后面20%的数据</span></span><br><span class="line">train_y = y[:train_size]  <span class="comment"># 训练输出</span></span><br><span class="line">test_y = y[train_size:]   <span class="comment"># 测试输出</span></span><br><span class="line"></span><br><span class="line">model2 = se.AdaBoostRegressor(st.DecisionTreeRegressor(max_depth=<span class="number">4</span>),</span><br><span class="line">                              n_estimators=<span class="number">400</span>,   <span class="comment"># 决策树数量</span></span><br><span class="line">                              random_state=random_seed) <span class="comment"># 随机种子</span></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">model2.fit(train_x, train_y)</span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">pre_test_y2 = model2.predict(test_x)</span><br><span class="line"><span class="comment"># 打印预测输出和实际输出的R2值</span></span><br><span class="line"><span class="built_in">print</span>(sm.r2_score(test_y, pre_test_y2))</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[<span class="symbol">&#x27;CRIM</span>&#x27; <span class="symbol">&#x27;ZN</span>&#x27; <span class="symbol">&#x27;INDUS</span>&#x27; <span class="symbol">&#x27;CHAS</span>&#x27; <span class="symbol">&#x27;NOX</span>&#x27; <span class="symbol">&#x27;RM</span>&#x27; <span class="symbol">&#x27;AGE</span>&#x27; <span class="symbol">&#x27;DIS</span>&#x27; <span class="symbol">&#x27;RAD</span>&#x27; <span class="symbol">&#x27;TAX</span>&#x27; <span class="symbol">&#x27;PTRATIO</span>&#x27;</span><br><span class="line"> <span class="symbol">&#x27;B</span>&#x27; <span class="symbol">&#x27;LSTAT</span>&#x27;]</span><br><span class="line">(<span class="name">506</span>, <span class="number">13</span>)</span><br><span class="line">(<span class="name">506</span>,)</span><br><span class="line"><span class="number">0.9068598725149652</span></span><br></pre></td></tr></table></figure>

<p>可以看到，通过AdaBoosting算法，回归模型获得了更高的R2值.</p>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><h4 id="随机森林定义"><a href="#随机森林定义" class="headerlink" title="随机森林定义"></a>随机森林定义</h4><p>随机森林（Random Forest，简称RF）是专门为决策树设计的一种集成方法，是Bagging法的一种拓展，它是指每次构建决策树模型时，不仅随机选择部分样本，而且还随机选择部分特征来构建多棵决策树.  这样不仅规避了强势样本对预测结果的影响，而且也削弱了强势特征的影响，使模型具有更强的泛化能力. </p>
<p>随机森林简单、容易实现、计算开销小，在很多现实任务中展现出强大的性能，被誉为“代表集成学习技术水平的方法”.</p>
<h4 id="随机森林实现"><a href="#随机森林实现" class="headerlink" title="随机森林实现"></a>随机森林实现</h4><p>sklearn中，随机森林相关API：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn.ensemble <span class="keyword">as</span> se</span><br><span class="line"></span><br><span class="line">model = se.RandomForestRegressor(</span><br><span class="line">    max_depth, <span class="comment"># 决策树最大深度</span></span><br><span class="line">    n_estimators, <span class="comment"># 决策树数量</span></span><br><span class="line">    min_samples_split)<span class="comment"># 子表中最小样本数 若小于这个数字，则不再继续向下拆分</span></span><br></pre></td></tr></table></figure>

<p>以下是利用随机森林实现波士顿房价预测的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用随机森林预测波士顿房价</span></span><br><span class="line"><span class="keyword">import</span> sklearn.datasets <span class="keyword">as</span> sd</span><br><span class="line"><span class="keyword">import</span> sklearn.utils <span class="keyword">as</span> su</span><br><span class="line"><span class="keyword">import</span> sklearn.tree <span class="keyword">as</span> st</span><br><span class="line"><span class="keyword">import</span> sklearn.ensemble <span class="keyword">as</span> se</span><br><span class="line"><span class="keyword">import</span> sklearn.metrics <span class="keyword">as</span> sm</span><br><span class="line"></span><br><span class="line">boston = sd.load_boston()  <span class="comment"># 加载boston地区房价数据</span></span><br><span class="line"><span class="built_in">print</span>(boston.feature_names)</span><br><span class="line"><span class="built_in">print</span>(boston.data.shape)</span><br><span class="line"><span class="built_in">print</span>(boston.target.shape)</span><br><span class="line"></span><br><span class="line">random_seed = <span class="number">7</span>  <span class="comment"># 随机种子，计算随机值，相同的随机种子得到的随机值一样</span></span><br><span class="line">x, y = su.shuffle(boston.data, boston.target, random_state=random_seed)</span><br><span class="line"><span class="comment"># 计算训练数据的数量</span></span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="built_in">len</span>(x) * <span class="number">0.8</span>)  <span class="comment"># 以boston.data中80%的数据作为训练数据</span></span><br><span class="line"><span class="comment"># 构建训练数据、测试数据</span></span><br><span class="line">train_x = x[:train_size]  <span class="comment"># 训练输入, x前面80%的数据</span></span><br><span class="line">test_x = x[train_size:]  <span class="comment"># 测试输入, x后面20%的数据</span></span><br><span class="line">train_y = y[:train_size]  <span class="comment"># 训练输出</span></span><br><span class="line">test_y = y[train_size:]  <span class="comment"># 测试输出</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建随机森林回归器，并进行训练</span></span><br><span class="line">model = se.RandomForestRegressor(max_depth=<span class="number">10</span>,  <span class="comment"># 最大深度</span></span><br><span class="line">                                 n_estimators=<span class="number">1000</span>,  <span class="comment"># 树数量</span></span><br><span class="line">                                 min_samples_split=<span class="number">2</span>)  <span class="comment"># 最小样本数量，小于该数就不再划分子节点</span></span><br><span class="line">model.fit(train_x, train_y)  <span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于天统计数据的特征重要性</span></span><br><span class="line">fi_dy = model.feature_importances_</span><br><span class="line"><span class="comment"># print(fi_dy)</span></span><br><span class="line">pre_test_y = model.predict(test_x)</span><br><span class="line"><span class="built_in">print</span>(sm.r2_score(test_y, pre_test_y))  <span class="comment"># 打印r2得分</span></span><br></pre></td></tr></table></figure>

<p>打印输出：</p>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[<span class="symbol">&#x27;CRIM</span>&#x27; <span class="symbol">&#x27;ZN</span>&#x27; <span class="symbol">&#x27;INDUS</span>&#x27; <span class="symbol">&#x27;CHAS</span>&#x27; <span class="symbol">&#x27;NOX</span>&#x27; <span class="symbol">&#x27;RM</span>&#x27; <span class="symbol">&#x27;AGE</span>&#x27; <span class="symbol">&#x27;DIS</span>&#x27; <span class="symbol">&#x27;RAD</span>&#x27; <span class="symbol">&#x27;TAX</span>&#x27; <span class="symbol">&#x27;PTRATIO</span>&#x27;</span><br><span class="line"> <span class="symbol">&#x27;B</span>&#x27; <span class="symbol">&#x27;LSTAT</span>&#x27;]</span><br><span class="line">(<span class="name">506</span>, <span class="number">13</span>)</span><br><span class="line">(<span class="name">506</span>,)</span><br><span class="line"><span class="number">0.9271955403309159</span></span><br></pre></td></tr></table></figure>

</div><script type="text/javascript" src="../../../../js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://deeplearner.top/2022/10/30/MachineLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-7-%E5%88%86%E7%B1%BB-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB/" data-id="clkor69p4004xc6s624j5d92m" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAT4AAAE+CAAAAAAxUyPsAAAFUElEQVR42u3aMZLcSBADQP3/0zrnHMVKHADFlaOktbEzw24mGcFqVP/4ER8/vxx/+v/XT7fzf/1tfub8t88jvnbgw4cPHz58wdSTIZNPk1ESuOT/+bjJeZLf4sOHDx8+fBvf8+u/nWJSUjyXF89wz/XD84jPWLkJPnz48OHD93f48rLgXojkocD2af6I4MOHDx8+fH+fry0C8hLnQpbfwvt88OHDhw8fvrf42hLkErVvNyaPFS6h/Df2OvDhw4cPH75yF9a/8/fLBz58+PDhwxe8ni+bxvL2c1tMbC3wtwR+OT8+fPjw4cM3aeSRQR4BbAvytnhqz9OOFUX2+PDhw4cP35kvaSTnsfi9xMkvJm82JJFB1DDAhw8fPnz4Yr62RZ0vsy/Fx1sjtjc1p8SHDx8+fPgufG81ldugP99eloTvW+h/Konw4cOHDx++b+BrLzhvVLcRfz63fKzTGfDhw4cPH779DEXIvsX0W2HRoudthvZRiGIRfPjw4cOHL+C7x99bcZNcxhYr3HGL7+PDhw8fPnzf1uetl9Dxxq82Pmhb3flte555XUPhw4cPHz58U3Vziebz4GBrwOehQFugfJg/Pnz48OHDd+Brw/cfj8dbC/7tFl4i+6I5gQ8fPnz48E18+ev8GevSCE84ttuwXWkUiODDhw8fPnwT37bx661JtA31e3sgj+w/mODDhw8fPnwl31astLF7O63t9rSN/K08iooYfPjw4cOHL07at3b4try/lCNt2+BeDNVb1vDhw4cPH75gtRuF1OeQPS+PtkKnLbDa1v6HvQb48OHDhw/fH852WfZvbem8JGq3o20BfTKrPxZt+PDhw4cP34GvXYQnxcEljr8f263NCyB8+PDhw4ev5dvaw+20Ls3y/Dt5adWOG6Hjw4cPHz58wVW0BUfbFN+W9PlY9011G+uHxAUfPnz48OF75Kv76mV8sBUxl81kLWsRDbywLQEfPnz48P27fEmhcA+y29JkC+WTpf69SfCb7+PDhw8fPnwTX75R7B4B3FvU2xa67TsfHhF8+PDhw4cv5nt3u1gb1udBwM/HYxs9Dx3w4cOHDx++d/mSiW6v+XwqSavg0lrYtgJ8KI/w4cOHDx++ku/+ms+3puVAyS1sb3Z75iLgwIcPHz58+GK+fKPYqZF8OHMbtW8hRVv0/P8ffPjw4cOHb+LLF945RDuVvD3fjpK33vNbiw8fPnz48G18l6V7HtO3Le17odPejK3BgA8fPnz48LV8Oc1WuFwCgnzZv815K1bw4cOHDx++C9/24r8E5ZcGQHtL2iD+hZnjw4cPHz58QeGSR9g5aL4IzxftbUC/hRp5hIEPHz58+PDd+fIXf/6CTxrSbSmTz3krjKJHBx8+fPjw4Zs6zvdXftvwbgP07xg93yLw4bbhw4cPHz58E1870Xs5kpxha8a3m+q2WeHDhw8fPnwb3/OLP1muX5ribWMgbxK0W+La9gA+fPjw4cPX8m1L6K3NnJQal9j9srUunwk+fPjw4cN357sMnC/Xt4vMf7s15rfi5pddBvjw4cOHD9+B7w7RhhEtyha1bw/B2CDHhw8fPnz49ky+uMj81Hkcf48ANpq3rhQfPnz48OH7eoY2lG8jgKQRvmXgr+0RiCP76Gf48OHDhw9fwHcpL/Jf5Qv4bZ7bdbWsvylc8OHDhw8fvm/me4tgK1PyUiM/Q37V+PDhw4cP39/k+77FfNIgv2wyS2ZVPzr48OHDhw/fxPdWWH8J99staPdxX4gt8OHDhw8fvmsqXhc321awbcG/NbOThyO59tP+Pnz48OHD96/z/QcN9VlAjcDOJAAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="../../../../tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><i class="fa fa-tag"></i>机器学习笔记</a><a href="../../../../tags/%E5%91%A8%E5%BF%97%E5%8D%8E/"><i class="fa fa-tag"></i>周志华</a><a href="../../../../tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"><i class="fa fa-tag"></i>吴恩达</a><a href="../../../../tags/%E6%9D%8E%E8%88%AA/"><i class="fa fa-tag"></i>李航</a></div><div class="post-nav"><a class="pre" href="../../31/MachineLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-8-%E5%88%86%E7%B1%BB-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">MachineLearning-8.（分类）支持向量机SVM</a><a class="next" href="../../29/MachineLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-6-%E5%88%86%E7%B1%BB-Logistic%E5%9B%9E%E5%BD%92/">MachineLearning-6.（分类）Logistic回归</a></div><div class="nofancybox" id="waline"></div><script src="//unpkg.com/@waline/client@v2/dist/waline.js"></script><link rel="stylesheet" type="text/css" href="//unpkg.com/@waline/client@v2/dist/waline.css"><script>let metaInfo = ['nick', 'mail', 'link']
let requiredMeta = 'nick'.split(',').filter(item => {
  return metaInfo.indexOf(item) > -1
})
Waline.init({
  el: '#waline',
  comment: true,
  serverURL: 'https://waline-deeplearner.vercel.app',
  pageSize: '10',
  wordLimit: '500',
  requiredMeta,
})
</script></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/images/blog/avatar_new.png"/></a><p>心外无物，知行合一</p><a class="info-icon" href="/resume/" title="个人简历" target="_blank" style="margin-inline:5px"> <i class="fa fa-share-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:conghaoyuan@gmail.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/conghaoyuan" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/AIGC/">AIGC</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Algorithm/">Algorithm</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/CV/">CV</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/GUI/">GUI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/NLP/">NLP</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Python/">Python</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%97%E4%BA%BA/">数字人</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%A6/">数学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/">深度学习模型</a><span class="category-list-count">11</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/06/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8BPrompt%20Engineering/">AIGC-LLM-辟邪剑谱之Prompt Engineering</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/11/18/AIGC-Agent-%E4%BD%9C%E6%96%87%E6%89%B9%E6%94%B9%E6%A0%87%E6%B3%A8/">AIGC-Agent-作文批改标注</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/09/27/LLama32%E8%B0%83%E7%A0%94/">LLama3.2调研报告</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/04/09/AIGC-LLM-Prompt%E6%8A%80%E8%83%BD/">AIGC-LLM-Prompt工程技能综述</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/03/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-BERT/">Deeeplearning模型-NLP-BERT</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/01/16/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E8%AF%AD%E6%96%99%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/">Deeeplearning模型-NLP-大语言模型微调语料生成工具</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-Transformer%E6%A8%A1%E5%9E%8B/">Deeeplearning模型-NLP-Transformer模型</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/01/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-NLP%E5%9F%BA%E7%A1%80/">Deeeplearning模型-NLP-NLP基础</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/10/26/%E6%95%B0%E5%AD%97%E4%BA%BA-%E6%95%B0%E5%AD%97%E4%BA%BA%E6%8A%80%E6%9C%AF%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94/">数字人-数字人技术效果对比</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/09/15/AIGC-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83-LLama2-Lora%E5%8C%BB%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%832/">AIGC-大模型微调-LLama2-Lora医学大模型微调【风格学习】</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://conghaoyuan.github.io" title="Fern[个人早期博客]" target="_blank">Fern[个人早期博客]</a></div><div class="widget"><div class="widget-title"><i class="fa fa-star"> 天文学</i></div><ul></ul><a href="/tools/tianwenli.html" title="天文历" target="_self">天文历</a></div><div class="widget"><div class="widget-title"><i class="fa fa-plus"> 数学小工具</i></div><ul></ul><a href="/tools/buy_house.html" title="丐版买房计算器" target="_self">丐版买房计算器</a><ul></ul><a href="/tools/equation_12.html" title="一元二次方程" target="_self">一元二次方程</a><ul></ul><a href="/tools/equation_line.html" title="直线方程" target="_self">直线方程</a><ul></ul><a href="/tools/equation_polynomial.html" title="多项式方程" target="_self">多项式方程</a><ul></ul><a href="/tools/regression_ridge.html" title="岭回归" target="_self">岭回归</a><ul></ul><a href="/tools/fourier_transform.html" title="傅里叶变换" target="_self">傅里叶变换</a><ul></ul><a href="/tools/gradient_descent.html" title="梯度下降" target="_self">梯度下降</a><ul></ul><a href="/tools/function_e.html" title="指数函数" target="_self">指数函数</a><ul></ul><a href="/tools/function_inverse_scale.html" title="反比例函数" target="_self">反比例函数</a><ul></ul><a href="/tools/function_log.html" title="对数函数" target="_self">对数函数</a><ul></ul><a href="/tools/function_sin.html" title="正弦函数" target="_self">正弦函数</a><ul></ul><a href="/tools/coord_grid.html" title="网格化坐标" target="_self">网格化坐标</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="../../../../." rel="nofollow">DeepLearner.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="../../../../js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="../../../../js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="../../../../css/search.css?v=1.0.0"><script type="text/javascript" src="../../../../js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="/static/js/MathJax.js?config=TeX-MML-AM_CHTML" async></script><div id="script" type="text/javascript" src="../../../../js/mathjaxs.js" async></div><script type="text/javascript" src="../../../../js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="../../../../js/smartresize.js?v=1.0.0"></script></div><!-- hexo injector body_end start -->
<script src="assets/prism-bundle.js"></script>
<script src="assets/prism-plus.js" data-pjax=""></script>
<!-- hexo injector body_end end --></body></html>