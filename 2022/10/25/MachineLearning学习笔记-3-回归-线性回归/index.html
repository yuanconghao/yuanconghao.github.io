<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Just Do It"><title>MachineLearning-3.（回归）线性回归 | DeepLearner</title><link rel="stylesheet" type="text/css" href="../../../../css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/static/css/normalize.css"><link rel="stylesheet" type="text/css" href="/static/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/static/css/grids-responsive-min.css"><link rel="stylesheet" href="/static/css/font-awesome.min.css"><script type="text/javascript" src="/static/js/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="../../../../favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="../../../../favicon.ico"><link rel="apple-touch-icon" href="../../../../apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="../../../../apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="../../../../atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-83251621-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + 'b207d9febf3295d375e6e9b42006f9eb';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/static/js/clipboard.min.js"></script><script type="text/javascript" src="/static/js/toastr.min.js"></script><link rel="stylesheet" href="/static/css/toastr.min.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">MachineLearning-3.（回归）线性回归</h1><a id="logo" href="../../../../.">DeepLearner</a><p class="description">Deep Learning Notes</p></div><div id="nav-menu"><a class="current" href="../../../../."><i class="fa fa-home"> 首页</i></a><a href="../../../../archives/"><i class="fa fa-archive"> 归档</i></a><a href="../../../../tagcloud/"><i class="fa fa-tags"> 标签</i></a><a href="../../../../resource/"><i class="fa fa-cloud"> 资源</i></a><a href="../../../../guestbook/"><i class="fa fa-wechat"> 留言</i></a><a href="../../../../codeline/"><i class="fa fa-circle"> 码迹</i></a><a href="../../../../resume/"><i class="fa fa-user"> 个人简历</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">MachineLearning-3.（回归）线性回归</h1><div class="post-meta">created:2022-10-25</div><div class="post-meta">updated:2023-07-30<span> | </span><span class="category"><a href="../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><a class="disqus-comment-count" href="#waline"><span class="waline-comment-count" id=""></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="toc-number">1.</span> <span class="toc-text">线性模型定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">2.</span> <span class="toc-text">模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.2.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.2.1.</span> <span class="toc-text">为什么使用梯度下降</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.2.2.</span> <span class="toc-text">什么是梯度下降</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">线性回归实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.1.</span> <span class="toc-text">编程实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%9A%E8%BF%87sklearn-API%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.2.</span> <span class="toc-text">通过sklearn API实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">4.</span> <span class="toc-text">模型评价指标</span></a></li></ol></div></div><div class="post-content"><p>线性模型描述了一个或多个自变量对另一个因变量的影响所呈现的线性比例和关系。线性模型在二维空间内为一条直线，在三维空间中为一个平面，更高维度下的线性模型称为超平面。</p>
<table rules="none" align="center">
    <tr>
        <td>
            <center>
                <img src="/images/ml/regression/linear_1.png" width="80%" />
            </center>
        </td>
        <td>
            <center>
                <img src="/images/ml/regression/linear_2.png" width="80%" />
            </center>
        </td>
    </tr>
    <tr style="background:none">
        <td><center><font size="2">二维空间下线性模型表现为一条直线</font></center></td>
        <td><center><font size="2">三维空间下线性模型表现为一个平面</font></center></td>
    </tr>
</table>

<span id="more"></span>

<p>线性回归是要根据一组输入和输出数据（样本），寻找一个线性模型，能最佳程度拟合给定的数值分布，从而对新给定的输入数据进行输出预测。如：</p>
<table>
<thead>
<tr>
<th>输入(x)</th>
<th>输出(y)</th>
</tr>
</thead>
<tbody><tr>
<td>0.5</td>
<td>5.0</td>
</tr>
<tr>
<td>0.6</td>
<td>5.5</td>
</tr>
<tr>
<td>0.8</td>
<td>6.0</td>
</tr>
<tr>
<td>1.1</td>
<td>6.8</td>
</tr>
<tr>
<td>1.4</td>
<td>6.8</td>
</tr>
</tbody></table>
<p>根据样本拟合的线性模型：</p>
<p><img src="/images/ml/regression/linear_3.png" width="40%" alt="线性模型拟合" style="border:1px solid black"></img></p>
<h3 id="线性模型定义"><a href="#线性模型定义" class="headerlink" title="线性模型定义"></a>线性模型定义</h3><p>设给定一组属性$x, x&#x3D;(x_1;x_2;…;x_n)$，线性方程的一般表达形式为：<br>$$<br>y &#x3D; w_1x_1 + w_2x_2 + w_3x_3 + … + w_nx_n + b<br>$$<br>写成向量形式为：<br>$$<br>y &#x3D; w^Tx + b<br>$$<br>其中，$w&#x3D;(w_1;w_2;…;w_n), x&#x3D;(x_1;x_2;…;x_n)$，w和b经过学习后，模型就可以确定. 当自变量数量为1时，上述线性模型即为平面下的直线方程：<br>$$<br>y &#x3D; wx + b<br>$$<br>线性模型形式简单、易于建模，却蕴含着机器学习中一些重要的基本思想。许多功能强大的非线性模型可以在线性模型基础上引入层级结构或高维映射而得。此外，由于$w$直观表达了各属性在预测中的重要性，因此线性模型具有很好的可解释性。</p>
<p>例如，判断一个西瓜是否为好瓜，可以用如下表达式来判断：（参考周志华机器学习及西瓜书案例）<br>$$<br>f_{好瓜}(x) &#x3D; 0.2x_{色泽} + 0.5x_{根蒂} + 0.3x_{敲声} + 1<br>$$<br>上述公式可以解释为，一个西瓜是否为好瓜，可以通过色泽、根蒂、敲声等因素共同判断，其中根蒂最重要(权重最高)，其次是敲声和色泽.</p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>在二维平面中，给定两点可以确定一条直线。但在实际工程中，很多个样本点，无法找到一条直线精确穿过所有样本点，只能找到一条与样本“足够接近”或“距离足够小”的直线，近似拟合给定的样本，如：</p>
<p><img src="/images/ml/regression/linear_4.png" width="40%" alt="线性模型拟合" style="border:1px solid black"></img></p>
<p>可使用<strong>损失函数</strong>度量所有样本到直线的距离。</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>损失函数用来度量真实值（由样本中给出）和预测值（由模型算出）之间的差异。</p>
<ul>
<li>损失函数值越小，表明模型预测值和真实值之间差异越小，模型性能越好；</li>
<li>损失函数值越大，模型预测值和真实值之间差异越大，模型性能越差。</li>
</ul>
<p>在回归问题中，<strong>均方差</strong>是常用的损失函数，其表达式如下所示：<br>$$<br>E &#x3D; \frac{1}{2}\sum_{i&#x3D;1}^{n}{(y - y’)^2}<br>$$</p>
<p>其中，y为模型预测值，y’为真实值。线性回归的任务是寻找最优线性模型，损失函数值最小，即：<br>$$<br>(w^*, b^*) &#x3D; arg min \frac{1}{2}\sum_{i&#x3D;1}^{n}{(y - y’)^2} \<br> &#x3D; arg min \frac{1}{2}\sum_{i&#x3D;1}^{n}{(y’ - wx_i - b)^2}<br>$$</p>
<p>基于均方误差最小化来进行模型求解的方法称为“最小二乘法”。线性回归中，最小二乘法试图找到一条直线，使所有样本到直线的欧式距离之和最小。可将损失函数对w和b分别求导，得到损失函数的导函数，并令导函数为0即可得到w和b的最优解。</p>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><h5 id="为什么使用梯度下降"><a href="#为什么使用梯度下降" class="headerlink" title="为什么使用梯度下降"></a>为什么使用梯度下降</h5><p>在实际计算中，通过最小二乘法求解最优参数有一定的问题：</p>
<ol>
<li>最小二乘法需要计算逆矩阵，有可能逆矩阵不存在；</li>
<li>当样本特征数量较多时，计算逆矩阵非常耗时甚至不可行.</li>
</ol>
<p>所以，在实际计算中，通常采用梯度下降法来求解损失函数的极小值，从而找到模型的最优参数.</p>
<h5 id="什么是梯度下降"><a href="#什么是梯度下降" class="headerlink" title="什么是梯度下降"></a>什么是梯度下降</h5><p>已在<a href="http://deeplearner.top/2022/01/04/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97/#%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95">DeepLearning学习笔记-4-数值计算&#x2F;#基于梯度的优化方法</a>中对梯度下降做了详细介绍。</p>
<h3 id="线性回归实现"><a href="#线性回归实现" class="headerlink" title="线性回归实现"></a>线性回归实现</h3><h4 id="编程实现"><a href="#编程实现" class="headerlink" title="编程实现"></a>编程实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 线性回归示例</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> axes3d</span><br><span class="line"><span class="keyword">import</span> sklearn.preprocessing <span class="keyword">as</span> sp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练数据集</span></span><br><span class="line">train_x = np.array([<span class="number">0.5</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1.1</span>, <span class="number">1.4</span>])  <span class="comment"># 输入集</span></span><br><span class="line">train_y = np.array([<span class="number">5.0</span>, <span class="number">5.5</span>, <span class="number">6.0</span>, <span class="number">6.8</span>, <span class="number">7.0</span>])  <span class="comment"># 输出集</span></span><br><span class="line"></span><br><span class="line">n_epochs = <span class="number">1000</span>  <span class="comment"># 迭代次数</span></span><br><span class="line">lrate = <span class="number">0.01</span>  <span class="comment"># 学习率</span></span><br><span class="line">epochs = []  <span class="comment"># 记录迭代次数</span></span><br><span class="line">losses = []  <span class="comment"># 记录损失值</span></span><br><span class="line"></span><br><span class="line">w0, w1 = [<span class="number">1</span>], [<span class="number">1</span>]  <span class="comment"># 模型初始值</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</span><br><span class="line">    epochs.append(i)  <span class="comment"># 记录第几次迭代</span></span><br><span class="line"></span><br><span class="line">    y = w0[-<span class="number">1</span>] + w1[-<span class="number">1</span>] * train_x  <span class="comment"># 取出最新的w0,w1计算线性方程输出</span></span><br><span class="line">    <span class="comment"># 损失函数(均方差)</span></span><br><span class="line">    loss = (((train_y - y) ** <span class="number">2</span>).<span class="built_in">sum</span>()) / <span class="number">2</span></span><br><span class="line">    losses.append(loss)  <span class="comment"># 记录每次迭代的损失值</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%d: w0=%f, w1=%f, loss=%f&quot;</span> % (i, w0[-<span class="number">1</span>], w1[-<span class="number">1</span>], loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算w0,w1的偏导数</span></span><br><span class="line">    d0 = -(train_y - y).<span class="built_in">sum</span>()</span><br><span class="line">    d1 = -(train_x * (train_y - y)).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新w0,w1</span></span><br><span class="line">    w0.append(w0[-<span class="number">1</span>] - (d0 * lrate))</span><br><span class="line">    w1.append(w1[-<span class="number">1</span>] - (d1 * lrate))</span><br></pre></td></tr></table></figure>

<p>程序执行结果：</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1 <span class="attribute">w0</span>=1.00000000 <span class="attribute">w1</span>=1.00000000 <span class="attribute">loss</span>=44.17500000</span><br><span class="line">2 <span class="attribute">w0</span>=1.20900000 <span class="attribute">w1</span>=1.19060000 <span class="attribute">loss</span>=36.53882794</span><br><span class="line">3 <span class="attribute">w0</span>=1.39916360 <span class="attribute">w1</span>=1.36357948 <span class="attribute">loss</span>=30.23168666</span><br><span class="line">4 <span class="attribute">w0</span>=1.57220792 <span class="attribute">w1</span>=1.52054607 <span class="attribute">loss</span>=25.02222743</span><br><span class="line">5 <span class="attribute">w0</span>=1.72969350 <span class="attribute">w1</span>=1.66296078 <span class="attribute">loss</span>=20.71937337</span><br><span class="line"><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span></span><br><span class="line">996 <span class="attribute">w0</span>=4.06506160 <span class="attribute">w1</span>=2.26409126 <span class="attribute">loss</span>=0.08743506</span><br><span class="line">997 <span class="attribute">w0</span>=4.06518850 <span class="attribute">w1</span>=2.26395572 <span class="attribute">loss</span>=0.08743162</span><br><span class="line">998 <span class="attribute">w0</span>=4.06531502 <span class="attribute">w1</span>=2.26382058 <span class="attribute">loss</span>=0.08742820</span><br><span class="line">999 <span class="attribute">w0</span>=4.06544117 <span class="attribute">w1</span>=2.26368585 <span class="attribute">loss</span>=0.08742480</span><br><span class="line">1000 <span class="attribute">w0</span>=4.06556693 <span class="attribute">w1</span>=2.26355153 <span class="attribute">loss</span>=0.08742142</span><br></pre></td></tr></table></figure>

<p>数据可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###################### 训练过程可视化 ######################</span></span><br><span class="line"><span class="comment"># 训练过程可视化</span></span><br><span class="line"><span class="comment">## 损失函数收敛过程</span></span><br><span class="line">w0 = np.array(w0[:-<span class="number">1</span>])</span><br><span class="line">w1 = np.array(w1[:-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">mp.figure(<span class="string">&quot;Losses&quot;</span>, facecolor=<span class="string">&quot;lightgray&quot;</span>)  <span class="comment"># 创建一个窗体</span></span><br><span class="line">mp.title(<span class="string">&quot;epoch&quot;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">mp.ylabel(<span class="string">&quot;loss&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.grid(linestyle=<span class="string">&quot;:&quot;</span>)  <span class="comment"># 网格线：虚线</span></span><br><span class="line">mp.plot(epochs, losses, c=<span class="string">&quot;blue&quot;</span>, label=<span class="string">&quot;loss&quot;</span>)</span><br><span class="line">mp.legend()  <span class="comment"># 图例</span></span><br><span class="line">mp.tight_layout()  <span class="comment"># 紧凑格式</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 显示模型直线</span></span><br><span class="line">pred_y = w0[-<span class="number">1</span>] + w1[-<span class="number">1</span>] * train_x  <span class="comment"># 根据x预测y</span></span><br><span class="line">mp.figure(<span class="string">&quot;Linear Regression&quot;</span>, facecolor=<span class="string">&quot;lightgray&quot;</span>)</span><br><span class="line">mp.title(<span class="string">&quot;Linear Regression&quot;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">mp.xlabel(<span class="string">&quot;x&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.ylabel(<span class="string">&quot;y&quot;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.grid(linestyle=<span class="string">&quot;:&quot;</span>)</span><br><span class="line">mp.scatter(train_x, train_y, c=<span class="string">&quot;blue&quot;</span>, label=<span class="string">&quot;Traing&quot;</span>)  <span class="comment"># 绘制样本散点图</span></span><br><span class="line">mp.plot(train_x, pred_y, c=<span class="string">&quot;red&quot;</span>, label=<span class="string">&quot;Regression&quot;</span>)</span><br><span class="line">mp.legend()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示梯度下降过程(复制粘贴即可，不需要编写)</span></span><br><span class="line"><span class="comment"># 计算损失函数曲面上的点 loss = f(w0, w1)</span></span><br><span class="line">arr1 = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">500</span>)  <span class="comment"># 0~9间产生500个元素的均匀列表</span></span><br><span class="line">arr2 = np.linspace(<span class="number">0</span>, <span class="number">3.5</span>, <span class="number">500</span>)  <span class="comment"># 0~3.5间产生500个元素的均匀列表</span></span><br><span class="line"></span><br><span class="line">grid_w0, grid_w1 = np.meshgrid(arr1, arr2)  <span class="comment"># 产生二维矩阵</span></span><br><span class="line"></span><br><span class="line">flat_w0, flat_w1 = grid_w0.ravel(), grid_w1.ravel()  <span class="comment"># 二维矩阵扁平化</span></span><br><span class="line">loss_metrix = train_y.reshape(-<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 生成误差矩阵（-1,1）表示自动计算维度</span></span><br><span class="line">outer = np.outer(train_x, flat_w1)  <span class="comment"># 求外积（train_x和flat_w1元素两两相乘的新矩阵）</span></span><br><span class="line"><span class="comment"># 计算损失：((w0 + w1*x - y)**2)/2</span></span><br><span class="line">flat_loss = (((flat_w0 + outer - loss_metrix) ** <span class="number">2</span>).<span class="built_in">sum</span>(axis=<span class="number">0</span>)) / <span class="number">2</span></span><br><span class="line">grid_loss = flat_loss.reshape(grid_w0.shape)</span><br><span class="line"></span><br><span class="line">mp.figure(<span class="string">&#x27;Loss Function&#x27;</span>)</span><br><span class="line">ax = mp.gca(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">mp.title(<span class="string">&#x27;Loss Function&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;w0&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;w1&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">&#x27;loss&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax.plot_surface(grid_w0, grid_w1, grid_loss, rstride=<span class="number">10</span>, cstride=<span class="number">10</span>, cmap=<span class="string">&#x27;jet&#x27;</span>)</span><br><span class="line">ax.plot(w0, w1, losses, <span class="string">&#x27;o-&#x27;</span>, c=<span class="string">&#x27;orangered&#x27;</span>, label=<span class="string">&#x27;BGD&#x27;</span>, zorder=<span class="number">5</span>)</span><br><span class="line">mp.legend(loc=<span class="string">&#x27;lower left&#x27;</span>)</span><br><span class="line"></span><br><span class="line">mp.show()</span><br></pre></td></tr></table></figure>

<p><img src="/images/ml/regression/linear_5.png" width="400px" alt="线性回归模型"></img></p>
<p><img src="/images/ml/regression/linear_loss.png" width="400px" alt="损失函数收敛过程"></img></p>
<p><img src="/images/ml/regression/linear_6.png" width="400px" alt="梯度下降过程" style="border:1px solid black"></img></p>
<h4 id="通过sklearn-API实现"><a href="#通过sklearn-API实现" class="headerlink" title="通过sklearn API实现"></a>通过sklearn API实现</h4><p>同样，可以使用sklearn库提供的API实现线性回归。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用LinearRegression实现线性回归</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model <span class="keyword">as</span> lm  <span class="comment"># 线性模型# 线性模型</span></span><br><span class="line"><span class="keyword">import</span> sklearn.metrics <span class="keyword">as</span> sm  <span class="comment"># 模型性能评价模块</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> mp</span><br><span class="line"></span><br><span class="line">train_x = np.array([[<span class="number">0.5</span>], [<span class="number">0.6</span>], [<span class="number">0.8</span>], [<span class="number">1.1</span>], [<span class="number">1.4</span>]])  <span class="comment"># 输入集</span></span><br><span class="line">train_y = np.array([<span class="number">5.0</span>, <span class="number">5.5</span>, <span class="number">6.0</span>, <span class="number">6.8</span>, <span class="number">7.0</span>])  <span class="comment"># 输出集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建线性回归器</span></span><br><span class="line">model = lm.LinearRegression()</span><br><span class="line"><span class="comment"># 用已知输入、输出数据集训练回归器</span></span><br><span class="line">model.fit(train_x, train_y)</span><br><span class="line"><span class="comment"># 根据训练模型预测输出</span></span><br><span class="line">pred_y = model.predict(train_x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;coef_:&quot;</span>, model.coef_)  <span class="comment"># 系数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;intercept_:&quot;</span>, model.intercept_)  <span class="comment"># 截距</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化回归曲线</span></span><br><span class="line">mp.figure(<span class="string">&#x27;Linear Regression&#x27;</span>, facecolor=<span class="string">&#x27;lightgray&#x27;</span>)</span><br><span class="line">mp.title(<span class="string">&#x27;Linear Regression&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">mp.xlabel(<span class="string">&#x27;x&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.ylabel(<span class="string">&#x27;y&#x27;</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">mp.tick_params(labelsize=<span class="number">10</span>)</span><br><span class="line">mp.grid(linestyle=<span class="string">&#x27;:&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制样本点</span></span><br><span class="line">mp.scatter(train_x, train_y, c=<span class="string">&#x27;blue&#x27;</span>, alpha=<span class="number">0.8</span>, s=<span class="number">60</span>, label=<span class="string">&#x27;Sample&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制拟合直线</span></span><br><span class="line">mp.plot(train_x,  <span class="comment"># x坐标数据</span></span><br><span class="line">        pred_y,  <span class="comment"># y坐标数据</span></span><br><span class="line">        c=<span class="string">&#x27;orangered&#x27;</span>, label=<span class="string">&#x27;Regression&#x27;</span>)</span><br><span class="line"></span><br><span class="line">mp.legend()</span><br><span class="line">mp.show()</span><br></pre></td></tr></table></figure>

<p>执行结果：</p>
<p><img src="/images/ml/regression/linear_10.png" width="400px" alt="线性回归模型"></img></p>
<h3 id="模型评价指标"><a href="#模型评价指标" class="headerlink" title="模型评价指标"></a>模型评价指标</h3><ul>
<li><p>平均绝对误差（Mean Absolute Deviation）：单个观测值与算术平均值的偏差的绝对值的平均；</p>
</li>
<li><p>均方误差：单个样本到平均值差值的平方平均值；</p>
</li>
<li><p>MAD(中位数绝对偏差)：与数据中值绝对偏差的中值；</p>
</li>
<li><p>R2决定系数：趋向于1，模型越好；趋向于0，模型越差.</p>
</li>
</ul>
</div><script type="text/javascript" src="../../../../js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://deeplearner.top/2022/10/25/MachineLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-%E5%9B%9E%E5%BD%92-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" data-id="clkor69p3004ic6s66onn1e3b" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAEmCAAAAADqr2IGAAAEgklEQVR42u3by2rEOBAF0Pz/T2dgmOV0594qBVrO8So4YEvHAdUrX1/x9f3v9erOq5/b5+/f8uoJ+xViwoQJE6Z7mb7fXsk2khdvvuH7VbXverWvV2/BhAkTJkzPYEoe9/7Rm+Vugob2+B9CY8KECROmP8C0P5jbbSTPfx+45L/FhAkTJkyYZiluHkDMjvk2cMGECRMmTH+Tafa4tumYb2nW5kwChV+vhWPChAkTpg9jagd3nvTz4QsTJkyYMH0Y0/fimqWm+9Gc2Ro2+8WECRMmTPcy5SnoqRfvB3dO3a/bnJgwYcKE6UKmvIS6CQva0Zx2+Gbf4Pwh4MCECRMmTNcytcXctq3YDuLM2qLtUGzCgQkTJkyYnseUJ4r7DW8S41lang/0RFNOmDBhwoTpEUx5G7IdOZ29dzOUc2r4FRMmTJgw3cvUHpk56G+UjN8/oW2v/hAEYMKECROmy5nyA/VUYjkb3GlT303ZOppvwoQJEyZMH8+Ub7JdytlDOvk87ZhO8vz/fosJEyZMmK5lypPbHG42fNMWlE8N8eRJOyZMmDBhupFpNmSTEyTc+6Bh1kwtCr6YMGHChOlypmHnM0kX4+LsrJ2ZJ+E508uICRMmTJgwXci0H+KcpbXtMZ9/wjw5b9ufmDBhwoTpXqbV1M+oRJsfxnlocgqomG/ChAkTJkyXMO2P9jwlbgu7bdF29nM9MosJEyZMmC5hmoluBmjaUvKpovDsTwQTJkyYMN3ONNtScr9NLNsybp7Q/krijQkTJkyYrmKaubbL2iTYs/v5+A4mTJgwYXoq0+yQno2K5mlqMsTTFnPbgODl3xEmTJgwYbqKaVbizA/49jBuR3NOla2HtQFMmDBhwnQ503d8tYf3++dvmqmnBomG/VVMmDBhwvRhTPkr8/R4kyTPis5twFEP92DChAkTpsuZ8lGb/chOu9zkXb/xDx5RvxcTJkyYMF3FlCeip1qMedCwSWIPDLxiwoQJE6ZHMLUjnm0JuE2/N4f9vvj7ckQVEyZMmDBdxbQ5ODeNyXyreQG3TZvrcSJMmDBhwnQhU3ug5otrU+VTxdz2sw2LvJgwYcKE6RKmWevx7KJn5d22Ifp19sKECRMmTJcwzUZb8qXvU+X8fv4B8hT6f2rhmDBhwoTpQqa2mLtpc7bPnAUrZ4MhTJgwYcJ0L9MMYjMW097JA4j9mGwtigkTJkyYPp5pk+juh1bbAdb3QPlb2gEgTJgwYcL0DKZkWCcPCNqjPSFom6abj/FDkRcTJkyYMF3C1AK1B+2sVTlrZ7Yl41WpFxMmTJgwXcK0T1nzgKBthbYfb/80TJgwYcL0PKZNm3NWBc3pZ8l2njwf7gNjwoQJE6YPZsq3kQDlB+rZZuqm/Rml95gwYcKE6XKmUwXTJDiYxSb5Olv6aI+YMGHChOmPMeWIyeDOpqSbhzUbdEyYMGHC9BeYvhZXGxDMkurZgFFUvMaECRMmTJcztUXedrltYJE0PmdhRL5TTJgwYcL0DKbZ4M4e4lQAMTvg23AEEyZMmDBdyPQPnvZIgZnz+gMAAAAASUVORK5CYII=">分享</a><div class="tags"><a href="../../../../tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><i class="fa fa-tag"></i>机器学习笔记</a><a href="../../../../tags/%E5%91%A8%E5%BF%97%E5%8D%8E/"><i class="fa fa-tag"></i>周志华</a><a href="../../../../tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"><i class="fa fa-tag"></i>吴恩达</a><a href="../../../../tags/%E6%9D%8E%E8%88%AA/"><i class="fa fa-tag"></i>李航</a></div><div class="post-nav"><a class="pre" href="../../26/MachineLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-%E5%9B%9E%E5%BD%92-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92/">MachineLearning-4.（回归）多项式回归</a><a class="next" href="../../23/MachineLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/">MachineLearning-2.数据预处理</a></div><div class="nofancybox" id="waline"></div><script src="//unpkg.com/@waline/client@v2/dist/waline.js"></script><link rel="stylesheet" type="text/css" href="//unpkg.com/@waline/client@v2/dist/waline.css"><script>let metaInfo = ['nick', 'mail', 'link']
let requiredMeta = 'nick'.split(',').filter(item => {
  return metaInfo.indexOf(item) > -1
})
Waline.init({
  el: '#waline',
  comment: true,
  serverURL: 'https://waline-deeplearner.vercel.app',
  pageSize: '10',
  wordLimit: '500',
  requiredMeta,
})
</script></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/images/blog/avatar_new.png"/></a><p>心外无物，知行合一</p><a class="info-icon" href="/resume/" title="个人简历" target="_blank" style="margin-inline:5px"> <i class="fa fa-share-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:conghaoyuan@gmail.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/conghaoyuan" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/AIGC/">AIGC</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Algorithm/">Algorithm</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/CV/">CV</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/GUI/">GUI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/NLP/">NLP</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Python/">Python</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%97%E4%BA%BA/">数字人</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%A6/">数学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/">深度学习模型</a><span class="category-list-count">11</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/10/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8B%E9%80%9A%E4%B9%89Prompt%E5%9B%9B%E5%8D%81%E5%BC%8F/">AIGC-LLM-辟邪剑谱之通义Prompt四十式</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/06/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8BPrompt%20Engineering/">AIGC-LLM-辟邪剑谱之Prompt Engineering</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/11/18/AIGC-Agent-%E4%BD%9C%E6%96%87%E6%89%B9%E6%94%B9%E6%A0%87%E6%B3%A8/">AIGC-Agent-作文批改标注</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/09/27/LLama32%E8%B0%83%E7%A0%94/">LLama3.2调研报告</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/04/09/AIGC-LLM-Prompt%E6%8A%80%E8%83%BD/">AIGC-LLM-Prompt工程技能综述</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/03/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-BERT/">Deeeplearning模型-NLP-BERT</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/01/16/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E8%AF%AD%E6%96%99%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/">Deeeplearning模型-NLP-大语言模型微调语料生成工具</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-Transformer%E6%A8%A1%E5%9E%8B/">Deeeplearning模型-NLP-Transformer模型</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/01/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-NLP%E5%9F%BA%E7%A1%80/">Deeeplearning模型-NLP-NLP基础</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/10/26/%E6%95%B0%E5%AD%97%E4%BA%BA-%E6%95%B0%E5%AD%97%E4%BA%BA%E6%8A%80%E6%9C%AF%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94/">数字人-数字人技术效果对比</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://conghaoyuan.github.io" title="Fern[个人早期博客]" target="_blank">Fern[个人早期博客]</a></div><div class="widget"><div class="widget-title"><i class="fa fa-star"> 天文学</i></div><ul></ul><a href="/tools/tianwenli.html" title="天文历" target="_self">天文历</a></div><div class="widget"><div class="widget-title"><i class="fa fa-plus"> 数学小工具</i></div><ul></ul><a href="/tools/buy_house.html" title="丐版买房计算器" target="_self">丐版买房计算器</a><ul></ul><a href="/tools/equation_12.html" title="一元二次方程" target="_self">一元二次方程</a><ul></ul><a href="/tools/equation_line.html" title="直线方程" target="_self">直线方程</a><ul></ul><a href="/tools/equation_polynomial.html" title="多项式方程" target="_self">多项式方程</a><ul></ul><a href="/tools/regression_ridge.html" title="岭回归" target="_self">岭回归</a><ul></ul><a href="/tools/fourier_transform.html" title="傅里叶变换" target="_self">傅里叶变换</a><ul></ul><a href="/tools/gradient_descent.html" title="梯度下降" target="_self">梯度下降</a><ul></ul><a href="/tools/function_e.html" title="指数函数" target="_self">指数函数</a><ul></ul><a href="/tools/function_inverse_scale.html" title="反比例函数" target="_self">反比例函数</a><ul></ul><a href="/tools/function_log.html" title="对数函数" target="_self">对数函数</a><ul></ul><a href="/tools/function_sin.html" title="正弦函数" target="_self">正弦函数</a><ul></ul><a href="/tools/coord_grid.html" title="网格化坐标" target="_self">网格化坐标</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="../../../../." rel="nofollow">DeepLearner.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="../../../../js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="../../../../js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="../../../../css/search.css?v=1.0.0"><script type="text/javascript" src="../../../../js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="/static/js/MathJax.js?config=TeX-MML-AM_CHTML" async></script><div id="script" type="text/javascript" src="../../../../js/mathjaxs.js" async></div><script type="text/javascript" src="../../../../js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="../../../../js/smartresize.js?v=1.0.0"></script></div><!-- hexo injector body_end start -->
<script src="assets/prism-bundle.js"></script>
<script src="assets/prism-plus.js" data-pjax=""></script>
<!-- hexo injector body_end end --></body></html>