<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Just Do It"><title>DeepLearning-6.深度前馈网络(二) | DeepLearner</title><link rel="stylesheet" type="text/css" href="../../../../css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/static/css/normalize.css"><link rel="stylesheet" type="text/css" href="/static/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/static/css/grids-responsive-min.css"><link rel="stylesheet" href="/static/css/font-awesome.min.css"><script type="text/javascript" src="/static/js/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="../../../../favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="../../../../favicon.ico"><link rel="apple-touch-icon" href="../../../../apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="../../../../apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="../../../../atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-83251621-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + 'b207d9febf3295d375e6e9b42006f9eb';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/static/js/clipboard.min.js"></script><script type="text/javascript" src="/static/js/toastr.min.js"></script><link rel="stylesheet" href="/static/css/toastr.min.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">DeepLearning-6.深度前馈网络(二)</h1><a id="logo" href="../../../../.">DeepLearner</a><p class="description">Deep Learning Notes</p></div><div id="nav-menu"><a class="current" href="../../../../."><i class="fa fa-home"> 首页</i></a><a href="../../../../archives/"><i class="fa fa-archive"> 归档</i></a><a href="../../../../tagcloud/"><i class="fa fa-tags"> 标签</i></a><a href="../../../../resource/"><i class="fa fa-cloud"> 资源</i></a><a href="../../../../guestbook/"><i class="fa fa-wechat"> 留言</i></a><a href="../../../../codeline/"><i class="fa fa-circle"> 码迹</i></a><a href="../../../../resume/"><i class="fa fa-user"> 个人简历</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">DeepLearning-6.深度前馈网络(二)</h1><div class="post-meta">created:2022-02-28</div><div class="post-meta">updated:2023-07-30<span> | </span><span class="category"><a href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><a class="disqus-comment-count" href="#waline"><span class="waline-comment-count" id=""></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">1.</span> <span class="toc-text">感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8E%A8%E5%AF%BC"><span class="toc-number">1.1.</span> <span class="toc-text">手动推导</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#XOR"><span class="toc-number">2.</span> <span class="toc-text">XOR</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8E%A8%E5%AF%BC-1"><span class="toc-number">2.1.</span> <span class="toc-text">手动推导</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C"><span class="toc-number">3.</span> <span class="toc-text">深度前馈网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.</span> <span class="toc-text">基于梯度的学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-number">4.1.</span> <span class="toc-text">代价函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%8D%95%E5%85%83"><span class="toc-number">4.2.</span> <span class="toc-text">输出单元</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%AB%98%E6%96%AF%E8%BE%93%E5%87%BA%E5%88%86%E5%B8%83%E7%9A%84%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83"><span class="toc-number">4.2.1.</span> <span class="toc-text">高斯输出分布的线性单元</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BC%AF%E5%8A%AA%E5%88%A9%E8%BE%93%E5%87%BA%E5%88%86%E5%B8%83%E7%9A%84sigmoid%E5%8D%95%E5%85%83"><span class="toc-number">4.2.2.</span> <span class="toc-text">伯努利输出分布的sigmoid单元</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%8C%83%E7%95%B4%E8%BE%93%E5%87%BA%E5%88%86%E5%B8%83%E7%9A%84softmax%E5%8D%95%E5%85%83"><span class="toc-number">4.2.3.</span> <span class="toc-text">范畴输出分布的softmax单元</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%90%E8%97%8F%E5%8D%95%E5%85%83"><span class="toc-number">5.</span> <span class="toc-text">隐藏单元</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">6.</span> <span class="toc-text">架构设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E5%85%B6%E4%BB%96%E5%BE%AE%E5%88%86%E7%AE%97%E6%B3%95"><span class="toc-number">7.</span> <span class="toc-text">反向传播和其他微分算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">7.1.</span> <span class="toc-text">计算图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BE%AE%E7%A7%AF%E5%88%86%E4%B8%AD%E7%9A%84%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-number">7.2.</span> <span class="toc-text">微积分中的链式法则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%92%E5%BD%92%E5%9C%B0%E4%BD%BF%E7%94%A8%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99%E6%9D%A5%E5%AE%9E%E7%8E%B0%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">7.3.</span> <span class="toc-text">递归地使用链式法则来实现反向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5MLP%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97"><span class="toc-number">7.4.</span> <span class="toc-text">全连接MLP中的反向传播计算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%A6%E5%8F%B7%E5%88%B0%E7%AC%A6%E5%8F%B7%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="toc-number">7.5.</span> <span class="toc-text">符号到符号的导数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E8%88%AC%E5%8C%96%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">7.6.</span> <span class="toc-text">一般化的反向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%A8%E4%BA%8EMLP%E8%AE%AD%E7%BB%83%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">7.7.</span> <span class="toc-text">用于MLP训练的反向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%8D%E6%9D%82%E5%8C%96"><span class="toc-number">7.8.</span> <span class="toc-text">复杂化</span></a></li></ol></li></ol></div></div><div class="post-content"><p>&emsp;&emsp;<strong>深度前馈网络（Deep Feedforward Network，DFN）</strong>，也叫做<strong>前馈神经网络（Feedforward Neural Network，FNN）</strong>或者<strong>多层感知机（Multilayer Perception，MLP）</strong>，典型的深度学习模型。目标是拟合一个函数，如有一个分类器$y&#x3D;f^{*}(x)$将输入$x$映射到输出类别$y$。深度前馈网将这个映射定义为$f(x,\theta)$，并学习这个参数$\theta$的值来得到最好的函数拟合。</p>
<p><img src="/images/feedforward.png" width="400px"></img></p>
<span id="more"></span>

<h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>&emsp;&emsp;感知机（Perception）由Rosenblatt在1957年提出，是<strong>神经网络和支持向量机</strong>的基础。由n个输入数据，通过权重与各数据之前的计算和，比较激活函数结果，得出输出，可解决与、或、非问题，一般用于分类问题。有输入输出、权重和偏置、NetSum、激活函数四部分组成。</p>
<p><img src="/images/fnn_perception.png" width="500px"></img></p>
<p>&emsp;&emsp;工作步骤：</p>
<ul>
<li>输入$x_{i}$并乘以对于的权重$w_{i}$，得到$k_{i}$。</li>
<li>将所有相乘的值$k_{i}$相加，得到加权和。</li>
<li>将加权和应用于正确的激活函数。</li>
<li>数据应用，如将数据分为两部分（线性二院分类器）</li>
</ul>
<h4 id="手动推导"><a href="#手动推导" class="headerlink" title="手动推导"></a>手动推导</h4><p><img src="/images/fnn_perception1.jpg" width="600px"></img><br><img src="/images/fnn_perception2.jpg" width="600px"></img></p>
<h3 id="XOR"><a href="#XOR" class="headerlink" title="XOR"></a>XOR</h3><p>&emsp;&emsp;<strong>异或问题，相同为0，不同为1。</strong> 如上图所示，无法用一条直线将异或问题来分类。为解决该问题，可再添加一条直线，用两条直线分割，也就意味着需要再添加一个感知机。</p>
<p>&emsp;&emsp;一个好玩的神经网络演示<a target="_blank" rel="noopener" href="https://playground.tensorflow.org/">playground.tensorflow.org</a>，如下图的分类类似于异或问题，当用一层神经网络时，无法分类。当增加一层，用两层神经网络便可对所有的点进行分类。</p>
<p><img src="/images/fnn_xor1.png" width="800px"></img><br><img src="/images/fnn_xor2.png" width="800px"></img></p>
<p>&emsp;&emsp;解决网络如图：<br><img src="/images/fnn_xor.png" width="300px"></img><br>&emsp;&emsp;整个网络为：</p>
<center>$f(x;W,c,w,b)=w^{\top }max \{ 0,W^{\top}x+c \}+b$</center>

<p>&emsp;&emsp;激活函数使用ReLU：<br>$$ReLU(x)&#x3D;\begin{cases}<br> x &amp; \text{ if } x&gt; 0 \\<br> 0 &amp; \text{ if } x\le 0<br>\end{cases}$$</p>
<p><img src="/images/activation_leru.png" width="400px"></img></p>
<h4 id="手动推导-1"><a href="#手动推导-1" class="headerlink" title="手动推导"></a>手动推导</h4><p><img src="/images/fnn_xor3.jpg" width="600px"></img></p>
<h3 id="深度前馈网络"><a href="#深度前馈网络" class="headerlink" title="深度前馈网络"></a>深度前馈网络</h3><p>&emsp;&emsp;深度前馈网络中信息从$x$流入，通过中间$f$的计算，最后到达输出$y$。深度前馈网络示意图如下：</p>
<p><img src="/images/fnn_3.png" width="300px"></img></p>
<p>&emsp;&emsp;函数$f^{(1)},f^{(2)},f^{(3)}$链式连接，可表示为$f(x)&#x3D;f^{(3)}(f^{(2)}(f^{(1)}(x)))$，这种链式结构是神经网络最为常用结构。$f^{(1)},f^{(2)}$被称为神经网络的第一层，第二层，也为网络的隐藏层（Hidden Layer），最后一层$f^{(3)}$为输出层（Output Layer）。链的长度为神经网络的深度，输入向量的每个元素均视作一个神经元。</p>
<h3 id="基于梯度的学习"><a href="#基于梯度的学习" class="headerlink" title="基于梯度的学习"></a>基于梯度的学习</h3><p>&emsp;&emsp;在[<a href="https://deeplearner.top/2022/01/04/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97/">DeepLearning学习笔记-4-数值计算</a>] 一文中已经介绍了梯度下降优化方法，训练算法几乎总是基于使用梯度来使得代价函数下降的各种方法。（基于梯度下降思想的改进和提纯）</p>
<h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>&emsp;&emsp;任何能够衡量模型预测值与真实值之间的差异的函数都可以叫做代价函数。当输出神经元的激活函数是线性时(如ReLU函数)，二次代价函数是一种合适的选择；当输出神经元的激活函数是S型函数时(如sigmoid、tanh函数)，选择交叉熵代价函数则比较合理。</p>
<h4 id="输出单元"><a href="#输出单元" class="headerlink" title="输出单元"></a>输出单元</h4><p>&emsp;&emsp;常用的线性、sigmoid、softmax输出单元为最常见输出单元。</p>
<h5 id="高斯输出分布的线性单元"><a href="#高斯输出分布的线性单元" class="headerlink" title="高斯输出分布的线性单元"></a>高斯输出分布的线性单元</h5><h5 id="伯努利输出分布的sigmoid单元"><a href="#伯努利输出分布的sigmoid单元" class="headerlink" title="伯努利输出分布的sigmoid单元"></a>伯努利输出分布的sigmoid单元</h5><h5 id="范畴输出分布的softmax单元"><a href="#范畴输出分布的softmax单元" class="headerlink" title="范畴输出分布的softmax单元"></a>范畴输出分布的softmax单元</h5><h3 id="隐藏单元"><a href="#隐藏单元" class="headerlink" title="隐藏单元"></a>隐藏单元</h3><p>&emsp;&emsp;层与层之间是全连接的，第$i$层的任意一个神经元一定与第$i+1$层的任意一个神经元相连。如图所示，大多数隐藏单元都可以描述为接受输入向量$x$，计算仿射变换$z&#x3D;W^{\top}x+b$，然后使用一个逐元素的非线性函数$g(z)$得到隐藏单元的输出$\alpha$。而大多数隐藏单元的区别仅仅在于激活函数$g(z)$的形式。</p>
<p><img src="/images/fnn_4.png" width="300px"></img></p>
<p>&emsp;&emsp;如图所示，假设激活函数$g(z)$为$\sigma$，于是$f^{(1)}$层的隐藏单元可描述为：</p>
<p>$$\left\{\begin{matrix}<br>a_{1}&#x3D;\sigma (z_{1})&#x3D;\sigma (w_{11}x_{1}+w_{12}x_{2}+w_{13}x_{3}+b_{1}) \\<br>a_{2}&#x3D;\sigma (z_{2})&#x3D;\sigma (w_{21}x_{1}+w_{22}x_{2}+w_{23}x_{3}+b_{2}) \\<br>a_{3}&#x3D;\sigma (z_{3})&#x3D;\sigma (w_{31}x_{1}+w_{32}x_{2}+w_{33}x_{3}+b_{3}) \\<br>\end{matrix}\right.$$</p>
<p>&emsp;&emsp;选择隐藏单元实际上就是要选择一个合适的激活函数。常见激活函数：</p>
<ul>
<li>整流线性单元（ReLU）：$g(z)&#x3D;max{0, z}$。优点是易于优化，二阶导数几乎处处为0，<strong>处于激活状态时一阶导数处处为1</strong>，相比于引入二阶效应的激活函数，梯度方向对学习更有用。如果使用ReLU，第一步做线性变换$W^{\top}x+b$时的$b$一般设置成小的正值。缺陷时不能通过基于梯度的方法学习那些单元激活为0的样本。ReLU函数的梯度为</li>
</ul>
<p>$$g^{\prime}(z)&#x3D;\begin{cases}<br> 1 &amp; x&gt; 0 \\<br> 0 &amp; x\le 0<br>\end{cases}$$</p>
<ul>
<li><p>sigmoid函数或双曲正切函数$tanh$。两者之间有一个联系:$tanh(z)&#x3D;2\sigma (2z)-1$。两者都比较容易饱和，仅当$z$接近0时才对输入强烈敏感，因此使得基于梯度的学习变得非常困难，不适合做前馈网络中的隐藏单元。<strong>如果必须要使用两种中的一个，那么tanh通常表现更好</strong>，因为在0附近其类似于单位函数。即，如果网络的激活能一直很小，训练$\hat{y}&#x3D;w^{\top }tanh(U^{\top }tanh(V^{\top }x))$类似于训练一个线性模型$\hat{y}&#x3D;w^{\top }U^{\top }V^{\top }x$。RNN和一些自编码器有一些额外的要求，因此不能使用分段激活函数，此时这种类sigmoid单元更合适。<br>sigmoid函数写作$g(z)&#x3D;\sigma (z)&#x3D;\frac{1}{1+e^{-z}}$，梯度为$g^{\prime}(z)&#x3D;\sigma (z)(1-\sigma (z))$<br>双曲正切函数写作$g(z)&#x3D;tanh(z)&#x3D;\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} $，梯度为$g^{\prime }(z)&#x3D;1-tanh^{2}(z)$</p>
</li>
<li><p>softplus函数</p>
</li>
<li><p>径向基函数</p>
</li>
<li><p>硬双曲正切函数</p>
</li>
</ul>
<h3 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h3><p>&emsp;&emsp;架构指网络的整体结构，具有多少单元，以及这些单元应该如何连接。在实践中，神经网络具有多样性，卷积神经网络、循环神经网络会在后边章节学习、分析、总结。</p>
<h3 id="反向传播和其他微分算法"><a href="#反向传播和其他微分算法" class="headerlink" title="反向传播和其他微分算法"></a>反向传播和其他微分算法</h3><p>&emsp;&emsp;<strong>前向传播（Forward Propagation）</strong>：前馈神经网络接受输入$x$并产生输出$\hat{y}$时，信息通过网络向前流动，输入$x$提供初始信息，然后传播到每一层的隐藏单元，最终产生输出$\hat{y}$。<strong>反向传播（Back Propagation）</strong>：允许来自代价函数的信息通过网络向后流动，以便计算梯度，是指计算梯度的方法。</p>
<h4 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h4><p>&emsp;&emsp;主要用图语言来描述神经网络。</p>
<h4 id="微积分中的链式法则"><a href="#微积分中的链式法则" class="headerlink" title="微积分中的链式法则"></a>微积分中的链式法则</h4><p>&emsp;&emsp;微积分中的链式法则用于计算复合函数的导数。反向传播是一种计算链式法则的算法，使用高效的特定运算顺序。设$x$是实数，$f$和$g$是从实数映射到实数的函数。假设$y&#x3D;g(x)$，并且$z&#x3D;f(g(x))&#x3D;f(y)$。链式法则为：</p>
<center>$\frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}$</center>

<p>&emsp;&emsp;将这种标量进行扩展，假设$x\in \mathbb{R}^{m},y\in \mathbb{R}^{n}$，$g$是从$\mathbb{R}^{m}$到$\mathbb{R}^{n}$的映射，$f$是从$\mathbb{R}^{n}$到$\mathbb{R}$的映射。如果$y&#x3D;g(x)$，并且$z&#x3D;f(y)$，那么：</p>
<center>$\frac{\partial z}{\partial x_{i}} =\sum_{j}\frac{\partial z}{\partial y_{j}} \frac{\partial y_{j}}{\partial x_{i}}$</center>

<p>&emsp;&emsp;使用向量法，可以等价的写成</p>
<center>$\nabla_{x}z=\left ( \frac{\partial y}{\partial x}  \right ) ^{\top }\nabla_{y}z$</center>

<p>&emsp;&emsp;$\frac{\partial y}{\partial x}$是$g$的$n\times m$的Jacobian矩阵。</p>
<h4 id="递归地使用链式法则来实现反向传播"><a href="#递归地使用链式法则来实现反向传播" class="headerlink" title="递归地使用链式法则来实现反向传播"></a>递归地使用链式法则来实现反向传播</h4><p>&emsp;&emsp;许多子表达式可能在梯度的整个表达式中重复若干次。任何计算梯度的程序都需要选择是存储这些子表达式还是重新计算他们几次。</p>
<h4 id="全连接MLP中的反向传播计算"><a href="#全连接MLP中的反向传播计算" class="headerlink" title="全连接MLP中的反向传播计算"></a>全连接MLP中的反向传播计算</h4><h4 id="符号到符号的导数"><a href="#符号到符号的导数" class="headerlink" title="符号到符号的导数"></a>符号到符号的导数</h4><h4 id="一般化的反向传播"><a href="#一般化的反向传播" class="headerlink" title="一般化的反向传播"></a>一般化的反向传播</h4><h4 id="用于MLP训练的反向传播"><a href="#用于MLP训练的反向传播" class="headerlink" title="用于MLP训练的反向传播"></a>用于MLP训练的反向传播</h4><h4 id="复杂化"><a href="#复杂化" class="headerlink" title="复杂化"></a>复杂化</h4></div><script type="text/javascript" src="../../../../js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://deeplearner.top/2022/02/28/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-6-%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C2/" data-id="clkor69p20042c6s6cbjf3rut" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAEmCAAAAADqr2IGAAAEiElEQVR42u3a0U7lOgwFUP7/p7nSPM3oQtnb7pFOyuoTKqVJVpFiO/74iK/PP1f7zN93Pv+6ru+0s/r/e757ZjbKByZMmDBhOpzp8/K6nlAyiXyi1wvIP1UCl68dEyZMmDA9gyl/XbvZXwcKyXSvl53P+btnok+CCRMmTJh+AdMmxb0OEa7vzz7bbG6YMGHChAlTMokca1Zi3oQUmDBhwoTpdzLlr3vdZp8XkfP55O+/rRaOCRMmTJjejKndkp/08wv7mzBhwoQJ0xswbdLF/Jn8gDP/bb7sdv5fjIIJEyZMmI5lStLC2V6Z/9XrnpzFR0WFABMmTJgwvT1TvSmWG3DC127nM6ZV8RoTJkyYMD2IKZ9c3pQzS4A34+5biH6oEGDChAkTpkOY2kfzjbld3uwMsW3caQMOTJgwYcL0DKa7ktVXhBez9HsWlPzAjQkTJkyYDmfabMx56rgnmxWCZzRfBASYMGHChOkRTLNFtpvuLBx5RRgRBTGYMGHChOlApmTZbRPP5vAy/7mdTxtk/PNbTJgwYcL0CKbrDTVZ6mx7bj9Si97e/yEgwIQJEyZMBzK1W2lyf8PXFnzbIm89CiZMmDBhOpypLdrmhdQZfVLeTVLiu4ISTJgwYcL0DKZ9othCz667wo68PI0JEyZMmM5lyg8v80S3OCZcTHr2kW74YJgwYcKE6Simz/JKFnzX1p4ztXdysihuwoQJEyZMb8x0ndzmBda27JuP1R523pVgY8KECROmZzAl2+GMr22j2YQCedLblq0xYcKECdMzmNpC6j5Z3Y81KxwnYxVxByZMmDBhOpApDwiSTT1JgzdjtSFIPtYXqS8mTJgwYTqcaX8Y+epS7CyJnb3/29QXEyZMmDAdzrRp4mkTzrtGn32koriMCRMmTJiOZcobbmal3llwMKOZpdlFmIIJEyZMmA5kStLFmf1db96MUh9bfrc6TJgwYcJ0OFObrCYl1Hwqs9/mi9w0D2HChAkTptOZ2sPCV7Tm5GSzw8vk+eJ4FRMmTJgwPY7pFc03OVkbguTpbv4JMWHChAnTuUzJ1phDJO04myJsC5oz/TBbTJgwYcJ0LFNR3CwPI2ctQXc162xS9Oh/ChMmTJgwHci0n9z+0LFt8cm39tn7MWHChAnT6Ux5uTaZYhtG5EeVLcps3GLSmDBhwoTpcKbro762NJxPfVN6rhe/T3oxYcKECdMhTHkCnAcNeevPLJ2eBRbD8jQmTJgwYXoEU54S56HDPjWdJe2zWWHChAkTpucxbdLRtsy6udq2m00BupgQJkyYMGF6e6bP8poFBLNScltEzj9PPs9vG3cwYcKECdMhTHdtybMYZN8MlC9y9jMmTJgwYXoGUx4EJIXaGVxebN1s/7MABRMmTJgwPYOp3bAT3OuFtYXj9j1t1BMdYWLChAkTpkczbRLIWUjRhhH3fnJMmDBhwvSbmWbl2rbhddZF05ato7/FhAkTJkyHM7VF3k2rzfWdzYLbknQRoGDChAkTpmOZZtvnvuEmOVZsy8HtcebHvRcmTJgwYXovpv8AldzTBFtBK/cAAAAASUVORK5CYII=">分享</a><div class="tags"><a href="../../../../tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><i class="fa fa-tag"></i>深度学习笔记</a><a href="../../../../tags/%E8%8A%B1%E4%B9%A6/"><i class="fa fa-tag"></i>花书</a></div><div class="post-nav"><a class="pre" href="../../../05/19/Python-1-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90matplotlib%E4%BD%BF%E7%94%A8/">Python-1.数据分析Matplotlib使用</a><a class="next" href="../../10/DeepLearning%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-6-%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/">DeepLearning-6.深度前馈网络(一)</a></div><div class="nofancybox" id="waline"></div><script src="//unpkg.com/@waline/client@v2/dist/waline.js"></script><link rel="stylesheet" type="text/css" href="//unpkg.com/@waline/client@v2/dist/waline.css"><script>let metaInfo = ['nick', 'mail', 'link']
let requiredMeta = 'nick'.split(',').filter(item => {
  return metaInfo.indexOf(item) > -1
})
Waline.init({
  el: '#waline',
  comment: true,
  serverURL: 'https://waline-deeplearner.vercel.app',
  pageSize: '10',
  wordLimit: '500',
  requiredMeta,
})
</script></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/images/blog/avatar_new.png"/></a><p>心外无物，知行合一</p><a class="info-icon" href="/resume/" title="个人简历" target="_blank" style="margin-inline:5px"> <i class="fa fa-share-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:conghaoyuan@gmail.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/conghaoyuan" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/AIGC/">AIGC</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Algorithm/">Algorithm</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/CV/">CV</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/GUI/">GUI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/NLP/">NLP</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Python/">Python</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%97%E4%BA%BA/">数字人</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E5%AD%A6/">数学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/">深度学习模型</a><span class="category-list-count">11</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/10/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8B%E9%80%9A%E4%B9%89Prompt%E5%9B%9B%E5%8D%81%E5%BC%8F/">AIGC-LLM-辟邪剑谱之通义Prompt四十式</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2025/03/06/AIGC-LLM-%E8%BE%9F%E9%82%AA%E5%89%91%E8%B0%B1%E4%B9%8BPrompt%20Engineering/">AIGC-LLM-辟邪剑谱之Prompt Engineering</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/11/18/AIGC-Agent-%E4%BD%9C%E6%96%87%E6%89%B9%E6%94%B9%E6%A0%87%E6%B3%A8/">AIGC-Agent-作文批改标注</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/09/27/LLama32%E8%B0%83%E7%A0%94/">LLama3.2调研报告</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/04/09/AIGC-LLM-Prompt%E6%8A%80%E8%83%BD/">AIGC-LLM-Prompt工程技能综述</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/03/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-BERT/">Deeeplearning模型-NLP-BERT</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2024/01/16/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E8%AF%AD%E6%96%99%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7/">Deeeplearning模型-NLP-大语言模型微调语料生成工具</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/05/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-Transformer%E6%A8%A1%E5%9E%8B/">Deeeplearning模型-NLP-Transformer模型</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/12/01/Deeeplearning%E6%A8%A1%E5%9E%8B-NLP-NLP%E5%9F%BA%E7%A1%80/">Deeeplearning模型-NLP-NLP基础</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2023/10/26/%E6%95%B0%E5%AD%97%E4%BA%BA-%E6%95%B0%E5%AD%97%E4%BA%BA%E6%8A%80%E6%9C%AF%E6%95%88%E6%9E%9C%E5%AF%B9%E6%AF%94/">数字人-数字人技术效果对比</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://conghaoyuan.github.io" title="Fern[个人早期博客]" target="_blank">Fern[个人早期博客]</a></div><div class="widget"><div class="widget-title"><i class="fa fa-star"> 天文学</i></div><ul></ul><a href="/tools/tianwenli.html" title="天文历" target="_self">天文历</a></div><div class="widget"><div class="widget-title"><i class="fa fa-plus"> 数学小工具</i></div><ul></ul><a href="/tools/buy_house.html" title="丐版买房计算器" target="_self">丐版买房计算器</a><ul></ul><a href="/tools/equation_12.html" title="一元二次方程" target="_self">一元二次方程</a><ul></ul><a href="/tools/equation_line.html" title="直线方程" target="_self">直线方程</a><ul></ul><a href="/tools/equation_polynomial.html" title="多项式方程" target="_self">多项式方程</a><ul></ul><a href="/tools/regression_ridge.html" title="岭回归" target="_self">岭回归</a><ul></ul><a href="/tools/fourier_transform.html" title="傅里叶变换" target="_self">傅里叶变换</a><ul></ul><a href="/tools/gradient_descent.html" title="梯度下降" target="_self">梯度下降</a><ul></ul><a href="/tools/function_e.html" title="指数函数" target="_self">指数函数</a><ul></ul><a href="/tools/function_inverse_scale.html" title="反比例函数" target="_self">反比例函数</a><ul></ul><a href="/tools/function_log.html" title="对数函数" target="_self">对数函数</a><ul></ul><a href="/tools/function_sin.html" title="正弦函数" target="_self">正弦函数</a><ul></ul><a href="/tools/coord_grid.html" title="网格化坐标" target="_self">网格化坐标</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="../../../../." rel="nofollow">DeepLearner.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="../../../../js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="../../../../js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="../../../../css/search.css?v=1.0.0"><script type="text/javascript" src="../../../../js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="/static/js/MathJax.js?config=TeX-MML-AM_CHTML" async></script><div id="script" type="text/javascript" src="../../../../js/mathjaxs.js" async></div><script type="text/javascript" src="../../../../js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="../../../../js/smartresize.js?v=1.0.0"></script></div><!-- hexo injector body_end start -->
<script src="assets/prism-bundle.js"></script>
<script src="assets/prism-plus.js" data-pjax=""></script>
<!-- hexo injector body_end end --></body></html>